{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# nndebugger functions\n",
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Remove all 'importlib' statements\n",
    "1. Clean up arguments passed into DebugSession\n",
    "1. Add ReadMe to repo\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Clear outputs\n",
    "1. Delete this cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "len(data_df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3380"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(MyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : MyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "data_set = train_X\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here. The correct line is 'return x.view(data.num_graphs,)'\n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-80c0c8fa0ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 134.40209197998047\n",
      "..last epoch zero_data_loss 134.75465393066406\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-66eab23cba70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    158\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    159\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    161\u001b[0m                 )\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "..Epoch 0\n",
      "....Outputs -0.1179 -0.0832 -0.0910 -0.1023 -0.1179\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "....Loss: 5.164145787847168\n",
      "....R2: -24.459165859291733\n",
      "..Epoch 1\n",
      "....Outputs -0.0351 -0.0503 -0.0165 -0.0337 -0.0503\n",
      "....Labels  2.9694 5.7012 5.3739 5.0497 5.6991\n",
      "....Loss: 5.1002019730011945\n",
      "....R2: -23.832585224647396\n",
      "..Epoch 2\n",
      "....Outputs 0.0199 0.0257 0.0519 0.0095 0.0095\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "....Loss: 5.040911384104809\n",
      "....R2: -23.258576547216546\n",
      "..Epoch 3\n",
      "....Outputs 0.0799 0.1340 0.0778 0.0778 0.0908\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 4.973188421236432\n",
      "....R2: -22.61114393548639\n",
      "..Epoch 4\n",
      "....Outputs 0.2393 0.1645 0.1709 0.1514 0.1645\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "....Loss: 4.888306674583275\n",
      "....R2: -21.81203662041188\n",
      "..Epoch 5\n",
      "....Outputs 0.2755 0.2755 0.2396 0.2719 0.3727\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "....Loss: 4.7807606964063485\n",
      "....R2: -20.819321674296184\n",
      "..Epoch 6\n",
      "....Outputs 0.5403 0.4175 0.3498 0.4175 0.4008\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 4.644630629981769\n",
      "....R2: -19.594419743915928\n",
      "..Epoch 7\n",
      "....Outputs 0.4866 0.7494 0.5966 0.5966 0.5624\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 4.474223379589342\n",
      "....R2: -18.1109610909653\n",
      "..Epoch 8\n",
      "....Outputs 1.0087 0.7640 0.6550 0.8208 0.8208\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "....Loss: 4.262562752986111\n",
      "....R2: -16.345577851918957\n",
      "..Epoch 9\n",
      "....Outputs 1.1004 0.8625 1.3298 1.0138 1.1004\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 4.000683249294005\n",
      "....R2: -14.279722807063035\n",
      "..Epoch 10\n",
      "....Outputs 1.4460 1.4460 1.3233 1.7261 1.1165\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "....Loss: 3.6791838931022047\n",
      "....R2: -11.922608164542643\n",
      "..Epoch 11\n",
      "....Outputs 1.7023 1.8692 2.2155 1.8692 1.4256\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 3.2884296161716975\n",
      "....R2: -9.323436171779115\n",
      "..Epoch 12\n",
      "....Outputs 2.3882 2.8105 2.3882 2.1640 1.8034\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "....Loss: 2.8176726352501023\n",
      "....R2: -6.579285535627756\n",
      "..Epoch 13\n",
      "....Outputs 2.2582 2.7181 3.0192 3.5265 3.0192\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "....Loss: 2.2646333826378098\n",
      "....R2: -3.896018166044801\n",
      "..Epoch 14\n",
      "....Outputs 4.3795 2.8022 3.3662 3.7759 3.7759\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "....Loss: 1.6494165198046378\n",
      "....R2: -1.597214734515767\n",
      "..Epoch 15\n",
      "....Outputs 4.6727 4.0771 3.4411 5.3692 4.6727\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "....Loss: 1.0886559799013982\n",
      "....R2: -0.13143215244130046\n",
      "..Epoch 16\n",
      "....Outputs 4.7595 5.6851 6.4372 5.6851 4.1538\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.0137416690620507\n",
      "....R2: 0.01892604886001803\n",
      "..Epoch 17\n",
      "....Outputs 6.6780 4.8544 7.3817 5.2751 6.6780\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 1.5032213905818341\n",
      "....R2: -1.157213013288092\n",
      "..Epoch 18\n",
      "....Outputs 7.3862 5.4967 7.3862 7.9418 5.3938\n",
      "....Labels  5.7012 2.9694 5.6991 5.3739 5.0497\n",
      "....Loss: 1.9383025814360197\n",
      "....R2: -2.586661051331369\n",
      "..Epoch 19\n",
      "....Outputs 7.6840 5.4178 7.6840 8.0510 5.6893\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "....Loss: 2.0708371055563553\n",
      "....R2: -3.0939173116671617\n",
      "..Epoch 20\n",
      "....Outputs 5.7587 7.8103 7.6210 7.6210 5.1292\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 1.922636734027545\n",
      "....R2: -2.5289187914808067\n",
      "..Epoch 21\n",
      "....Outputs 7.3226 7.3511 7.3226 4.7229 5.6589\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "....Loss: 1.5886829834496452\n",
      "....R2: -1.409470771767074\n",
      "..Epoch 22\n",
      "....Outputs 6.8872 4.2626 5.4514 6.8872 6.7891\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 1.1536622344057446\n",
      "....R2: -0.27058740992268193\n",
      "..Epoch 23\n",
      "....Outputs 5.1924 6.4081 6.4081 3.8045 6.2042\n",
      "....Labels  5.0497 5.6991 5.7012 2.9694 5.3739\n",
      "....Loss: 0.6942094990871401\n",
      "....R2: 0.5399253597092853\n",
      "..Epoch 24\n",
      "....Outputs 4.9270 3.3841 5.6515 5.9386 5.9386\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "....Loss: 0.27488628496386613\n",
      "....R2: 0.9278637890996442\n",
      "..Epoch 25\n",
      "....Outputs 4.6885 5.5193 5.1660 5.5193 3.0193\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "....Loss: 0.21981452876476146\n",
      "....R2: 0.9538724937890292\n",
      "..Epoch 26\n",
      "....Outputs 4.7667 5.1738 2.7204 5.1738 4.4982\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "....Loss: 0.5077087589815735\n",
      "....R2: 0.7539199019938686\n",
      "..Epoch 27\n",
      "....Outputs 2.4897 4.9118 4.9118 4.3658 4.4588\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 0.7453867573403361\n",
      "....R2: 0.46959146915359573\n",
      "..Epoch 28\n",
      "....Outputs 4.7337 4.2404 4.7337 2.3185 4.2931\n",
      "....Labels  5.7012 5.3739 5.6991 2.9694 5.0497\n",
      "....Loss: 0.9109139297079434\n",
      "....R2: 0.2078600807200528\n",
      "..Epoch 29\n",
      "....Outputs 2.1993 4.6357 4.2793 4.6357 4.1057\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 1.0060799662354818\n",
      "....R2: 0.03369955076384068\n",
      "..Epoch 30\n",
      "....Outputs 2.1268 4.6118 4.3216 4.0460 4.6118\n",
      "....Labels  2.9694 5.6991 5.0497 5.3739 5.7012\n",
      "....Loss: 1.0365583604759292\n",
      "....R2: -0.025733929924308452\n",
      "..Epoch 31\n",
      "....Outputs 4.0527 4.6545 4.4154 4.6545 2.0950\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 1.0098522156530685\n",
      "....R2: 0.026439810236142502\n",
      "..Epoch 32\n",
      "....Outputs 4.7562 4.5555 4.7562 2.0996 4.1178\n",
      "....Labels  5.7012 5.0497 5.6991 2.9694 5.3739\n",
      "....Loss: 0.9338850580408317\n",
      "....R2: 0.16740451592750571\n",
      "..Epoch 33\n",
      "....Outputs 4.9086 4.2337 4.7348 2.1358 4.9086\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "....Loss: 0.8181928223895837\n",
      "....R2: 0.36091510065974053\n",
      "..Epoch 34\n",
      "....Outputs 2.1978 4.9443 5.1026 5.1026 4.3922\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "....Loss: 0.6759368849569443\n",
      "....R2: 0.563826308446634\n",
      "..Epoch 35\n",
      "....Outputs 2.2805 5.3268 5.1722 5.3268 4.5841\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "....Loss: 0.527662251680925\n",
      "....R2: 0.7341973813829212\n",
      "..Epoch 36\n",
      "....Outputs 5.5701 4.7984 2.3781 5.5701 5.4033\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 0.409812110758741\n",
      "....R2: 0.839669267345576\n",
      "..Epoch 37\n",
      "....Outputs 5.0203 5.8125 5.8125 5.6192 2.4830\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "....Loss: 0.3771536101229026\n",
      "....R2: 0.8642050176552158\n",
      "..Epoch 38\n",
      "....Outputs 6.0330 5.2339 6.0330 2.5872 5.7999\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "....Loss: 0.43588266130304976\n",
      "....R2: 0.8186212715101856\n",
      "..Epoch 39\n",
      "....Outputs 6.2105 5.4244 5.9271 6.2105 2.6834\n",
      "....Labels  5.7012 5.3739 5.0497 5.6991 2.9694\n",
      "....Loss: 0.5243911880249628\n",
      "....R2: 0.7374826918167596\n",
      "..Epoch 40\n",
      "....Outputs 5.9880 6.3295 6.3295 2.7627 5.5719\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 0.5923538356879318\n",
      "....R2: 0.6650271608890641\n",
      "..Epoch 41\n",
      "....Outputs 2.8192 6.3797 6.3797 5.9781 5.6645\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 0.6152078774210783\n",
      "....R2: 0.6386808738259417\n",
      "..Epoch 42\n",
      "....Outputs 2.8514 6.3624 5.7012 6.3624 5.9023\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "....Loss: 0.5874103599502376\n",
      "....R2: 0.6705948491418257\n",
      "..Epoch 43\n",
      "....Outputs 6.2856 5.7726 6.2856 2.8608 5.6874\n",
      "....Labels  5.7012 5.0497 5.6991 2.9694 5.3739\n",
      "....Loss: 0.5134432613910938\n",
      "....R2: 0.7483296176133878\n",
      "..Epoch 44\n",
      "....Outputs 6.1669 6.1669 5.6370 5.6054 2.8520\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 0.4067964729730544\n",
      "....R2: 0.8420202013770374\n",
      "..Epoch 45\n",
      "....Outputs 2.8314 5.4186 5.5598 6.0214 6.0214\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 0.28145683153816775\n",
      "....R2: 0.9243740608383282\n",
      "..Epoch 46\n",
      "....Outputs 5.8651 5.4695 2.8054 5.2289 5.8651\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "....Loss: 0.15655958381381085\n",
      "....R2: 0.9766005030694587\n",
      "..Epoch 47\n",
      "....Outputs 2.7805 5.0505 5.7134 5.7134 5.3789\n",
      "....Labels  2.9694 5.0497 5.6991 5.7012 5.3739\n",
      "....Loss: 0.08490780835394113\n",
      "....R2: 0.9931175588885507\n",
      "Verified that a small batch can be overfit since the R2 was greater than 0.99\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x) # Spoiler! The \"bug\" is here.\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "..Epoch 0\n",
      "....Outputs 0.0960 0.1009 0.0976 0.0959 0.0960\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "....Loss: 4.967861455966612\n",
      "....R2: -22.56058749155358\n",
      "..Epoch 1\n",
      "....Outputs 0.4603 0.4596 0.4610 0.4603 0.4656\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 4.612118489684607\n",
      "....R2: -19.307108964786604\n",
      "..Epoch 2\n",
      "....Outputs 0.8247 0.8262 0.8318 0.8262 0.8257\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 4.25635726278205\n",
      "....R2: -16.29511017505854\n",
      "..Epoch 3\n",
      "....Outputs 1.1943 1.2021 1.1937 1.1962 1.1962\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "....Loss: 3.8985215294598055\n",
      "....R2: -13.509319622212006\n",
      "..Epoch 4\n",
      "....Outputs 1.5791 1.5731 1.5694 1.5731 1.5693\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "....Loss: 3.5366168803582694\n",
      "....R2: -10.940519153596973\n",
      "..Epoch 5\n",
      "....Outputs 1.9537 1.9596 1.9658 1.9542 1.9596\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "....Loss: 3.169038152289894\n",
      "....R2: -8.58742797416274\n",
      "..Epoch 6\n",
      "....Outputs 2.3578 2.3503 2.3492 2.3641 2.3578\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "....Loss: 2.7955982554755727\n",
      "....R2: -6.460994034218278\n",
      "..Epoch 7\n",
      "....Outputs 2.7686 2.7750 2.7584 2.7686 2.7567\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 2.4184601366102867\n",
      "....R2: -4.583738045746185\n",
      "..Epoch 8\n",
      "....Outputs 3.1755 3.1914 3.1776 3.1914 3.1977\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "....Loss: 2.0434909248047863\n",
      "....R2: -2.986507955522678\n",
      "..Epoch 9\n",
      "....Outputs 3.6300 3.6034 3.6238 3.6238 3.6051\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 1.682830212569815\n",
      "....R2: -1.703508717408753\n",
      "..Epoch 10\n",
      "....Outputs 4.0355 4.0611 4.0358 4.0611 4.0671\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 1.3597971381327747\n",
      "....R2: -0.7652060017334563\n",
      "..Epoch 11\n",
      "....Outputs 4.4607 4.4959 4.5014 4.4959 4.4650\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.1160294726523736\n",
      "....R2: -0.18904556969473707\n",
      "..Epoch 12\n",
      "....Outputs 4.9171 4.9171 4.8803 4.9219 4.8699\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "....Loss: 1.0070958509654984\n",
      "....R2: 0.03174716575486525\n",
      "..Epoch 13\n",
      "....Outputs 5.2668 5.3101 5.3101 5.2488 5.3136\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 1.0536261976106052\n",
      "....R2: -0.05979116053787359\n",
      "..Epoch 14\n",
      "....Outputs 5.6567 5.6069 5.6587 5.5801 5.6567\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 1.200926071696285\n",
      "....R2: -0.37682814505300977\n",
      "..Epoch 15\n",
      "....Outputs 5.8470 5.9392 5.9392 5.9392 5.8829\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.3717555435065871\n",
      "....R2: -0.79639009310764\n",
      "..Epoch 16\n",
      "....Outputs 6.0376 6.1446 6.1425 6.1446 6.0825\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "....Loss: 1.5143530315926412\n",
      "....R2: -1.1892806459561651\n",
      "..Epoch 17\n",
      "....Outputs 6.2690 6.1486 6.2645 6.2690 6.2018\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 1.6046634334256402\n",
      "....R2: -1.4581880928344138\n",
      "..Epoch 18\n",
      "....Outputs 6.3164 6.1843 6.3094 6.2449 6.3164\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 1.6369872680942603\n",
      "....R2: -1.5582194961778422\n",
      "..Epoch 19\n",
      "....Outputs 6.2212 6.2962 6.2868 6.2962 6.1540\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "....Loss: 1.616064372720949\n",
      "....R2: -1.493242363358093\n",
      "..Epoch 20\n",
      "....Outputs 6.0697 6.1425 6.2085 6.2202 6.2202\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 1.551991641346554\n",
      "....R2: -1.2994603505412257\n",
      "..Epoch 21\n",
      "....Outputs 6.0872 6.1010 6.0212 6.1010 5.9435\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 1.4573489090877358\n",
      "....R2: -1.0275623808254042\n",
      "..Epoch 22\n",
      "....Outputs 5.9509 5.8694 5.7875 5.9509 5.9351\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "....Loss: 1.345685363563733\n",
      "....R2: -0.7287582806982291\n",
      "..Epoch 23\n",
      "....Outputs 5.7812 5.7812 5.7634 5.6983 5.6127\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "....Loss: 1.2306597364646579\n",
      "....R2: -0.44584985003685107\n",
      "..Epoch 24\n",
      "....Outputs 5.6018 5.5177 5.4289 5.6018 5.5824\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "....Loss: 1.125260005151305\n",
      "....R2: -0.2087958179601639\n",
      "..Epoch 25\n",
      "....Outputs 5.4216 5.2445 5.3363 5.4216 5.4005\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "....Loss: 1.0406045012416847\n",
      "....R2: -0.03375726580915317\n",
      "..Epoch 26\n",
      "....Outputs 5.2476 5.0666 5.2250 5.2476 5.1611\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 0.984091248563267\n",
      "....R2: 0.07547656580158124\n",
      "..Epoch 27\n",
      "....Outputs 4.9979 5.0615 4.9005 5.0856 5.0856\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "....Loss: 0.9575498460389573\n",
      "....R2: 0.12467367191720768\n",
      "..Epoch 28\n",
      "....Outputs 4.9399 4.7504 4.8507 4.9399 4.9144\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 0.9569266125832833\n",
      "....R2: 0.12581279331159312\n",
      "..Epoch 29\n",
      "....Outputs 4.7226 4.7867 4.6192 4.8135 4.8135\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "....Loss: 0.9742587267176727\n",
      "....R2: 0.09385895651353193\n",
      "..Epoch 30\n",
      "....Outputs 4.6801 4.6154 4.7083 4.7083 4.5086\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 1.0006584741353195\n",
      "....R2: 0.044085805176937165\n",
      "..Epoch 31\n",
      "....Outputs 4.6253 4.5957 4.4195 4.6253 4.5300\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 1.028501778990042\n",
      "....R2: -0.009851019747165513\n",
      "..Epoch 32\n",
      "....Outputs 4.3517 4.5644 4.4666 4.5644 4.5336\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 1.0522865942575543\n",
      "....R2: -0.057098079839926985\n",
      "..Epoch 33\n",
      "....Outputs 4.3048 4.5254 4.4932 4.5254 4.4246\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 1.0686019259487405\n",
      "....R2: -0.09013201692220196\n",
      "..Epoch 34\n",
      "....Outputs 4.5073 4.4030 4.5073 4.2777 4.4737\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "....Loss: 1.0757633271452316\n",
      "....R2: -0.10479236552420845\n",
      "..Epoch 35\n",
      "....Outputs 4.5085 4.4735 4.2691 4.5085 4.4006\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "....Loss: 1.073417614526367\n",
      "....R2: -0.09997950021196411\n",
      "..Epoch 36\n",
      "....Outputs 4.2771 4.5276 4.5276 4.4911 4.4155\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 1.0622127369023118\n",
      "....R2: -0.07713514107067376\n",
      "..Epoch 37\n",
      "....Outputs 4.4460 4.5245 4.5625 4.2999 4.5625\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "....Loss: 1.0435509931701348\n",
      "....R2: -0.03961973460790569\n",
      "..Epoch 38\n",
      "....Outputs 4.6110 4.3354 4.5715 4.4898 4.6110\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "....Loss: 1.0193873044410984\n",
      "....R2: 0.007968190478933157\n",
      "..Epoch 39\n",
      "....Outputs 4.6299 4.5448 4.6709 4.6709 4.3813\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 0.992050281557803\n",
      "....R2: 0.06046147502941146\n",
      "..Epoch 40\n",
      "....Outputs 4.7396 4.7396 4.6971 4.6083 4.4352\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 0.9640384355753514\n",
      "....R2: 0.11277067601841428\n",
      "..Epoch 41\n",
      "....Outputs 4.6779 4.4944 4.8144 4.8144 4.7704\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "....Loss: 0.9377810374864188\n",
      "....R2: 0.16044326554564725\n",
      "..Epoch 42\n",
      "....Outputs 4.5565 4.7508 4.8927 4.8927 4.8472\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "....Loss: 0.9153560061028047\n",
      "....R2: 0.20011551782628745\n",
      "..Epoch 43\n",
      "....Outputs 4.9248 4.9718 4.9718 4.6187 4.8244\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "....Loss: 0.8982010903509194\n",
      "....R2: 0.22981625048153642\n",
      "..Epoch 44\n",
      "....Outputs 4.6786 5.0489 5.0489 5.0004 4.8961\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 0.8868917220880018\n",
      "....R2: 0.2490890872660474\n",
      "..Epoch 45\n",
      "....Outputs 5.1215 4.9632 4.7337 5.0715 5.1215\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "....Loss: 0.8810746972245804\n",
      "....R2: 0.2589070801770629\n",
      "..Epoch 46\n",
      "....Outputs 4.7819 5.1360 5.0237 5.1874 5.1874\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "....Loss: 0.8795891715295255\n",
      "....R2: 0.26140399982317497\n",
      "..Epoch 47\n",
      "....Outputs 5.1917 5.0756 4.8213 5.2446 5.2446\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "....Loss: 0.8807525569998844\n",
      "....R2: 0.2594489823492452\n",
      "..Epoch 48\n",
      "....Outputs 5.2916 5.2916 4.8506 5.1173 5.2373\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "....Loss: 0.882719321945923\n",
      "....R2: 0.2561378376772363\n",
      "..Epoch 49\n",
      "....Outputs 5.3275 4.8688 5.1480 5.3275 5.2717\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "....Loss: 0.883815027350806\n",
      "....R2: 0.25429006099922247\n",
      "..Epoch 50\n",
      "....Outputs 5.3518 5.2946 4.8756 5.3518 5.1671\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "....Loss: 0.8827778969793904\n",
      "....R2: 0.2560391933532138\n",
      "..Epoch 51\n",
      "....Outputs 4.8712 5.3646 5.1749 5.3646 5.3059\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 0.8788876340801939\n",
      "....R2: 0.2625817177519435\n",
      "..Epoch 52\n",
      "....Outputs 5.1718 5.3666 5.3666 5.3063 4.8561\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "....Loss: 0.8719851120084692\n",
      "....R2: 0.27411923811759487\n",
      "..Epoch 53\n",
      "....Outputs 5.3588 4.8316 5.1590 5.3588 5.2970\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "....Loss: 0.8624043909237995\n",
      "....R2: 0.28998241920297085\n",
      "..Epoch 54\n",
      "....Outputs 5.2792 4.7990 5.1378 5.3426 5.3426\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "....Loss: 0.850848922630265\n",
      "....R2: 0.30888213161523415\n",
      "..Epoch 55\n",
      "....Outputs 5.3199 5.2548 5.3199 5.1101 4.7599\n",
      "....Labels  5.7012 5.3739 5.6991 5.0497 2.9694\n",
      "....Loss: 0.8382254333969179\n",
      "....R2: 0.3292373882930053\n",
      "..Epoch 56\n",
      "....Outputs 5.2256 5.2923 5.2923 5.0775 4.7163\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "....Loss: 0.8254734992447197\n",
      "....R2: 0.34949072554159244\n",
      "..Epoch 57\n",
      "....Outputs 5.2619 5.1936 4.6698 5.0421 5.2619\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "....Loss: 0.8134061821834174\n",
      "....R2: 0.36837091474830674\n",
      "..Epoch 58\n",
      "....Outputs 4.6223 5.0056 5.1604 5.2304 5.2304\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 0.8025917613370189\n",
      "....R2: 0.38505456682734873\n",
      "..Epoch 59\n",
      "....Outputs 5.1997 4.9698 5.1997 4.5753 5.1281\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "....Loss: 0.7932887441729334\n",
      "....R2: 0.3992278871826729\n",
      "..Epoch 60\n",
      "....Outputs 5.1713 5.1713 4.9361 5.0979 4.5303\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "....Loss: 0.7854485797891338\n",
      "....R2: 0.41104418590712166\n",
      "..Epoch 61\n",
      "....Outputs 5.1465 5.0714 5.1465 4.9059 4.4883\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "....Loss: 0.7787692547372574\n",
      "....R2: 0.42101834756208334\n",
      "..Epoch 62\n",
      "....Outputs 5.0494 4.4504 5.1263 5.1263 4.8802\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "....Loss: 0.7727887679303608\n",
      "....R2: 0.4298766989373952\n",
      "..Epoch 63\n",
      "....Outputs 5.1115 4.4171 4.8597 5.0329 5.1115\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "....Loss: 0.7669890961027391\n",
      "....R2: 0.4384019614757564\n",
      "..Epoch 64\n",
      "....Outputs 5.0221 5.1025 5.1025 4.8449 4.3888\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "....Loss: 0.7608913133457006\n",
      "....R2: 0.44729622083891074\n",
      "..Epoch 65\n",
      "....Outputs 4.8359 5.0995 4.3655 5.0995 5.0174\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.754127707098565\n",
      "....R2: 0.4570785641765577\n",
      "..Epoch 66\n",
      "....Outputs 5.1025 4.3472 5.1025 4.8326 5.0185\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "....Loss: 0.7464796336615803\n",
      "....R2: 0.46803497623589263\n",
      "..Epoch 67\n",
      "....Outputs 4.3334 5.1110 4.8348 5.1110 5.0253\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "....Loss: 0.7378889383727911\n",
      "....R2: 0.48020847547989887\n",
      "..Epoch 68\n",
      "....Outputs 4.8418 5.1246 4.3237 5.1246 5.0371\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.7284437456030975\n",
      "....R2: 0.4934302863799591\n",
      "..Epoch 69\n",
      "....Outputs 4.8530 5.0533 5.1425 5.1425 4.3173\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 0.7183405497720265\n",
      "....R2: 0.507384597478441\n",
      "..Epoch 70\n",
      "....Outputs 5.1640 5.1640 4.8676 4.3135 5.0729\n",
      "....Labels  5.7012 5.6991 5.0497 2.9694 5.3739\n",
      "....Loss: 0.7078411458599799\n",
      "....R2: 0.5216797378599101\n",
      "..Epoch 71\n",
      "....Outputs 4.8846 4.3114 5.1880 5.0951 5.1880\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "....Loss: 0.697217410688613\n",
      "....R2: 0.5359298328641495\n",
      "..Epoch 72\n",
      "....Outputs 5.2135 4.9031 5.2135 4.3102 5.1189\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "....Loss: 0.6867071044420492\n",
      "....R2: 0.5498157986514478\n",
      "..Epoch 73\n",
      "....Outputs 5.1432 5.2396 5.2396 4.3090 4.9220\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "....Loss: 0.6764751466032873\n",
      "....R2: 0.5631313748289979\n",
      "..Epoch 74\n",
      "....Outputs 5.2654 4.3070 5.2654 4.9405 5.1672\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "....Loss: 0.6665944020523531\n",
      "....R2: 0.5758001308007352\n",
      "..Epoch 75\n",
      "....Outputs 4.3034 5.2899 5.2899 5.1899 4.9578\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 0.6570437717038334\n",
      "....R2: 0.5878684947651321\n",
      "..Epoch 76\n",
      "....Outputs 4.9731 4.2978 5.2106 5.3125 5.3125\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "....Loss: 0.6477244259575521\n",
      "....R2: 0.599476745172237\n",
      "..Epoch 77\n",
      "....Outputs 4.2896 5.3326 4.9858 5.3326 5.2288\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 0.6384901202265685\n",
      "....R2: 0.6108154437653239\n",
      "..Epoch 78\n",
      "....Outputs 5.3496 4.9956 5.2440 5.3496 4.2785\n",
      "....Labels  5.6991 5.0497 5.3739 5.7012 2.9694\n",
      "....Loss: 0.6291810850887084\n",
      "....R2: 0.6220811961824907\n",
      "..Epoch 79\n",
      "....Outputs 5.0021 5.3635 4.2644 5.3635 5.2560\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.6196569461956151\n",
      "....R2: 0.6334359849930644\n",
      "..Epoch 80\n",
      "....Outputs 5.3742 5.0054 5.2647 4.2473 5.3742\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 0.6098224513219457\n",
      "....R2: 0.6449790328138523\n",
      "..Epoch 81\n",
      "....Outputs 5.2702 5.0056 5.3817 4.2273 5.3817\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "....Loss: 0.5996400657528143\n",
      "....R2: 0.6567358144343186\n",
      "..Epoch 82\n",
      "....Outputs 5.0030 5.3864 5.3864 4.2048 5.2729\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 0.5891300709063243\n",
      "....R2: 0.6686632807583603\n",
      "..Epoch 83\n",
      "....Outputs 5.2731 4.9979 5.3886 4.1800 5.3886\n",
      "....Labels  5.3739 5.0497 5.7012 2.9694 5.6991\n",
      "....Loss: 0.5783594274772055\n",
      "....R2: 0.6806677020616605\n",
      "..Epoch 84\n",
      "....Outputs 4.9909 5.3890 4.1536 5.2714 5.3890\n",
      "....Labels  5.0497 5.7012 2.9694 5.3739 5.6991\n",
      "....Loss: 0.5674239650909993\n",
      "....R2: 0.692629238484385\n",
      "..Epoch 85\n",
      "....Outputs 5.3879 4.1259 5.2683 5.3879 4.9825\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 0.5564250595568528\n",
      "....R2: 0.704429862049881\n",
      "..Epoch 86\n",
      "....Outputs 5.3861 4.0975 5.2643 4.9734 5.3861\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 0.5454513274715607\n",
      "....R2: 0.7159732883576753\n",
      "..Epoch 87\n",
      "....Outputs 5.3841 5.3841 5.2602 4.0688 4.9641\n",
      "....Labels  5.7012 5.6991 5.3739 2.9694 5.0497\n",
      "....Loss: 0.5345630560240613\n",
      "....R2: 0.7271995534010516\n",
      "..Epoch 88\n",
      "....Outputs 5.3824 5.3824 5.2564 4.9551 4.0404\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "....Loss: 0.5237848922560068\n",
      "....R2: 0.7380893775480186\n",
      "..Epoch 89\n",
      "....Outputs 5.2533 5.3815 4.0126 5.3815 4.9469\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "....Loss: 0.5131070182428114\n",
      "....R2: 0.7486591285611888\n",
      "..Epoch 90\n",
      "....Outputs 5.3818 5.3818 5.2515 4.9400 3.9856\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 0.5024940752277962\n",
      "....R2: 0.7589489266856049\n",
      "..Epoch 91\n",
      "....Outputs 5.2512 5.3836 5.3836 4.9345 3.9599\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "....Loss: 0.4918948317605048\n",
      "....R2: 0.7690107751646336\n",
      "..Epoch 92\n",
      "....Outputs 4.9308 5.2525 5.3870 5.3870 3.9353\n",
      "....Labels  5.0497 5.3739 5.7012 5.6991 2.9694\n",
      "....Loss: 0.48125858051527776\n",
      "....R2: 0.7788921467779588\n",
      "..Epoch 93\n",
      "....Outputs 5.2556 3.9121 5.3922 4.9288 5.3922\n",
      "....Labels  5.3739 2.9694 5.7012 5.0497 5.6991\n",
      "....Loss: 0.47054519219804575\n",
      "....R2: 0.7886268201350339\n",
      "..Epoch 94\n",
      "....Outputs 4.9286 5.3991 3.8902 5.3991 5.2605\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.4597341055100927\n",
      "....R2: 0.7982281088741685\n",
      "..Epoch 95\n",
      "....Outputs 3.8695 5.4075 4.9301 5.2669 5.4075\n",
      "....Labels  2.9694 5.6991 5.0497 5.3739 5.7012\n",
      "....Loss: 0.4488264942056562\n",
      "....R2: 0.8076889763555616\n",
      "..Epoch 96\n",
      "....Outputs 3.8498 5.4174 5.4174 4.9330 5.2747\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 0.43784516206573126\n",
      "....R2: 0.8169843146653664\n",
      "..Epoch 97\n",
      "....Outputs 3.8310 5.4283 4.9371 5.2837 5.4283\n",
      "....Labels  2.9694 5.7012 5.0497 5.3739 5.6991\n",
      "....Loss: 0.4268271697575124\n",
      "....R2: 0.8260792840874727\n",
      "..Epoch 98\n",
      "....Outputs 5.4400 5.2934 3.8127 4.9421 5.4400\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "....Loss: 0.4158168192331947\n",
      "....R2: 0.8349364094031608\n",
      "..Epoch 99\n",
      "....Outputs 5.3036 4.9477 5.4522 5.4522 3.7948\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "....Loss: 0.40485691589477846\n",
      "....R2: 0.8435230688109571\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The maximum R2 over 100 epochs was not greater than 0.99",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3d84f2f421bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 193\u001b[0;31m                                of data. The maximum R2 over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not greater than {k.DL_DBG_SUFFICIENT_R2_SMALL_BATCH}''')\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the R2 was greater than {k.DL_DBG_SUFFICIENT_R2_SMALL_BATCH}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The maximum R2 over 100 epochs was not greater than 0.99"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not mix information from separate instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graphnet.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph_batch2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# data to illustrate the point\n",
    "\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "\n",
    "data_set_ = train_X_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=2)\n",
    "        x = x - x_mean[:, :, None] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.3991 -0.3810 -0.3220 -0.3801\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.3991156816482544\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0) # Spoiler! this is the bug.\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.1933 -0.1732 -0.1948 -0.1743\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.19329825043678284\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Data is getting mixed between instances in the same batch.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-64c781c65389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_chart_dependencies=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest_model_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_chart_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_choose_model_size_by_overfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mchart_dependencies\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# print(data.x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is getting mixed between instances in the same batch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished charting dependencies. Data is not getting mixed between instances in the same batch.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is getting mixed between instances in the same batch."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data\n",
    "The capacity of your architecture should be just large enough to overfit the training data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# this cell should return the integer 1\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.612570724896483 [r2] -7.962049069402992\n",
      "......Outputs -0.0134 0.0056 0.0020 0.0056 -0.0062 0.0249 0.0047 0.0048 0.0136 -0.0035\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.612570724896483 [best r2] -7.962049069402992\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.700185972197337 [r2] -4.767244261588079\n",
      "......Outputs 0.4713 0.6923 0.5647 0.6923 0.6992 0.9448 0.5743 0.6922 0.5705 0.7246\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.700185972197337 [best r2] -4.767244261588079\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.662060907978595 [r2] -1.9850885368496551\n",
      "......Outputs 2.4216 3.8171 2.9447 3.8171 3.8818 5.0764 3.0276 3.7627 2.8387 4.1700\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.662060907978595 [best r2] -1.9850885368496551\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.917622249133612 [r2] -0.5489855299241717\n",
      "......Outputs 3.7666 6.7800 4.1433 6.7800 7.0142 7.2724 5.5517 6.0386 4.1660 5.5382\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.917622249133612 [best r2] -0.5489855299241717\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.816375127743182 [r2] -0.3897360825934004\n",
      "......Outputs 2.4769 4.9367 2.2778 4.9367 5.2034 4.1900 4.3693 3.9452 2.5403 2.7541\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.816375127743182 [best r2] -0.3897360825934004\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4936885897274133 [r2] 0.06018717393353634\n",
      "......Outputs 2.7516 5.9816 2.1272 5.9816 6.3541 4.0580 5.5970 4.3998 2.6101 2.4037\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4936885897274133 [best r2] 0.06018717393353634\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.389373341714013 [r2] 0.18687153559796832\n",
      "......Outputs 3.4819 7.5349 2.3861 7.5349 8.0204 4.5329 7.4332 5.5612 3.0812 2.6824\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.389373341714013 [best r2] 0.18687153559796832\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.225120302647218 [r2] 0.36776473708918356\n",
      "......Outputs 3.2763 6.1955 2.0956 6.1955 6.5390 3.6519 6.7372 4.9646 2.6588 2.4229\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.225120302647218 [best r2] 0.36776473708918356\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0615929938977864 [r2] 0.5252802193998929\n",
      "......Outputs 3.7441 5.8854 2.3449 5.8854 6.2014 3.6093 6.9631 5.3067 2.7551 2.8301\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0615929938977864 [best r2] 0.5252802193998929\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9455229771444711 [r2] 0.6234129208997812\n",
      "......Outputs 4.5924 6.0785 2.8576 6.0785 6.3823 3.8422 7.6277 6.0020 3.0336 3.5022\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9455229771444711 [best r2] 0.6234129208997812\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.82824427326015 [r2] 0.7110397367357976\n",
      "......Outputs 4.6207 5.4600 2.8053 5.4600 5.6387 3.3658 7.1695 5.3843 2.6965 3.2817\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.82824427326015 [best r2] 0.7110397367357976\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.734717623580045 [r2] 0.7726148201340741\n",
      "......Outputs 5.1516 5.6801 3.0957 5.6801 5.7789 3.3369 7.4896 5.4638 2.6738 3.3238\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.734717623580045 [best r2] 0.7726148201340741\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6515646769123894 [r2] 0.8211716765961633\n",
      "......Outputs 5.2966 5.6203 3.2040 5.6203 5.6067 3.1688 7.3336 5.2268 2.5289 3.1701\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6515646769123894 [best r2] 0.8211716765961633\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.5848437092934777 [r2] 0.8559209332714756\n",
      "......Outputs 5.3711 5.6068 3.3105 5.6068 5.4955 3.0516 7.1430 5.1135 2.4368 3.0986\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5848437092934777 [best r2] 0.8559209332714756\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.525377210043242 [r2] 0.8837310633219779\n",
      "......Outputs 5.4238 5.7349 3.3769 5.7349 5.5902 3.0089 7.0908 5.1319 2.3946 3.1338\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.525377210043242 [best r2] 0.8837310633219779\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.47373543045859867 [r2] 0.9054649247719143\n",
      "......Outputs 5.2499 5.6641 3.2561 5.6641 5.6254 2.8976 6.9008 4.9884 2.3024 3.1359\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.47373543045859867 [best r2] 0.9054649247719143\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.429087646637449 [r2] 0.9224443818378368\n",
      "......Outputs 5.2014 5.6976 3.2224 5.6976 5.7826 2.8677 6.8359 5.0400 2.2747 3.2596\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.429087646637449 [best r2] 0.9224443818378368\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.3890475409634053 [r2] 0.9362431884375152\n",
      "......Outputs 5.0830 5.5985 3.1313 5.5985 5.7460 2.8064 6.6793 4.9835 2.2123 3.2973\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3890475409634053 [best r2] 0.9362431884375152\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.35375432522677913 [r2] 0.9472861473908468\n",
      "......Outputs 5.0640 5.6597 3.0863 5.6597 5.7724 2.7804 6.6556 5.0459 2.1746 3.3181\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.35375432522677913 [best r2] 0.9472861473908468\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3221132437714617 [r2] 0.9562942674813023\n",
      "......Outputs 5.0196 5.6479 3.0236 5.6479 5.6946 2.7400 6.5896 5.0228 2.1334 3.2890\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3221132437714617 [best r2] 0.9562942674813023\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.2949909950848845 [r2] 0.9633445328539412\n",
      "......Outputs 5.0307 5.6820 3.0115 5.6820 5.6594 2.7323 6.5893 5.0439 2.1240 3.2918\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2949909950848845 [best r2] 0.9633445328539412\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.27119424175126683 [r2] 0.9690199461179154\n",
      "......Outputs 5.0231 5.6914 2.9854 5.6914 5.6149 2.7183 6.5902 5.0463 2.1128 3.2790\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.27119424175126683 [best r2] 0.9690199461179154\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.2508537461853753 [r2] 0.9734928873970639\n",
      "......Outputs 5.0246 5.7057 2.9602 5.7057 5.5859 2.7110 6.6035 5.0744 2.1066 3.2789\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2508537461853753 [best r2] 0.9734928873970639\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.23320053805148686 [r2] 0.9770923606505634\n",
      "......Outputs 5.0308 5.7184 2.9346 5.7184 5.5646 2.7072 6.6255 5.1017 2.1045 3.2787\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.23320053805148686 [best r2] 0.9770923606505634\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.2179870196771307 [r2] 0.9799837596004356\n",
      "......Outputs 5.0296 5.7079 2.9116 5.7079 5.5352 2.7165 6.6431 5.1058 2.0990 3.2720\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2179870196771307 [best r2] 0.9799837596004356\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.20480762937386732 [r2] 0.9823309376913419\n",
      "......Outputs 5.0368 5.7080 2.9023 5.7080 5.5066 2.7502 6.6668 5.1174 2.0999 3.2723\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20480762937386732 [best r2] 0.9823309376913419\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.19320034943053904 [r2] 0.9842769405688445\n",
      "......Outputs 5.0341 5.7140 2.8876 5.7140 5.4766 2.7842 6.6848 5.1290 2.0994 3.2698\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19320034943053904 [best r2] 0.9842769405688445\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.18304942878072686 [r2] 0.9858857437871312\n",
      "......Outputs 5.0330 5.7212 2.8816 5.7212 5.4695 2.8139 6.7039 5.1436 2.1020 3.2658\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18304942878072686 [best r2] 0.9858857437871312\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1739165094829875 [r2] 0.987259019181252\n",
      "......Outputs 5.0349 5.7236 2.8815 5.7236 5.4580 2.8357 6.7157 5.1526 2.1079 3.2615\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1739165094829875 [best r2] 0.987259019181252\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16559139093501843 [r2] 0.9884496073217659\n",
      "......Outputs 5.0415 5.7337 2.8756 5.7337 5.4473 2.8499 6.7175 5.1493 2.1096 3.2554\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16559139093501843 [best r2] 0.9884496073217659\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.1580086324025285 [r2] 0.9894832180575353\n",
      "......Outputs 5.0460 5.7408 2.8813 5.7408 5.4386 2.8609 6.7141 5.1535 2.1163 3.2526\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1580086324025285 [best r2] 0.9894832180575353\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.15107585684710703 [r2] 0.9903858393886542\n",
      "......Outputs 5.0482 5.7323 2.8866 5.7323 5.4337 2.8613 6.7089 5.1521 2.1205 3.2422\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15107585684710703 [best r2] 0.9903858393886542\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.14453851919958527 [r2] 0.9911998830665341\n",
      "......Outputs 5.0485 5.7289 2.8907 5.7289 5.4220 2.8578 6.7034 5.1495 2.1194 3.2358\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14453851919958527 [best r2] 0.9911998830665341\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.13844337432168302 [r2] 0.99192643041496\n",
      "......Outputs 5.0466 5.7313 2.8938 5.7313 5.4179 2.8579 6.7021 5.1524 2.1175 3.2290\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13844337432168302 [best r2] 0.99192643041496\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.13277024467730686 [r2] 0.9925745504444003\n",
      "......Outputs 5.0469 5.7403 2.9014 5.7403 5.4170 2.8608 6.7063 5.1609 2.1196 3.2258\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13277024467730686 [best r2] 0.9925745504444003\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.12745610583712277 [r2] 0.9931570631584685\n",
      "......Outputs 5.0438 5.7443 2.9100 5.7443 5.4122 2.8603 6.7055 5.1614 2.1221 3.2198\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12745610583712277 [best r2] 0.9931570631584685\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.12227812473269917 [r2] 0.9937017661700718\n",
      "......Outputs 5.0375 5.7283 2.9141 5.7283 5.3988 2.8564 6.6966 5.1542 2.1214 3.2104\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12227812473269917 [best r2] 0.9937017661700718\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.11738826183677765 [r2] 0.9941954228878314\n",
      "......Outputs 5.0340 5.7278 2.9158 5.7278 5.3908 2.8559 6.6976 5.1591 2.1193 3.2044\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11738826183677765 [best r2] 0.9941954228878314\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.11312453033432392 [r2] 0.994609428397163\n",
      "......Outputs 5.0506 5.7511 2.9289 5.7511 5.4004 2.8661 6.7145 5.1739 2.1282 3.2045\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11312453033432392 [best r2] 0.994609428397163\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.10944935488633138 [r2] 0.9949539952722799\n",
      "......Outputs 5.0625 5.7527 2.9413 5.7527 5.4067 2.8720 6.7155 5.1679 2.1365 3.2014\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10944935488633138 [best r2] 0.9949539952722799\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.10558716923285401 [r2] 0.9953038329799332\n",
      "......Outputs 5.0321 5.7067 2.9241 5.7067 5.3809 2.8515 6.6845 5.1300 2.1177 3.1842\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10558716923285401 [best r2] 0.9953038329799332\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.1042143240818461 [r2] 0.9954251582788638\n",
      "......Outputs 5.0056 5.7063 2.9067 5.7063 5.3508 2.8463 6.6792 5.1268 2.0926 3.1713\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1042143240818461 [best r2] 0.9954251582788638\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.10410014098360348 [r2] 0.9954351776967792\n",
      "......Outputs 5.0638 5.7932 2.9502 5.7932 5.3968 2.8878 6.7467 5.2050 2.1197 3.1915\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.11561073242741556 [r2] 0.9943698813669831\n",
      "......Outputs 5.1305 5.8063 3.0098 5.8063 5.4710 2.9131 6.7753 5.2444 2.1706 3.2112\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.11939525813358469 [r2] 0.9939952435333599\n",
      "......Outputs 5.0059 5.6332 2.9373 5.6332 5.3688 2.8354 6.6467 5.0784 2.1282 3.1620\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.15411672193864664 [r2] 0.9899949153626412\n",
      "......Outputs 4.8581 5.5867 2.8176 5.5867 5.2203 2.7731 6.5731 4.9690 2.0211 3.1129\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.16312699524207172 [r2] 0.9887908439783327\n",
      "......Outputs 5.0700 5.8963 2.9386 5.8963 5.3760 2.9041 6.8127 5.2607 2.0820 3.1943\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.21924506443527114 [r2] 0.9797520578277934\n",
      "......Outputs 5.3501 5.9983 3.1384 5.9983 5.6744 3.0351 6.9696 5.4582 2.2343 3.2704\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.2004936580985175 [r2] 0.9830674440033895\n",
      "......Outputs 4.9458 5.4905 2.9327 5.4905 5.3116 2.8015 6.5012 4.9667 2.1363 3.1037\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.22925381835330558 [r2] 0.977861183707076\n",
      "......Outputs 4.6727 5.4802 2.7517 5.4802 5.0991 2.7118 6.4222 4.8156 1.9986 3.0610\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.19033484319359947 [r2] 0.9847398838860809\n",
      "......Outputs 5.1377 6.0279 3.0124 6.0279 5.5168 2.9971 7.0342 5.3938 2.1321 3.2977\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.16217146208093683 [r2] 0.9889217769449832\n",
      "......Outputs 5.2552 5.7857 3.0692 5.7857 5.5214 2.9810 6.8414 5.3404 2.1592 3.2111\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.13774578662369535 [r2] 0.9920075875457565\n",
      "......Outputs 4.9299 5.5077 2.8918 5.5077 5.2652 2.7540 6.4123 4.9595 2.0666 3.0323\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10410014098360348 [best r2] 0.9954351776967792\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.1036520652710585 [r2] 0.9954743896331739\n",
      "......Outputs 5.0143 5.8571 2.9552 5.8571 5.4140 2.8425 6.7196 5.1526 2.1321 3.1890\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1036520652710585 [best r2] 0.9954743896331739\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.09501191697463733 [r2] 0.996197428384894\n",
      "......Outputs 5.1088 5.8339 3.0119 5.8339 5.4543 2.9194 6.8297 5.2483 2.1675 3.2578\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09501191697463733 [best r2] 0.996197428384894\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.0786549765341538 [r2] 0.9973940045534886\n",
      "......Outputs 5.0110 5.5934 2.9396 5.5934 5.3413 2.8334 6.6073 5.1049 2.1015 3.1234\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0786549765341538 [best r2] 0.9973940045534886\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.07072992178796121 [r2] 0.9978926940345879\n",
      "......Outputs 5.0374 5.7243 2.9619 5.7243 5.3814 2.8363 6.6750 5.1647 2.1102 3.1229\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07072992178796121 [best r2] 0.9978926940345879\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.06865817962492464 [r2] 0.9980143357814267\n",
      "......Outputs 5.0852 5.8196 2.9967 5.8196 5.4147 2.8756 6.7691 5.2038 2.1421 3.1866\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06865817962492464 [best r2] 0.9980143357814267\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.0624876544272344 [r2] 0.9983552129159466\n",
      "......Outputs 5.0384 5.7087 2.9615 5.7087 5.3821 2.8581 6.6857 5.1207 2.1223 3.1621\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0624876544272344 [best r2] 0.9983552129159466\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.06053495303266901 [r2] 0.9984564039432497\n",
      "......Outputs 5.0245 5.7017 2.9531 5.7017 5.3779 2.8548 6.6740 5.1341 2.1129 3.1400\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06053495303266901 [best r2] 0.9984564039432497\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.0587581033107399 [r2] 0.9985456907117461\n",
      "......Outputs 5.0574 5.7681 2.9752 5.7681 5.3909 2.8730 6.7310 5.1899 2.1264 3.1506\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0587581033107399 [best r2] 0.9985456907117461\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.0567950057729322 [r2] 0.9986412438058194\n",
      "......Outputs 5.0573 5.7472 2.9730 5.7472 5.3860 2.8692 6.7128 5.1636 2.1258 3.1483\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0567950057729322 [best r2] 0.9986412438058194\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.0552053287005215 [r2] 0.9987162417693809\n",
      "......Outputs 5.0370 5.7140 2.9609 5.7140 5.3747 2.8590 6.6805 5.1354 2.1180 3.1409\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0552053287005215 [best r2] 0.9987162417693809\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.05374400523093071 [r2] 0.9987833061904198\n",
      "......Outputs 5.0430 5.7401 2.9631 5.7401 5.3820 2.8646 6.7065 5.1596 2.1203 3.1436\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05374400523093071 [best r2] 0.9987833061904198\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.053435566675014874 [r2] 0.9987972314057669\n",
      "......Outputs 5.0591 5.7529 2.9734 5.7529 5.3921 2.8729 6.7222 5.1738 2.1255 3.1444\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.053435566675014874 [best r2] 0.9987972314057669\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.052001050555812764 [r2] 0.9988609429508417\n",
      "......Outputs 5.0496 5.7280 2.9719 5.7280 5.3789 2.8663 6.6953 5.1529 2.1221 3.1364\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052001050555812764 [best r2] 0.9988609429508417\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.05092258133342434 [r2] 0.9989076996774839\n",
      "......Outputs 5.0343 5.7255 2.9619 5.7255 5.3670 2.8584 6.6884 5.1433 2.1161 3.1315\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05092258133342434 [best r2] 0.9989076996774839\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.05034517903714127 [r2] 0.9989323300484444\n",
      "......Outputs 5.0483 5.7475 2.9677 5.7475 5.3802 2.8673 6.7136 5.1632 2.1200 3.1394\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05034517903714127 [best r2] 0.9989323300484444\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.05045760602228166 [r2] 0.998927556247161\n",
      "......Outputs 5.0634 5.7470 2.9778 5.7470 5.3919 2.8738 6.7176 5.1713 2.1256 3.1397\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05034517903714127 [best r2] 0.9989323300484444\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.049148377746164755 [r2] 0.9989824878215517\n",
      "......Outputs 5.0456 5.7263 2.9697 5.7263 5.3755 2.8628 6.6919 5.1507 2.1205 3.1274\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.049148377746164755 [best r2] 0.9989824878215517\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.04865633370667166 [r2] 0.9990027592798473\n",
      "......Outputs 5.0299 5.7252 2.9596 5.7252 5.3604 2.8572 6.6869 5.1419 2.1137 3.1253\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04865633370667166 [best r2] 0.9990027592798473\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.048632934096241705 [r2] 0.9990037182272846\n",
      "......Outputs 5.0518 5.7481 2.9693 5.7481 5.3791 2.8701 6.7161 5.1660 2.1190 3.1364\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.048632934096241705 [best r2] 0.9990037182272846\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.04983643358853121 [r2] 0.9989537989485905\n",
      "......Outputs 5.0720 5.7518 2.9828 5.7518 5.3974 2.8775 6.7240 5.1788 2.1280 3.1363\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.048632934096241705 [best r2] 0.9990037182272846\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.04849609586406308 [r2] 0.9990093168054641\n",
      "......Outputs 5.0440 5.7237 2.9708 5.7237 5.3734 2.8623 6.6876 5.1480 2.1214 3.1219\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.049444295605613246 [r2] 0.9989701982410027\n",
      "......Outputs 5.0155 5.7137 2.9515 5.7137 5.3459 2.8519 6.6736 5.1284 2.1080 3.1182\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.049854812840413716 [r2] 0.9989530271462406\n",
      "......Outputs 5.0509 5.7510 2.9660 5.7510 5.3743 2.8708 6.7208 5.1692 2.1154 3.1334\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.055483337805942855 [r2] 0.9987032794291472\n",
      "......Outputs 5.0961 5.7728 2.9960 5.7728 5.4179 2.8879 6.7483 5.2024 2.1360 3.1393\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.05441819797272547 [r2] 0.9987525890460713\n",
      "......Outputs 5.0508 5.7215 2.9799 5.7215 5.3828 2.8641 6.6845 5.1476 2.1284 3.1187\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.06203368365665081 [r2] 0.9983790247534939\n",
      "......Outputs 4.9762 5.6773 2.9305 5.6773 5.3104 2.8345 6.6317 5.0891 2.0944 3.1027\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.0646257304483559 [r2] 0.9982407313220539\n",
      "......Outputs 5.0295 5.7445 2.9443 5.7445 5.3443 2.8634 6.7142 5.1599 2.0988 3.1268\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.08558990275590164 [r2] 0.9969142104301231\n",
      "......Outputs 5.1603 5.8346 3.0257 5.8346 5.4710 2.9186 6.8199 5.2684 2.1552 3.1559\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.093289982788526 [r2] 0.9963340100775997\n",
      "......Outputs 5.1022 5.7479 3.0236 5.7479 5.4490 2.8878 6.7134 5.1824 2.1628 3.1295\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.11892651046588888 [r2] 0.994042300515753\n",
      "......Outputs 4.8766 5.5776 2.8885 5.5776 5.2373 2.7917 6.5177 4.9856 2.0701 3.0721\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.14506406482308695 [r2] 0.9911357718382712\n",
      "......Outputs 4.9024 5.6618 2.8555 5.6618 5.2086 2.8130 6.6244 5.0655 2.0331 3.0948\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.18501388058809326 [r2] 0.9855811751734164\n",
      "......Outputs 5.2777 5.9857 3.0611 5.9857 5.5573 2.9823 6.9944 5.4221 2.1734 3.1944\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.23545219273896842 [r2] 0.9766478581781399\n",
      "......Outputs 5.3325 5.9069 3.1798 5.9069 5.6845 2.9943 6.8986 5.3776 2.2763 3.1895\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.25860582356396034 [r2] 0.9718292868120652\n",
      "......Outputs 4.7260 5.4043 2.8523 5.4043 5.1705 2.7089 6.2897 4.8095 2.0691 3.0075\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.3228445048203818 [r2] 0.9560956008413005\n",
      "......Outputs 4.5751 5.4113 2.6842 5.4113 4.9515 2.6710 6.3326 4.7489 1.9185 3.0214\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.3196223356687372 [r2] 0.9569676084889772\n",
      "......Outputs 5.3017 6.1575 3.0382 6.1575 5.6327 3.0627 7.2353 5.5140 2.1241 3.2838\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.33100522422804635 [r2] 0.953847956275408\n",
      "......Outputs 5.5452 6.0301 3.2610 6.0301 5.8512 3.1158 7.1059 5.5602 2.3051 3.2119\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.28443525519511315 [r2] 0.9659209019874729\n",
      "......Outputs 4.7373 5.2809 2.9000 5.2809 5.1355 2.6542 6.1211 4.7760 2.0797 2.8831\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.23826984479398317 [r2] 0.9760856046470985\n",
      "......Outputs 4.6979 5.6287 2.8348 5.6287 5.1822 2.7021 6.4357 4.8960 2.0372 3.0651\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.20340435630972367 [r2] 0.9825722331687072\n",
      "......Outputs 5.2358 6.1454 3.0717 6.1454 5.6542 3.0360 7.1870 5.4271 2.1843 3.3701\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.1318204164664078 [r2] 0.9926804126204298\n",
      "......Outputs 5.1587 5.5720 3.0282 5.5720 5.4047 2.9003 6.6975 5.1898 2.1202 3.1275\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.11846634403234658 [r2] 0.9940883159812455\n",
      "......Outputs 4.9647 5.4889 2.9263 5.4889 5.2774 2.7346 6.4441 5.0352 2.0512 2.9580\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.08545211819931253 [r2] 0.9969241375805571\n",
      "......Outputs 5.0930 5.9180 3.0097 5.9180 5.4738 2.8511 6.8021 5.2334 2.1316 3.1317\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.0719545513067374 [r2] 0.9978190898204325\n",
      "......Outputs 5.0858 5.8060 3.0195 5.8060 5.4176 2.8989 6.7762 5.1907 2.1464 3.2019\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.06100688279541813 [r2] 0.9984322424150198\n",
      "......Outputs 4.9887 5.6121 2.9451 5.6121 5.3469 2.8409 6.6030 5.0897 2.0912 3.1005\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.05383303844380863 [r2] 0.9987792716609519\n",
      "......Outputs 5.0449 5.7414 2.9665 5.7414 5.4101 2.8605 6.7006 5.1861 2.1025 3.0939\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.05215568264323776 [r2] 0.9988541586013917\n",
      "......Outputs 5.0821 5.7909 2.9938 5.7909 5.3977 2.8778 6.7486 5.2088 2.1247 3.1280\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04849609586406308 [best r2] 0.9990093168054641\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.739286033778833 [r2] -8.461218721300364\n",
      "......Outputs -0.1192 -0.1244 -0.1256 -0.1244 -0.1316 -0.1254 -0.1191 -0.1210 -0.1191 -0.1324\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.739286033778833 [best r2] -8.461218721300364\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.8495396535822177 [r2] -5.242216625871017\n",
      "......Outputs 0.3381 0.5200 0.4152 0.5200 0.5180 0.7332 0.4213 0.5464 0.3716 0.6045\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.8495396535822177 [best r2] -5.242216625871017\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.7282754967760106 [r2] -2.1354341626371243\n",
      "......Outputs 2.2510 3.4517 2.7845 3.4517 3.5434 4.6967 2.7103 3.5555 2.4722 3.9313\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.7282754967760106 [best r2] -2.1354341626371243\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 2.0166748631731624 [r2] -0.713140613367905\n",
      "......Outputs 3.9262 6.7879 4.4722 6.7879 7.1048 7.5819 5.4569 6.3086 4.1573 5.9245\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.0166748631731624 [best r2] -0.713140613367905\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8652722905693215 [r2] -0.4655671372973975\n",
      "......Outputs 2.5504 4.9423 2.4256 4.9423 5.2777 4.3301 4.2775 4.0827 2.5125 2.9798\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8652722905693215 [best r2] -0.4655671372973975\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.5500588882607915 [r2] -0.012086510377338877\n",
      "......Outputs 2.6809 5.7227 2.1804 5.7227 6.2255 3.9876 5.2211 4.3034 2.4653 2.4311\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.5500588882607915 [best r2] -0.012086510377338877\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.4542179563144393 [r2] 0.10919992141860213\n",
      "......Outputs 3.4364 7.4465 2.5305 7.4465 8.0840 4.4994 7.1110 5.5339 2.9711 2.6640\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4542179563144393 [best r2] 0.10919992141860213\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.29061610060464 [r2] 0.29835829012533266\n",
      "......Outputs 3.2709 6.3438 2.2332 6.3438 6.8337 3.6970 6.6354 5.0492 2.5984 2.3916\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.29061610060464 [best r2] 0.29835829012533266\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.1331165124319487 [r2] 0.4591580404373772\n",
      "......Outputs 3.5436 5.8707 2.3534 5.8707 6.2579 3.4545 6.7236 5.1583 2.5786 2.5582\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.1331165124319487 [best r2] 0.4591580404373772\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 1.0187941228162452 [r2] 0.5627859569400635\n",
      "......Outputs 4.3591 6.1934 2.9090 6.1934 6.5374 3.7871 7.6042 5.9231 2.9240 3.1509\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0187941228162452 [best r2] 0.5627859569400635\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8977764526074885 [r2] 0.6604860202003089\n",
      "......Outputs 4.4984 5.5519 2.9486 5.5519 5.8128 3.4141 7.2252 5.5740 2.6919 3.1304\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8977764526074885 [best r2] 0.6604860202003089\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.789258870880301 [r2] 0.7376022029787694\n",
      "......Outputs 4.9122 5.5267 3.1692 5.5267 5.6964 3.2899 7.3368 5.5395 2.6176 3.1670\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.789258870880301 [best r2] 0.7376022029787694\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.702826526853277 [r2] 0.7919261368889741\n",
      "......Outputs 5.2368 5.6577 3.3478 5.6577 5.7026 3.1918 7.3962 5.4682 2.5209 3.1282\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.702826526853277 [best r2] 0.7919261368889741\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.6264334006104745 [r2] 0.834700685617235\n",
      "......Outputs 5.2413 5.5349 3.3406 5.5349 5.4712 2.9832 7.0359 5.2028 2.3413 2.9648\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6264334006104745 [best r2] 0.834700685617235\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5646412350799999 [r2] 0.8657029655002768\n",
      "......Outputs 5.4110 5.7941 3.4858 5.7941 5.6519 2.9581 7.0394 5.2377 2.2978 2.9934\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5646412350799999 [best r2] 0.8657029655002768\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.5060745360096455 [r2] 0.8921176962129642\n",
      "......Outputs 5.2305 5.6666 3.3523 5.6666 5.6018 2.8357 6.7356 5.0827 2.1784 2.9328\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5060745360096455 [best r2] 0.8921176962129642\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4565142896529866 [r2] 0.9122130433681689\n",
      "......Outputs 5.1901 5.7711 3.3054 5.7711 5.7865 2.8241 6.6898 5.1060 2.1346 3.0156\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4565142896529866 [best r2] 0.9122130433681689\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.411288863582077 [r2] 0.9287450332869416\n",
      "......Outputs 5.0449 5.6104 3.1817 5.6104 5.7079 2.7613 6.4953 5.0553 2.0568 3.0623\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.411288863582077 [best r2] 0.9287450332869416\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.37245599543255525 [r2] 0.9415652514052814\n",
      "......Outputs 5.0261 5.6865 3.1256 5.6865 5.7206 2.7500 6.4932 5.1228 2.0138 3.1388\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.37245599543255525 [best r2] 0.9415652514052814\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.33811153833510765 [r2] 0.9518450180701821\n",
      "......Outputs 4.9740 5.6655 3.0401 5.6655 5.6140 2.6989 6.4634 5.1294 1.9562 3.1308\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.33811153833510765 [best r2] 0.9518450180701821\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.3086502403329954 [r2] 0.9598713564417731\n",
      "......Outputs 4.9870 5.7166 3.0169 5.7166 5.5817 2.6947 6.5081 5.1863 1.9270 3.1294\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3086502403329954 [best r2] 0.9598713564417731\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.282750284183675 [r2] 0.9663234695928442\n",
      "......Outputs 4.9891 5.7055 2.9855 5.7055 5.5352 2.6848 6.5245 5.2046 1.8997 3.1043\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.282750284183675 [best r2] 0.9663234695928442\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.2606233607332841 [r2] 0.9713880193750032\n",
      "......Outputs 4.9873 5.7046 2.9541 5.7046 5.5034 2.6814 6.5599 5.2238 1.8820 3.0945\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2606233607332841 [best r2] 0.9713880193750032\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.24184575101080888 [r2] 0.9753624133773905\n",
      "......Outputs 5.0013 5.7232 2.9357 5.7232 5.4825 2.6967 6.6155 5.2570 1.8759 3.1077\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24184575101080888 [best r2] 0.9753624133773905\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.22490172447203632 [r2] 0.9786937601429588\n",
      "......Outputs 4.9992 5.6924 2.9120 5.6924 5.4506 2.7111 6.6270 5.2499 1.8693 3.1080\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.22490172447203632 [best r2] 0.9786937601429588\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.210526314229774 [r2] 0.981330442902765\n",
      "......Outputs 5.0024 5.7070 2.8974 5.7070 5.4347 2.7344 6.6499 5.2396 1.8688 3.1160\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.210526314229774 [best r2] 0.981330442902765\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.19773014296472677 [r2] 0.9835310086436947\n",
      "......Outputs 5.0143 5.7358 2.8937 5.7358 5.4209 2.7699 6.6708 5.2344 1.8766 3.1294\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19773014296472677 [best r2] 0.9835310086436947\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.18649798946736246 [r2] 0.9853489230471609\n",
      "......Outputs 5.0172 5.7227 2.8851 5.7227 5.4067 2.7926 6.6828 5.2124 1.8837 3.1279\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18649798946736246 [best r2] 0.9853489230471609\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.17633257218271348 [r2] 0.9869025627566026\n",
      "......Outputs 5.0222 5.7239 2.8765 5.7239 5.3916 2.8131 6.6908 5.1927 1.8916 3.1258\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17633257218271348 [best r2] 0.9869025627566026\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16715075996523304 [r2] 0.988231043656447\n",
      "......Outputs 5.0343 5.7353 2.8767 5.7353 5.3914 2.8329 6.7067 5.1800 1.9015 3.1304\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16715075996523304 [best r2] 0.988231043656447\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.15877493390730896 [r2] 0.9893809632906477\n",
      "......Outputs 5.0475 5.7306 2.8889 5.7306 5.3942 2.8432 6.7140 5.1671 1.9134 3.1356\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15877493390730896 [best r2] 0.9893809632906477\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.15101534655000792 [r2] 0.9903935393510706\n",
      "......Outputs 5.0373 5.7263 2.8820 5.7263 5.3792 2.8501 6.7059 5.1502 1.9228 3.1301\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15101534655000792 [best r2] 0.9903935393510706\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.1437570064550776 [r2] 0.9912947894026637\n",
      "......Outputs 5.0266 5.7285 2.8776 5.7285 5.3611 2.8605 6.6966 5.1425 1.9320 3.1295\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1437570064550776 [best r2] 0.9912947894026637\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.13708815668635557 [r2] 0.9920837205881876\n",
      "......Outputs 5.0369 5.7436 2.8827 5.7436 5.3666 2.8725 6.7101 5.1581 1.9437 3.1327\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13708815668635557 [best r2] 0.9920837205881876\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.1314562533737726 [r2] 0.992720798499961\n",
      "......Outputs 5.0587 5.7548 2.8994 5.7548 5.3886 2.8818 6.7302 5.1730 1.9601 3.1351\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1314562533737726 [best r2] 0.992720798499961\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.1267614998079144 [r2] 0.9932314447409047\n",
      "......Outputs 5.0552 5.7384 2.9039 5.7384 5.3933 2.8807 6.7159 5.1476 1.9727 3.1320\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1267614998079144 [best r2] 0.9932314447409047\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.12298631977903925 [r2] 0.9936286002700113\n",
      "......Outputs 5.0112 5.6915 2.8799 5.6915 5.3406 2.8641 6.6617 5.1026 1.9752 3.1177\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.12398515498595407 [r2] 0.9935246891854106\n",
      "......Outputs 4.9802 5.6964 2.8516 5.6964 5.2950 2.8585 6.6567 5.1097 1.9784 3.1092\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.12407391117853074 [r2] 0.9935154150163366\n",
      "......Outputs 5.0579 5.8041 2.9011 5.8041 5.3720 2.8945 6.7648 5.2049 2.0067 3.1358\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.1442183780155328 [r2] 0.9912388229954392\n",
      "......Outputs 5.1609 5.8601 2.9986 5.8601 5.5182 2.9252 6.8461 5.2749 2.0499 3.1594\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.14330244568163814 [r2] 0.9913497542527587\n",
      "......Outputs 5.0055 5.6118 2.9318 5.6118 5.3875 2.8423 6.6043 5.0956 2.0315 3.0949\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.17421929743270675 [r2] 0.9872146165688529\n",
      "......Outputs 4.8260 5.5330 2.7888 5.5330 5.1957 2.7932 6.4917 4.9788 1.9927 3.0637\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.16205890691739586 [r2] 0.9889371492981963\n",
      "......Outputs 5.0548 5.9141 2.8958 5.9141 5.3707 2.9404 6.8617 5.2703 2.0569 3.1855\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.17656174073963826 [r2] 0.9868684967732062\n",
      "......Outputs 5.2567 5.9036 3.0427 5.9036 5.5509 2.9709 6.9469 5.3448 2.1198 3.1765\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.15298768176300068 [r2] 0.9901409704473643\n",
      "......Outputs 4.9364 5.4817 2.9091 5.4817 5.3041 2.7652 6.4697 4.9910 2.0560 3.0048\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.13864718176640486 [r2] 0.9919026421367484\n",
      "......Outputs 4.8880 5.7142 2.8582 5.7142 5.3206 2.8075 6.5942 5.0299 2.0480 3.0804\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.1304542492549941 [r2] 0.9928313446600509\n",
      "......Outputs 5.1500 5.9748 2.9798 5.9748 5.4926 2.9833 6.9405 5.3075 2.1144 3.2372\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12298631977903925 [best r2] 0.9936286002700113\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.10291636540424252 [r2] 0.9955384052402599\n",
      "......Outputs 5.0802 5.5916 2.9493 5.5916 5.3292 2.8711 6.6589 5.1611 2.0898 3.1058\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10291636540424252 [best r2] 0.9955384052402599\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.09948907624406457 [r2] 0.9958306146335432\n",
      "......Outputs 4.9569 5.6221 2.8824 5.6221 5.2994 2.7813 6.5596 5.0478 2.0591 3.0258\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09948907624406457 [best r2] 0.9958306146335432\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.08689889916663313 [r2] 0.9968191016520017\n",
      "......Outputs 5.0764 5.9126 2.9592 5.9126 5.4380 2.8893 6.8037 5.1822 2.0966 3.1537\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08689889916663313 [best r2] 0.9968191016520017\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.07953909134329908 [r2] 0.9973350903378674\n",
      "......Outputs 5.0895 5.7368 2.9689 5.7368 5.4134 2.8983 6.7558 5.1815 2.1001 3.1593\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07953909134329908 [best r2] 0.9973350903378674\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.07612247883399949 [r2] 0.9975591163119953\n",
      "......Outputs 4.9912 5.6088 2.9095 5.6088 5.3374 2.8262 6.6111 5.1069 2.0754 3.0737\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07612247883399949 [best r2] 0.9975591163119953\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.06797358028320279 [r2] 0.9980537369752199\n",
      "......Outputs 5.0311 5.7838 2.9346 5.7838 5.3605 2.8579 6.7030 5.1600 2.0878 3.1059\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06797358028320279 [best r2] 0.9980537369752199\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.0696327181913169 [r2] 0.9979575664384903\n",
      "......Outputs 5.0927 5.8103 2.9695 5.8103 5.4059 2.8963 6.7772 5.1905 2.1043 3.1429\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06797358028320279 [best r2] 0.9980537369752199\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.06333770765706044 [r2] 0.9983101586880465\n",
      "......Outputs 5.0402 5.6720 2.9481 5.6720 5.3709 2.8606 6.6741 5.1344 2.0944 3.1055\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06333770765706044 [best r2] 0.9983101586880465\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.062439390127825976 [r2] 0.9983577527404756\n",
      "......Outputs 4.9997 5.7004 2.9263 5.7004 5.3432 2.8448 6.6499 5.1245 2.0878 3.0906\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.062439390127825976 [best r2] 0.9983577527404756\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.0599291406211772 [r2] 0.9984871448748738\n",
      "......Outputs 5.0580 5.8013 2.9539 5.8013 5.3847 2.8845 6.7493 5.1782 2.1028 3.1262\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0599291406211772 [best r2] 0.9984871448748738\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.060332040471645976 [r2] 0.9984667348369421\n",
      "......Outputs 5.0865 5.7513 2.9773 5.7513 5.4009 2.8904 6.7438 5.1805 2.1110 3.1286\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0599291406211772 [best r2] 0.9984871448748738\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.05735787142970131 [r2] 0.9986141785052678\n",
      "......Outputs 5.0204 5.6758 2.9479 5.6758 5.3525 2.8490 6.6502 5.1264 2.0983 3.0916\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05735787142970131 [best r2] 0.9986141785052678\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.05651325253376015 [r2] 0.9986546916207104\n",
      "......Outputs 5.0034 5.7284 2.9320 5.7284 5.3429 2.8524 6.6713 5.1308 2.0948 3.0997\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.058496711686764596 [r2] 0.9985586012275491\n",
      "......Outputs 5.0790 5.7983 2.9687 5.7983 5.3996 2.8957 6.7674 5.1932 2.1128 3.1360\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.05979460333089756 [r2] 0.9984939297866766\n",
      "......Outputs 5.0917 5.7439 2.9878 5.7439 5.4098 2.8853 6.7345 5.1857 2.1190 3.1202\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.06010088466070231 [r2] 0.9984784614147951\n",
      "......Outputs 5.0015 5.6682 2.9471 5.6682 5.3417 2.8375 6.6266 5.1123 2.1003 3.0833\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.06200833265768095 [r2] 0.9983803493546679\n",
      "......Outputs 4.9897 5.7228 2.9261 5.7228 5.3294 2.8502 6.6661 5.1213 2.0946 3.1026\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.06948022613882138 [r2] 0.9979665022910296\n",
      "......Outputs 5.1027 5.8232 2.9797 5.8232 5.4153 2.9071 6.8018 5.2159 2.1208 3.1443\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.07593894572492196 [r2] 0.9975708721804379\n",
      "......Outputs 5.1226 5.7597 3.0142 5.7597 5.4402 2.8920 6.7546 5.2067 2.1319 3.1223\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.08104504264814258 [r2] 0.9972332230358378\n",
      "......Outputs 4.9711 5.6237 2.9438 5.6237 5.3277 2.8182 6.5764 5.0810 2.0991 3.0706\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.09222531668007761 [r2] 0.9964172083679449\n",
      "......Outputs 4.9365 5.6905 2.8914 5.6905 5.2830 2.8309 6.6221 5.0865 2.0828 3.0928\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.10540562760287134 [r2] 0.9953199678361773\n",
      "......Outputs 5.1389 5.8912 2.9830 5.8912 5.4343 2.9321 6.8736 5.2603 2.1286 3.1618\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.12771547190677968 [r2] 0.9931291848319271\n",
      "......Outputs 5.2086 5.8112 3.0680 5.8112 5.5211 2.9269 6.8319 5.2681 2.1583 3.1395\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.1345942679366888 [r2] 0.9923691244605697\n",
      "......Outputs 4.9217 5.5305 2.9421 5.5305 5.3129 2.7849 6.4796 5.0230 2.0970 3.0427\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.16664587088216767 [r2] 0.9883020339823612\n",
      "......Outputs 4.8107 5.6012 2.8230 5.6012 5.1841 2.7795 6.5030 4.9934 2.0524 3.0647\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.17513848525410874 [r2] 0.9870793483021767\n",
      "......Outputs 5.1736 6.0122 2.9742 6.0122 5.4507 2.9752 6.9907 5.3274 2.1354 3.2051\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.20967035067115758 [r2] 0.9814819486534421\n",
      "......Outputs 5.3508 5.8921 3.1338 5.8921 5.6236 2.9882 6.9779 5.3803 2.1981 3.1720\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.2008648864135406 [r2] 0.983004682281788\n",
      "......Outputs 4.8539 5.4014 2.9311 5.4014 5.2706 2.7265 6.3477 4.9490 2.0935 2.9774\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.2208521509014589 [r2] 0.9794541313479203\n",
      "......Outputs 4.6980 5.5544 2.7938 5.5544 5.1416 2.7347 6.4149 4.9219 2.0382 3.0435\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.2081596090446015 [r2] 0.9817478441695408\n",
      "......Outputs 5.1976 6.1131 3.0012 6.1131 5.5217 3.0428 7.1188 5.4090 2.1615 3.2806\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.19574979357854108 [r2] 0.9838592442205434\n",
      "......Outputs 5.3130 5.7989 3.0980 5.7989 5.5367 2.9866 6.9211 5.3590 2.1928 3.1532\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.1809987334537094 [r2] 0.9862002152389436\n",
      "......Outputs 4.8449 5.3944 2.8941 5.3944 5.2081 2.6864 6.3082 4.9110 2.0731 2.9199\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.1464917695227649 [r2] 0.9909604316911825\n",
      "......Outputs 4.8666 5.7784 2.8927 5.7784 5.3132 2.7800 6.5958 5.0422 2.0811 3.0965\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.14566426298617463 [r2] 0.9910622691449242\n",
      "......Outputs 5.2025 6.0164 3.0382 6.0164 5.5428 3.0088 7.0306 5.3625 2.1672 3.2844\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.10324340131920837 [r2] 0.9955100050921896\n",
      "......Outputs 5.1161 5.5813 2.9854 5.5813 5.3659 2.8825 6.6751 5.1877 2.1271 3.0940\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.10555051850987109 [r2] 0.9953070926192542\n",
      "......Outputs 4.9286 5.5585 2.8923 5.5585 5.2580 2.7507 6.4794 5.0297 2.0732 2.9877\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.07963415309447242 [r2] 0.9973287165571226\n",
      "......Outputs 5.0506 5.9059 2.9644 5.9059 5.4139 2.8627 6.7836 5.1927 2.1161 3.1467\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.08174911164428504 [r2] 0.9971849421458284\n",
      "......Outputs 5.1407 5.8217 3.0141 5.8217 5.4560 2.9290 6.8440 5.2429 2.1414 3.1977\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.06503221900623095 [r2] 0.9982185305174189\n",
      "......Outputs 5.0179 5.5848 2.9463 5.5848 5.3369 2.8377 6.6093 5.1092 2.1024 3.0743\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.062021531787065994 [r2] 0.9983796597617447\n",
      "......Outputs 4.9880 5.7051 2.9289 5.7051 5.3272 2.8216 6.6254 5.1185 2.0939 3.0659\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.05971608252741897 [r2] 0.9984978826583412\n",
      "......Outputs 5.0875 5.8548 2.9820 5.8548 5.4101 2.8915 6.7905 5.2121 2.1228 3.1480\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05651325253376015 [best r2] 0.9986546916207104\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.055400886994583384 [r2] 0.9987071305392853\n",
      "......Outputs 5.0897 5.7338 2.9875 5.7338 5.4067 2.8894 6.7424 5.1793 2.1233 3.1374\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.055400886994583384 [best r2] 0.9987071305392853\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.0529279169104928 [r2] 0.9988199759889687\n",
      "......Outputs 5.0086 5.6469 2.9473 5.6469 5.3458 2.8396 6.6278 5.1140 2.1021 3.0838\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0529279169104928 [best r2] 0.9988199759889687\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.049297279153627645 [r2] 0.9989763131109896\n",
      "......Outputs 5.0171 5.7501 2.9477 5.7501 5.3571 2.8525 6.6859 5.1507 2.1053 3.1017\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.049297279153627645 [best r2] 0.9989763131109896\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.05228819745497968 [r2] 0.9988483285999478\n",
      "......Outputs 5.0845 5.8027 2.9816 5.8027 5.4062 2.8934 6.7702 5.2006 2.1232 3.1386\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.049297279153627645 [best r2] 0.9989763131109896\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.04870266368357746 [r2] 0.9990008592544005\n",
      "......Outputs 5.0713 5.7165 2.9804 5.7165 5.3908 2.8776 6.7104 5.1652 2.1199 3.1170\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04870266368357746 [best r2] 0.9990008592544005\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.04854559884747134 [r2] 0.9990072932692242\n",
      "......Outputs 5.0129 5.6840 2.9504 5.6840 5.3478 2.8433 6.6451 5.1228 2.1050 3.0883\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04854559884747134 [best r2] 0.9990072932692242\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.046552444914473044 [r2] 0.9990871356843081\n",
      "......Outputs 5.0294 5.7556 2.9545 5.7556 5.3645 2.8612 6.7019 5.1548 2.1094 3.1110\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046552444914473044 [best r2] 0.9990871356843081\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.05018587563824997 [r2] 0.9989390760513148\n",
      "......Outputs 5.0845 5.7807 2.9838 5.7807 5.4057 2.8925 6.7590 5.1946 2.1244 3.1344\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046552444914473044 [best r2] 0.9990871356843081\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.04749509752928159 [r2] 0.9990497917243316\n",
      "......Outputs 5.0672 5.7182 2.9808 5.7182 5.3890 2.8736 6.7038 5.1648 2.1210 3.1106\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046552444914473044 [best r2] 0.9990871356843081\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.047994497465910016 [r2] 0.9990297042296342\n",
      "......Outputs 5.0125 5.6937 2.9524 5.6937 5.3462 2.8429 6.6485 5.1240 2.1066 3.0894\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046552444914473044 [best r2] 0.9990871356843081\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.0468030970986833 [r2] 0.99907727895479\n",
      "......Outputs 5.0284 5.7497 2.9549 5.7497 5.3610 2.8612 6.7009 5.1512 2.1096 3.1125\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046552444914473044 [best r2] 0.9990871356843081\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.05129290397940374 [r2] 0.9988917549094045\n",
      "......Outputs 5.0871 5.7805 2.9854 5.7805 5.4087 2.8936 6.7623 5.1974 2.1259 3.1334\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046552444914473044 [best r2] 0.9990871356843081\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. The optimal capacity is 1\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run all tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_all_tests=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}