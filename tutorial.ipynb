{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim, reshape\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Consider using `trainer` function for all tests in `dl_debug`\n",
    "1. Remove all 'importlib' statements\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Try using all the polymers for \"chart dependencies\" instead of a small sample\n",
    "1. Include \"visualize large training batch\"?\n",
    "1. Delete this cell\n",
    "1. Delete the cell below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO for Rishi on presentation:\n",
    "1. Describe GNN?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "data_set = {}\n",
    "data_set['train'] = train_X\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class FFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(FFNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : FFNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here.\n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a3807f87d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import importlib\n",
    "importlib.reload(dl_debug)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'nndebugger.dl_debug' from '/data/rgur/nndebugger/nndebugger/dl_debug.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 137.30295944213867\n",
      "..last epoch zero_data_loss 135.52406883239746\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-64853c3debe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(model_type, correct_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    118\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    119\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    121\u001b[0m                 )\n\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import importlib\n",
    "importlib.reload(constants)\n",
    "importlib.reload(dl_debug)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'nndebugger.dl_debug' from '/data/rgur/nndebugger/nndebugger/dl_debug.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 5.080452065966185\n",
      "....Outputs -0.0215 -0.0118 -0.0215 -0.0048 -0.0261\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "..Epoch 1\n",
      "....Loss: 5.013669021778016\n",
      "....Outputs 0.0494 0.0489 0.0772 0.0489 0.0267\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 2\n",
      "....Loss: 4.939931536366104\n",
      "....Outputs 0.1281 0.1659 0.0844 0.1281 0.1160\n",
      "....Labels  5.7012 5.3739 5.0497 5.6991 2.9694\n",
      "..Epoch 3\n",
      "....Loss: 4.853417574776702\n",
      "....Outputs 0.2705 0.1530 0.1956 0.2201 0.2201\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "..Epoch 4\n",
      "....Loss: 4.747480226046666\n",
      "....Outputs 0.2944 0.3309 0.4002 0.3309 0.2392\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 5\n",
      "....Loss: 4.614684483414844\n",
      "....Outputs 0.5635 0.4170 0.3469 0.4701 0.4701\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 6\n",
      "....Loss: 4.448245296205006\n",
      "....Outputs 0.7672 0.6455 0.4823 0.6455 0.5710\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 4.2390397056813205\n",
      "....Outputs 1.0250 0.8664 0.8664 0.6520 0.7640\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 8\n",
      "....Loss: 3.9801818324969487\n",
      "....Outputs 0.8617 1.1400 1.3457 1.1400 1.0063\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 9\n",
      "....Loss: 3.6597889784133883\n",
      "....Outputs 1.4796 1.3076 1.4796 1.1209 1.7480\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 10\n",
      "....Loss: 3.2675646994269916\n",
      "....Outputs 1.4409 1.8985 2.2447 1.6792 1.8985\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 11\n",
      "....Loss: 2.7926936941155636\n",
      "....Outputs 1.8339 2.4141 2.4141 2.8514 2.1354\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "..Epoch 12\n",
      "....Loss: 2.2323890877199135\n",
      "....Outputs 3.0429 3.0429 2.3081 3.5855 2.6846\n",
      "....Labels  5.7012 5.6991 5.0497 5.3739 2.9694\n",
      "..Epoch 13\n",
      "....Loss: 1.6060330403968386\n",
      "....Outputs 3.8012 3.3304 3.8012 2.8759 4.4634\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 14\n",
      "....Loss: 1.043656140521598\n",
      "....Outputs 3.5428 4.6960 4.0406 4.6960 5.4793\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 15\n",
      "....Loss: 1.0062983767804468\n",
      "....Outputs 4.7193 5.6982 4.2834 6.5630 5.6982\n",
      "....Labels  2.9694 5.7012 5.0497 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.517124967533742\n",
      "....Outputs 7.5070 5.0037 6.6696 5.2224 6.6696\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "..Epoch 17\n",
      "....Loss: 1.9408354812463864\n",
      "....Outputs 7.3538 7.3538 8.0388 5.5404 5.4236\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 18\n",
      "....Loss: 2.0523108599887667\n",
      "....Outputs 5.8075 5.3273 7.6319 8.1057 7.6319\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "..Epoch 19\n",
      "....Loss: 1.885192316803471\n",
      "....Outputs 5.0242 7.8231 7.5613 5.8381 7.5613\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 20\n",
      "....Loss: 1.534874732531883\n",
      "....Outputs 5.6987 7.3279 7.2601 7.2601 4.6043\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.089509099081137\n",
      "....Outputs 6.8283 6.8283 4.1390 5.4584 6.7357\n",
      "....Labels  5.6991 5.7012 2.9694 5.0497 5.3739\n",
      "..Epoch 22\n",
      "....Loss: 0.6259785382839211\n",
      "....Outputs 3.6880 5.1734 6.1308 6.3539 6.3539\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 23\n",
      "....Loss: 0.21782274928121567\n",
      "....Outputs 5.8956 5.8956 3.2816 5.5692 4.8909\n",
      "....Labels  5.7012 5.6991 2.9694 5.3739 5.0497\n",
      "Verified that a small batch can be overfit since the RMSE was less than 0.22499709319526628\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() # Spoiler! The \"bug\" is here.\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 4.8753192503638925\n",
      "....Outputs 0.1923 0.1942 0.1926 0.1910 0.1910\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 1\n",
      "....Loss: 4.553087323097695\n",
      "....Outputs 0.5218 0.5235 0.5229 0.5208 0.5218\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "..Epoch 2\n",
      "....Loss: 4.230841369280654\n",
      "....Outputs 0.8527 0.8506 0.8557 0.8539 0.8539\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 3\n",
      "....Loss: 3.9045361230395352\n",
      "....Outputs 1.1918 1.1918 1.1881 1.1859 1.1938\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 4\n",
      "....Loss: 3.5711887878576944\n",
      "....Outputs 1.5302 1.5414 1.5391 1.5391 1.5327\n",
      "....Labels  5.0497 5.3739 5.7012 5.6991 2.9694\n",
      "..Epoch 5\n",
      "....Loss: 3.22896163453589\n",
      "....Outputs 1.8865 1.8989 1.9013 1.8989 1.8894\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 6\n",
      "....Loss: 2.877508313177779\n",
      "....Outputs 2.2564 2.2728 2.2754 2.2728 2.2598\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 2.518444968559342\n",
      "....Outputs 2.6618 2.6618 2.6645 2.6447 2.6408\n",
      "....Labels  5.6991 5.7012 5.3739 2.9694 5.0497\n",
      "..Epoch 8\n",
      "....Loss: 2.156291021426768\n",
      "....Outputs 3.0436 3.0392 3.0683 3.0654 3.0654\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 9\n",
      "....Loss: 1.8003493844140595\n",
      "....Outputs 3.4498 3.4848 3.4817 3.4541 3.4817\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "..Epoch 10\n",
      "....Loss: 1.4683976257976312\n",
      "....Outputs 3.9071 3.8689 3.9102 3.9071 3.8718\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 11\n",
      "....Loss: 1.1931570228048027\n",
      "....Outputs 4.3353 4.3382 4.2905 4.2899 4.3353\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "..Epoch 12\n",
      "....Loss: 1.0262729881025852\n",
      "....Outputs 4.7595 4.6989 4.7051 4.7570 4.7570\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "..Epoch 13\n",
      "....Loss: 1.011257935670853\n",
      "....Outputs 5.0998 5.0859 5.1590 5.1590 5.1607\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "..Epoch 14\n",
      "....Loss: 1.124956342061953\n",
      "....Outputs 5.5241 5.4574 5.4345 5.5241 5.5245\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 15\n",
      "....Loss: 1.2923077971288968\n",
      "....Outputs 5.7593 5.8331 5.8319 5.8331 5.7265\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 16\n",
      "....Loss: 1.4492287853952575\n",
      "....Outputs 6.0699 6.0667 5.9895 5.9468 6.0699\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 17\n",
      "....Loss: 1.5610774621847836\n",
      "....Outputs 6.2207 6.2262 6.2262 6.0880 6.1401\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 18\n",
      "....Loss: 1.6157343135723317\n",
      "....Outputs 6.3030 6.3030 6.1515 6.2952 6.2123\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "..Epoch 19\n",
      "....Loss: 1.6146005034815565\n",
      "....Outputs 6.3081 6.1453 6.2138 6.3081 6.2979\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 20\n",
      "....Loss: 1.5661985492703965\n",
      "....Outputs 6.1559 6.0806 6.2529 6.2529 6.2404\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 21\n",
      "....Loss: 1.482416889247627\n",
      "....Outputs 6.0508 5.9697 6.1351 6.1498 6.1498\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 22\n",
      "....Loss: 1.3765389759768603\n",
      "....Outputs 6.0114 5.9946 6.0114 5.9110 5.8248\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 23\n",
      "....Loss: 1.2622074562529506\n",
      "....Outputs 5.6574 5.8305 5.8493 5.7480 5.8493\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 24\n",
      "....Loss: 1.152689322687098\n",
      "....Outputs 5.4776 5.5720 5.6535 5.6741 5.6741\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 25\n",
      "....Loss: 1.0598960832725186\n",
      "....Outputs 5.4950 5.3923 5.4726 5.2944 5.4950\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 26\n",
      "....Loss: 0.9927712172731635\n",
      "....Outputs 5.3196 5.2163 5.2955 5.3196 5.1152\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 27\n",
      "....Loss: 0.9553191460105913\n",
      "....Outputs 5.0502 5.1542 4.9460 5.1542 5.1286\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 28\n",
      "....Loss: 0.9455680186569337\n",
      "....Outputs 5.0036 4.9765 5.0036 4.8987 4.7913\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 29\n",
      "....Loss: 0.9567437187648509\n",
      "....Outputs 4.8427 4.8712 4.6544 4.8712 4.7651\n",
      "....Labels  5.3739 5.6991 2.9694 5.7012 5.0497\n",
      "..Epoch 30\n",
      "....Loss: 0.9800864154317552\n",
      "....Outputs 4.7592 4.7592 4.7294 4.6517 4.5373\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 31\n",
      "....Loss: 1.0074514289298675\n",
      "....Outputs 4.6690 4.4411 4.6378 4.5596 4.6690\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 32\n",
      "....Loss: 1.0326139504497565\n",
      "....Outputs 4.6008 4.3661 4.5682 4.4891 4.6008\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 33\n",
      "....Loss: 1.0514856976467402\n",
      "....Outputs 4.5204 4.4399 4.5543 4.3118 4.5543\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "..Epoch 34\n",
      "....Loss: 1.061823854163299\n",
      "....Outputs 4.4935 4.5288 4.2773 4.5288 4.4112\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 35\n",
      "....Loss: 1.0628271160337697\n",
      "....Outputs 4.2614 4.4863 4.4019 4.5231 4.5231\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 36\n",
      "....Loss: 1.0547837390083121\n",
      "....Outputs 4.4102 4.5354 4.2625 4.5354 4.4972\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "..Epoch 37\n",
      "....Loss: 1.0387969287136536\n",
      "....Outputs 4.2788 4.5642 4.5642 4.4345 4.5245\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "..Epoch 38\n",
      "....Loss: 1.0165803963897364\n",
      "....Outputs 4.3083 4.4728 4.5660 4.6072 4.6072\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 39\n",
      "....Loss: 0.9902785251864882\n",
      "....Outputs 4.6623 4.3487 4.6196 4.6623 4.5229\n",
      "....Labels  5.7012 2.9694 5.3739 5.6991 5.0497\n",
      "..Epoch 40\n",
      "....Loss: 0.9622879773240117\n",
      "....Outputs 4.6829 4.3979 4.7272 4.7272 4.5824\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "..Epoch 41\n",
      "....Loss: 0.9350450479905756\n",
      "....Outputs 4.4534 4.7992 4.6489 4.7992 4.7534\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 42\n",
      "....Loss: 0.91076508815769\n",
      "....Outputs 4.7197 4.5126 4.8757 4.8757 4.8284\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 43\n",
      "....Loss: 0.8911575599016283\n",
      "....Outputs 4.9541 4.7924 4.5731 4.9052 4.9541\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "..Epoch 44\n",
      "....Loss: 0.8771709350065561\n",
      "....Outputs 4.6322 5.0317 5.0317 4.8642 4.9813\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "..Epoch 45\n",
      "....Loss: 0.8688488073899836\n",
      "....Outputs 5.1059 4.6876 5.0540 4.9326 5.1059\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 46\n",
      "....Loss: 0.8653660107919493\n",
      "....Outputs 5.1210 5.1743 4.9954 5.1743 4.7370\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 47\n",
      "....Loss: 0.8652473259294571\n",
      "....Outputs 5.2350 5.0504 5.1801 5.2350 4.7785\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 48\n",
      "....Loss: 0.8666997001539156\n",
      "....Outputs 5.2862 5.0960 4.8106 5.2862 5.2298\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 49\n",
      "....Loss: 0.8679634609112766\n",
      "....Outputs 5.2688 5.3267 4.8322 5.1311 5.3267\n",
      "....Labels  5.3739 5.7012 2.9694 5.0497 5.6991\n",
      "..Epoch 50\n",
      "....Loss: 0.867597946771854\n",
      "....Outputs 5.3558 4.8427 5.2964 5.3558 5.1549\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 51\n",
      "....Loss: 0.8646563457009314\n",
      "....Outputs 5.1673 5.3735 5.3735 4.8421 5.3125\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 52\n",
      "....Loss: 0.858750758163806\n",
      "....Outputs 4.8308 5.3800 5.3800 5.1688 5.3176\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "..Epoch 53\n",
      "....Loss: 0.8500217533132928\n",
      "....Outputs 5.3764 5.3764 5.3124 5.1602 4.8098\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 54\n",
      "....Loss: 0.8390323711647637\n",
      "....Outputs 5.1428 5.3639 5.2983 4.7803 5.3639\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 55\n",
      "....Loss: 0.8266217244785815\n",
      "....Outputs 5.1181 5.3441 5.3441 5.2769 4.7439\n",
      "....Labels  5.0497 5.7012 5.6991 5.3739 2.9694\n",
      "..Epoch 56\n",
      "....Loss: 0.8137309213678295\n",
      "....Outputs 5.3187 5.0879 5.2499 4.7023 5.3187\n",
      "....Labels  5.6991 5.0497 5.3739 2.9694 5.7012\n",
      "..Epoch 57\n",
      "....Loss: 0.801241300598147\n",
      "....Outputs 5.2897 4.6573 5.2193 5.0542 5.2897\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 58\n",
      "....Loss: 0.7898323156357365\n",
      "....Outputs 4.6107 5.0186 5.1868 5.2589 5.2589\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 59\n",
      "....Loss: 0.7798976255749281\n",
      "....Outputs 5.2281 5.1544 4.5641 5.2281 4.9830\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 60\n",
      "....Loss: 0.771516862016247\n",
      "....Outputs 5.1990 4.9490 5.1990 4.5189 5.1235\n",
      "....Labels  5.7012 5.0497 5.6991 2.9694 5.3739\n",
      "..Epoch 61\n",
      "....Loss: 0.7644894801302352\n",
      "....Outputs 5.1728 4.9179 5.1728 5.0957 4.4764\n",
      "....Labels  5.6991 5.0497 5.7012 5.3739 2.9694\n",
      "..Epoch 62\n",
      "....Loss: 0.7584152970366802\n",
      "....Outputs 4.4377 5.0720 5.1507 4.8908 5.1507\n",
      "....Labels  2.9694 5.3739 5.6991 5.0497 5.7012\n",
      "..Epoch 63\n",
      "....Loss: 0.7527953744272907\n",
      "....Outputs 5.1337 4.4033 4.8685 5.1337 5.0533\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 64\n",
      "....Loss: 0.7471327412768906\n",
      "....Outputs 5.1222 5.0401 5.1222 4.3738 4.8517\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 65\n",
      "....Loss: 0.7410143364074153\n",
      "....Outputs 4.3493 5.1165 4.8406 5.1165 5.0328\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 66\n",
      "....Loss: 0.7341626042770705\n",
      "....Outputs 4.8351 4.3299 5.0313 5.1167 5.1167\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 67\n",
      "....Loss: 0.7264582241252631\n",
      "....Outputs 4.8352 4.3151 5.1226 5.1226 5.0355\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "..Epoch 68\n",
      "....Loss: 0.7179348095995961\n",
      "....Outputs 4.3047 5.1336 5.0449 5.1336 4.8402\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "..Epoch 69\n",
      "....Loss: 0.7087502841847976\n",
      "....Outputs 5.0588 4.8497 5.1492 5.1492 4.2980\n",
      "....Labels  5.3739 5.0497 5.6991 5.7012 2.9694\n",
      "..Epoch 70\n",
      "....Loss: 0.6991462669950559\n",
      "....Outputs 5.0765 5.1685 5.1685 4.2943 4.8628\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "..Epoch 71\n",
      "....Loss: 0.689396647876871\n",
      "....Outputs 5.0970 4.8788 5.1907 4.2928 5.1907\n",
      "....Labels  5.3739 5.0497 5.7012 2.9694 5.6991\n",
      "..Epoch 72\n",
      "....Loss: 0.6797590546925605\n",
      "....Outputs 4.2927 5.2148 5.2148 5.1195 4.8965\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 73\n",
      "....Loss: 0.6704318938374312\n",
      "....Outputs 5.1429 5.2399 5.2399 4.2931 4.9151\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 74\n",
      "....Loss: 0.6615289039712576\n",
      "....Outputs 4.9338 4.2933 5.1663 5.2649 5.2649\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 75\n",
      "....Loss: 0.6530687563466259\n",
      "....Outputs 4.2924 5.2890 4.9515 5.2890 5.1888\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 76\n",
      "....Loss: 0.6449833064913121\n",
      "....Outputs 5.3114 4.9675 4.2900 5.2095 5.3114\n",
      "....Labels  5.7012 5.0497 2.9694 5.3739 5.6991\n",
      "..Epoch 77\n",
      "....Loss: 0.6371445899273283\n",
      "....Outputs 4.2854 4.9812 5.3314 5.3314 5.2280\n",
      "....Labels  2.9694 5.0497 5.6991 5.7012 5.3739\n",
      "..Epoch 78\n",
      "....Loss: 0.6293975380173888\n",
      "....Outputs 4.2783 5.3486 4.9922 5.3486 5.2435\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 79\n",
      "....Loss: 0.6215905415925532\n",
      "....Outputs 5.3627 4.2686 5.0001 5.3627 5.2560\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 80\n",
      "....Loss: 0.6136106104777013\n",
      "....Outputs 5.0049 5.3735 5.3735 4.2560 5.2652\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 81\n",
      "....Loss: 0.6053948388055784\n",
      "....Outputs 5.2711 5.3811 5.0065 5.3811 4.2408\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 82\n",
      "....Loss: 0.5969383815247237\n",
      "....Outputs 5.2740 5.3857 5.0052 5.3857 4.2232\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 83\n",
      "....Loss: 0.5882889168329647\n",
      "....Outputs 4.2034 5.2743 5.0013 5.3877 5.3877\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 84\n",
      "....Loss: 0.5795271597743873\n",
      "....Outputs 5.3874 4.9953 4.1820 5.3874 5.2724\n",
      "....Labels  5.6991 5.0497 2.9694 5.7012 5.3739\n",
      "..Epoch 85\n",
      "....Loss: 0.570749272354805\n",
      "....Outputs 4.1593 5.2688 5.3855 4.9877 5.3855\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 86\n",
      "....Loss: 0.5620452048950376\n",
      "....Outputs 5.2641 4.9791 4.1358 5.3825 5.3825\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "..Epoch 87\n",
      "....Loss: 0.5534819544940709\n",
      "....Outputs 5.3789 4.9700 4.1121 5.3789 5.2589\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 88\n",
      "....Loss: 0.5450937690474613\n",
      "....Outputs 5.3754 5.3754 4.0885 4.9611 5.2537\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 89\n",
      "....Loss: 0.5368806941715204\n",
      "....Outputs 5.3725 4.0655 5.2491 5.3725 4.9527\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 90\n",
      "....Loss: 0.5288133277575496\n",
      "....Outputs 4.9454 4.0434 5.3705 5.3705 5.2455\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "..Epoch 91\n",
      "....Loss: 0.5208443704071443\n",
      "....Outputs 4.0224 5.3698 5.3698 5.2432 4.9394\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 92\n",
      "....Loss: 0.5129197564898385\n",
      "....Outputs 4.0027 5.3706 5.3706 5.2424 4.9350\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 93\n",
      "....Loss: 0.5049928290736132\n",
      "....Outputs 5.3731 5.2434 5.3731 3.9845 4.9323\n",
      "....Labels  5.7012 5.3739 5.6991 2.9694 5.0497\n",
      "..Epoch 94\n",
      "....Loss: 0.4970308899001226\n",
      "....Outputs 5.2460 3.9676 4.9314 5.3773 5.3773\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 95\n",
      "....Loss: 0.489022142981577\n",
      "....Outputs 4.9321 5.2503 3.9520 5.3830 5.3830\n",
      "....Labels  5.0497 5.3739 2.9694 5.6991 5.7012\n",
      "..Epoch 96\n",
      "....Loss: 0.480973792389175\n",
      "....Outputs 5.2561 3.9376 5.3903 4.9343 5.3903\n",
      "....Labels  5.3739 2.9694 5.6991 5.0497 5.7012\n",
      "..Epoch 97\n",
      "....Loss: 0.4729090715910358\n",
      "....Outputs 5.3987 5.3987 5.2631 4.9378 3.9242\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 98\n",
      "....Loss: 0.4648603388286617\n",
      "....Outputs 5.4080 3.9116 5.4080 4.9423 5.2711\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 99\n",
      "....Loss: 0.4568609425721275\n",
      "....Outputs 4.9475 5.4180 5.4180 5.2796 3.8995\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cb30c21e4518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 150\u001b[0;31m                                of data. The minimum RMSE over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not less than {self.EPSILON}''')\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the RMSE was less than {self.EPSILON}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize predictions of a large batch as a function of epoch\n",
    "There should not be a large jump in predicted value between epochs (except, perhaps, in the first few epochs). However, predictions should not stay constant between epochs either."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not pass information along the batch dimension."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graphnet.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph_batch.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# data to illustrate the point\n",
    "\n",
    "import importlib\n",
    "importlib.reload(dl_debug)\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()\n",
    "polymer_smiles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[*]C(C#N)=C([*])c1ccccc1',\n",
       " '[*]CCCCOC(=O)C(=O)O[*]',\n",
       " '[*]CC(CCl)(CCl)C(=O)O[*]',\n",
       " '[*]c1[nH]c([*])c(C(=O)O)c1C']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# feature_array = np.zeros((N_DATA_, MAX_N_ATOMS, N_FEATURES_))\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "# for ind, smiles in enumerate(polymer_smiles):\n",
    "#     feature_array[ind, ].append(featurize_smiles_by_atom(smiles))\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "# for smiles,data in zip(polymer_smiles,train_X_):\n",
    "#     data.num_atoms = Chem.MolFromSmiles(smiles).GetNumAtoms()\n",
    "data_set_ = {'train': train_X_}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "[(smile, featurize_smiles_by_atom(smile)) for smile in polymer_smiles]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('[*]C(C#N)=C([*])c1ccccc1',\n",
       "  array([[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]])),\n",
       " ('[*]CCCCOC(=O)C(=O)O[*]',\n",
       "  array([[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]])),\n",
       " ('[*]CC(CCl)(CCl)C(=O)O[*]',\n",
       "  array([[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]])),\n",
       " ('[*]c1[nH]c([*])c(C(=O)O)c1C',\n",
       "  array([[1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]))]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=1)\n",
    "        x = x - x_mean[:, None, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession('gnn', correct_graphnet_class_ls, capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 0.0636 0.0636 0.0636 0.0636\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: 0.06361331045627594\n",
      "Finished charting dependencies. Data is not getting passed along the batch dimension.\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0)\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession('gnn', buggy_graphnet_class_ls, capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 0.2596 0.2020 0.2162 0.2014\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: 0.25957411527633667\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Data is getting passed along the batch dimension.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ee534407bd2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession('gnn', buggy_graphnet_class_ls, complexity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_chart_dependencies=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_chart_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_choose_model_size_by_overfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mchart_dependencies\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# print(data.x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is getting passed along the batch dimension.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished charting dependencies. Data is not getting passed along the batch dimension.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is getting passed along the batch dimension."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data & gradient check\n",
    "The capacity of your architecture should be just large enough to overfit the training data. Also, the gradients should not equal zero before overfitting all training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(constants)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'nndebugger.constants' from '/data/rgur/nndebugger/nndebugger/constants.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.728853643258016 [r2] -8.419611402272475\n",
      "......Outputs -0.1223 -0.1223 -0.1161 -0.1223 -0.1182 -0.1232 -0.1385 -0.1240 -0.1106 -0.1229\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.728853643258016 [best r2] -8.419611402272475\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.7929112024626206 [r2] -5.059915824095616\n",
      "......Outputs 0.3605 0.5941 0.4553 0.5941 0.5427 0.6862 0.4110 0.6120 0.4511 0.6152\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.7929112024626206 [best r2] -5.059915824095616\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.7049325653964638 [r2] -2.082010584696905\n",
      "......Outputs 2.3631 3.8501 2.9555 3.8501 3.7417 4.7116 2.8184 3.8961 2.7980 4.0575\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.7049325653964638 [best r2] -2.082010584696905\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.9587686015518038 [r2] -0.6161717397938871\n",
      "......Outputs 3.8338 6.9986 4.3741 6.9986 7.0115 7.1166 5.3845 6.4047 4.2929 5.6726\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.9587686015518038 [best r2] -0.6161717397938871\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.881045117111793 [r2] -0.4904577357426476\n",
      "......Outputs 2.4720 4.9617 2.3713 4.9617 5.1007 3.9548 4.1346 4.0857 2.5760 2.7799\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.881045117111793 [best r2] -0.4904577357426476\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.5481028385822067 [r2] -0.009533778474612786\n",
      "......Outputs 2.7079 5.9407 2.2157 5.9407 6.2319 3.7980 5.2723 4.4959 2.6415 2.3968\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.5481028385822067 [best r2] -0.009533778474612786\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.4520819952926052 [r2] 0.11181482089880401\n",
      "......Outputs 3.4580 7.6683 2.5477 7.6683 8.0902 4.3822 7.2046 5.7314 3.2044 2.6933\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4520819952926052 [best r2] 0.11181482089880401\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2886348589423817 [r2] 0.3005108353939976\n",
      "......Outputs 3.2293 6.3334 2.2097 6.3334 6.7145 3.5575 6.5812 5.1470 2.7761 2.3862\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2886348589423817 [best r2] 0.3005108353939976\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.1238270967245227 [r2] 0.4679894565350521\n",
      "......Outputs 3.4973 5.8693 2.3008 5.8693 6.2492 3.4365 6.7687 5.3163 2.8027 2.5928\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.1238270967245227 [best r2] 0.4679894565350521\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 1.0089133966675035 [r2] 0.5712254316335628\n",
      "......Outputs 4.2346 6.1464 2.7713 6.1464 6.5930 3.7878 7.6834 6.0881 3.1411 3.2390\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0089133966675035 [best r2] 0.5712254316335628\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8847377659521011 [r2] 0.6702761419377691\n",
      "......Outputs 4.2873 5.4974 2.7725 5.4974 5.8574 3.3845 7.2392 5.6312 2.8430 3.0970\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8847377659521011 [best r2] 0.6702761419377691\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7752000834567262 [r2] 0.7468669438827735\n",
      "......Outputs 4.6790 5.5273 3.0345 5.5273 5.8788 3.3037 7.4209 5.6319 2.7735 3.1095\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7752000834567262 [best r2] 0.7468669438827735\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6865565277584617 [r2] 0.8014481936815608\n",
      "......Outputs 4.9225 5.6024 3.1761 5.6024 5.8590 3.1562 7.4125 5.4959 2.6296 3.0014\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6865565277584617 [best r2] 0.8014481936815608\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.6123732614673395 [r2] 0.8420376158899239\n",
      "......Outputs 4.9731 5.5255 3.2016 5.5255 5.6351 2.9717 7.0964 5.2509 2.4320 2.8606\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6123732614673395 [best r2] 0.8420376158899239\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5494058517708574 [r2] 0.8728525067035807\n",
      "......Outputs 5.1543 5.7030 3.3618 5.7030 5.7434 2.9893 7.0454 5.2838 2.3786 2.9368\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5494058517708574 [best r2] 0.8728525067035807\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.490605335964956 [r2] 0.8986121822036993\n",
      "......Outputs 5.0683 5.6147 3.2526 5.6147 5.7007 2.8829 6.7785 5.1761 2.2498 2.9092\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.490605335964956 [best r2] 0.8986121822036993\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4399770724480794 [r2] 0.918458005091862\n",
      "......Outputs 5.0846 5.7246 3.2065 5.7246 5.8677 2.9074 6.7409 5.2328 2.1944 2.9898\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4399770724480794 [best r2] 0.918458005091862\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.3956506854744984 [r2] 0.9340605856079447\n",
      "......Outputs 5.0033 5.6396 3.0883 5.6396 5.7922 2.8615 6.5620 5.1919 2.1055 3.0127\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3956506854744984 [best r2] 0.9340605856079447\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.35744746341539224 [r2] 0.9461797538305747\n",
      "......Outputs 5.0152 5.6932 3.0585 5.6932 5.7971 2.8666 6.5492 5.2485 2.0548 3.0748\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.35744746341539224 [best r2] 0.9461797538305747\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3234562720091652 [r2] 0.9559290518144882\n",
      "......Outputs 4.9842 5.6772 3.0017 5.6772 5.7029 2.8210 6.5105 5.2390 1.9990 3.0676\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3234562720091652 [best r2] 0.9559290518144882\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.2947924619420324 [r2] 0.9633938555542587\n",
      "......Outputs 4.9884 5.7033 2.9724 5.7033 5.6653 2.7989 6.5264 5.2530 1.9632 3.0679\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2947924619420324 [best r2] 0.9633938555542587\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2700150606701259 [r2] 0.9692887694725503\n",
      "......Outputs 4.9899 5.7143 2.9579 5.7143 5.6070 2.8015 6.5420 5.2588 1.9417 3.0643\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2700150606701259 [best r2] 0.9692887694725503\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.24925718680149506 [r2] 0.9738292228638933\n",
      "......Outputs 4.9886 5.7070 2.9395 5.7070 5.5574 2.8083 6.5553 5.2533 1.9343 3.0710\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24925718680149506 [best r2] 0.9738292228638933\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.23134393655646515 [r2] 0.977455662187852\n",
      "......Outputs 5.0019 5.7213 2.9203 5.7213 5.5386 2.8199 6.6039 5.2587 1.9400 3.0908\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.23134393655646515 [best r2] 0.977455662187852\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.21550407741834912 [r2] 0.9804371455356464\n",
      "......Outputs 5.0083 5.7105 2.9086 5.7105 5.5012 2.8316 6.6325 5.2480 1.9498 3.0991\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.21550407741834912 [best r2] 0.9804371455356464\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.20187468180649948 [r2] 0.9828333737764259\n",
      "......Outputs 5.0200 5.7110 2.8989 5.7110 5.4662 2.8380 6.6559 5.2313 1.9649 3.1005\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20187468180649948 [best r2] 0.9828333737764259\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.19009946775252534 [r2] 0.9847776030507533\n",
      "......Outputs 5.0358 5.7343 2.8949 5.7343 5.4457 2.8498 6.6795 5.2225 1.9849 3.1025\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19009946775252534 [best r2] 0.9847776030507533\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.17949355784532028 [r2] 0.9864287774323134\n",
      "......Outputs 5.0380 5.7359 2.8959 5.7359 5.4258 2.8586 6.6895 5.2089 2.0013 3.1106\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17949355784532028 [best r2] 0.9864287774323134\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.17017925275409504 [r2] 0.9878007125229371\n",
      "......Outputs 5.0319 5.7352 2.8935 5.7352 5.4258 2.8594 6.6995 5.1950 2.0147 3.1131\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17017925275409504 [best r2] 0.9878007125229371\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16187281866385175 [r2] 0.9889625411096973\n",
      "......Outputs 5.0352 5.7373 2.8957 5.7373 5.4352 2.8655 6.7093 5.1891 2.0327 3.1183\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16187281866385175 [best r2] 0.9889625411096973\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.15424350731117167 [r2] 0.9899784470633265\n",
      "......Outputs 5.0368 5.7284 2.9004 5.7284 5.4308 2.8676 6.7084 5.1790 2.0500 3.1215\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15424350731117167 [best r2] 0.9899784470633265\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.14719928269605334 [r2] 0.9908729037956732\n",
      "......Outputs 5.0306 5.7290 2.8998 5.7290 5.4194 2.8596 6.7039 5.1606 2.0588 3.1154\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14719928269605334 [best r2] 0.9908729037956732\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.14049739294600952 [r2] 0.9916850858021243\n",
      "......Outputs 5.0375 5.7418 2.9110 5.7418 5.4158 2.8638 6.7074 5.1604 2.0693 3.1236\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14049739294600952 [best r2] 0.9916850858021243\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.1344385120527255 [r2] 0.9923867755250773\n",
      "......Outputs 5.0542 5.7445 2.9244 5.7445 5.4262 2.8725 6.7111 5.1670 2.0829 3.1277\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1344385120527255 [best r2] 0.9923867755250773\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.1284222579471519 [r2] 0.9930529272973788\n",
      "......Outputs 5.0478 5.7234 2.9239 5.7234 5.4166 2.8661 6.6966 5.1496 2.0911 3.1164\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1284222579471519 [best r2] 0.9930529272973788\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.12303968652180756 [r2] 0.9936230696610344\n",
      "......Outputs 5.0380 5.7229 2.9169 5.7229 5.3972 2.8561 6.6885 5.1361 2.0904 3.1145\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12303968652180756 [best r2] 0.9936230696610344\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.11802371110209971 [r2] 0.9941324098133488\n",
      "......Outputs 5.0514 5.7495 2.9292 5.7495 5.4060 2.8728 6.7054 5.1550 2.0980 3.1225\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11802371110209971 [best r2] 0.9941324098133488\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.11453223161288442 [r2] 0.9944744350703482\n",
      "......Outputs 5.0739 5.7640 2.9514 5.7640 5.4310 2.8888 6.7274 5.1825 2.1142 3.1253\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11453223161288442 [best r2] 0.9944744350703482\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.11065875843065204 [r2] 0.9948418635406051\n",
      "......Outputs 5.0513 5.7141 2.9409 5.7141 5.4076 2.8624 6.7012 5.1523 2.1113 3.1085\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11065875843065204 [best r2] 0.9948418635406051\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.11054811530631942 [r2] 0.9948521731993405\n",
      "......Outputs 5.0042 5.6750 2.9097 5.6750 5.3524 2.8260 6.6634 5.0982 2.0863 3.0969\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11054811530631942 [best r2] 0.9948521731993405\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.10955616318903756 [r2] 0.9949441419812124\n",
      "......Outputs 5.0269 5.7684 2.9246 5.7684 5.3643 2.8683 6.7025 5.1425 2.0988 3.1157\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.12225350211958175 [r2] 0.9937043024104903\n",
      "......Outputs 5.1253 5.8577 2.9982 5.8577 5.4855 2.9486 6.7959 5.2559 2.1608 3.1526\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.12669842368647924 [r2] 0.993238179088377\n",
      "......Outputs 5.0933 5.6912 2.9847 5.6912 5.4681 2.8704 6.7216 5.1812 2.1542 3.1153\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.15546475027952297 [r2] 0.9898191249340615\n",
      "......Outputs 4.9102 5.5219 2.8555 5.5219 5.2571 2.7155 6.5495 4.9677 2.0468 3.0515\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.15954990524431 [r2] 0.9892770484898975\n",
      "......Outputs 4.9693 5.7670 2.8760 5.7670 5.2920 2.8355 6.6899 5.0991 2.0521 3.1229\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.19768527024001872 [r2] 0.9835384827156725\n",
      "......Outputs 5.2626 6.0651 3.0883 6.0651 5.6257 3.0865 6.9970 5.4282 2.2069 3.2413\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.16846709238351842 [r2] 0.9880449498702525\n",
      "......Outputs 5.0761 5.5908 3.0284 5.5908 5.4554 2.8848 6.6324 5.1367 2.1911 3.0702\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.18601435381644002 [r2] 0.9854248122892958\n",
      "......Outputs 4.8040 5.5042 2.8188 5.5042 5.2150 2.6641 6.4316 4.8609 2.0435 2.9711\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.14795559238203285 [r2] 0.9907788728369467\n",
      "......Outputs 5.0652 5.9917 2.9536 5.9917 5.4552 2.9412 6.8861 5.2468 2.1161 3.2302\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.13587002486420785 [r2] 0.9922237797896223\n",
      "......Outputs 5.1814 5.7582 3.0489 5.7582 5.4940 2.9838 6.8410 5.3206 2.1749 3.2108\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.11831476770824909 [r2] 0.9941034341666444\n",
      "......Outputs 4.9666 5.4861 2.9328 5.4861 5.3142 2.7578 6.4920 5.0082 2.1013 2.9941\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10955616318903756 [best r2] 0.9949441419812124\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.09643509220831098 [r2] 0.9960826584317147\n",
      "......Outputs 5.0021 5.8508 2.9461 5.8508 5.4176 2.8427 6.6827 5.0959 2.1139 3.0940\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09643509220831098 [best r2] 0.9960826584317147\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.09518471661813163 [r2] 0.9961835842153641\n",
      "......Outputs 5.1095 5.8867 3.0105 5.8867 5.4815 2.9648 6.8353 5.2449 2.1566 3.2053\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09518471661813163 [best r2] 0.9961835842153641\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.07815492861092194 [r2] 0.9974270343851979\n",
      "......Outputs 5.0342 5.5893 2.9609 5.5893 5.3610 2.8473 6.6441 5.1460 2.1151 3.0909\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07815492861092194 [best r2] 0.9974270343851979\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.07395654158444714 [r2] 0.9976960426828277\n",
      "......Outputs 5.0059 5.6927 2.9422 5.6927 5.3568 2.8075 6.6423 5.1273 2.0976 3.0660\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07395654158444714 [best r2] 0.9976960426828277\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.07053404873025748 [r2] 0.9979043494391531\n",
      "......Outputs 5.0748 5.8513 2.9889 5.8513 5.4328 2.9005 6.7732 5.2036 2.1351 3.1514\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07053404873025748 [best r2] 0.9979043494391531\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.06444264840593796 [r2] 0.9982506850747125\n",
      "......Outputs 5.0673 5.7142 2.9815 5.7142 5.4106 2.8934 6.7172 5.1673 2.1359 3.1309\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06444264840593796 [best r2] 0.9982506850747125\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.06303682022254146 [r2] 0.998326175823133\n",
      "......Outputs 5.0173 5.6643 2.9454 5.6643 5.3585 2.8331 6.6457 5.1170 2.1090 3.0785\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06303682022254146 [best r2] 0.998326175823133\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.05851335103569559 [r2] 0.998557781101168\n",
      "......Outputs 5.0413 5.7807 2.9621 5.7807 5.3804 2.8640 6.7139 5.1640 2.1160 3.1132\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05851335103569559 [best r2] 0.998557781101168\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.06010422873717796 [r2] 0.9984782920900681\n",
      "......Outputs 5.0791 5.7769 2.9879 5.7769 5.4139 2.9009 6.7489 5.1924 2.1339 3.1386\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05851335103569559 [best r2] 0.998557781101168\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.05601746944347225 [r2] 0.9986781924969967\n",
      "......Outputs 5.0470 5.6874 2.9671 5.6874 5.3811 2.8596 6.6789 5.1397 2.1222 3.1002\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05601746944347225 [best r2] 0.9986781924969967\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.05566856914562423 [r2] 0.9986946067630402\n",
      "......Outputs 5.0185 5.7159 2.9459 5.7159 5.3556 2.8374 6.6669 5.1217 2.1074 3.0902\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05566856914562423 [best r2] 0.9986946067630402\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.0549273488249886 [r2] 0.9987291376437867\n",
      "......Outputs 5.0585 5.7842 2.9697 5.7842 5.3946 2.8841 6.7358 5.1773 2.1219 3.1280\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0549273488249886 [best r2] 0.9987291376437867\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.05627574503448211 [r2] 0.9986659756785782\n",
      "......Outputs 5.0807 5.7463 2.9880 5.7463 5.4118 2.8948 6.7337 5.1883 2.1350 3.1263\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0549273488249886 [best r2] 0.9987291376437867\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.054106008751705424 [r2] 0.9987668604193279\n",
      "......Outputs 5.0323 5.6892 2.9596 5.6892 5.3630 2.8422 6.6627 5.1276 2.1164 3.0875\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.054530881146631335 [r2] 0.9987474176967421\n",
      "......Outputs 5.0153 5.7286 2.9444 5.7286 5.3484 2.8419 6.6740 5.1245 2.1032 3.0976\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.057199742485401175 [r2] 0.9986218090680797\n",
      "......Outputs 5.0737 5.7882 2.9783 5.7882 5.4067 2.9004 6.7535 5.1960 2.1255 3.1367\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.05941431602419743 [r2] 0.9985130257609816\n",
      "......Outputs 5.0888 5.7456 2.9948 5.7456 5.4209 2.8938 6.7347 5.1940 2.1398 3.1200\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.060014151906272066 [r2] 0.9984828497698409\n",
      "......Outputs 5.0147 5.6742 2.9523 5.6742 5.3460 2.8241 6.6394 5.1053 2.1102 3.0775\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.063453064004001 [r2] 0.9983039976997982\n",
      "......Outputs 4.9988 5.7199 2.9333 5.7199 5.3278 2.8364 6.6645 5.1125 2.0919 3.0988\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.07156804871838324 [r2] 0.9978424563352473\n",
      "......Outputs 5.0953 5.8190 2.9907 5.8190 5.4257 2.9254 6.7881 5.2287 2.1319 3.1484\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.07982871443488125 [r2] 0.997315647707523\n",
      "......Outputs 5.1182 5.7646 3.0204 5.7646 5.4582 2.9111 6.7582 5.2190 2.1595 3.1226\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.0862819205665596 [r2] 0.996864109798938\n",
      "......Outputs 4.9837 5.6222 2.9397 5.6222 5.3178 2.7901 6.5879 5.0593 2.1031 3.0583\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.10144575283816311 [r2] 0.9956650012185027\n",
      "......Outputs 4.9439 5.6813 2.8933 5.6813 5.2661 2.8021 6.6162 5.0659 2.0602 3.0857\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.1181152137840938 [r2] 0.9941233081106353\n",
      "......Outputs 5.1314 5.8996 3.0034 5.8996 5.4624 2.9727 6.8627 5.2928 2.1370 3.1762\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.14518962127238158 [r2] 0.9911204207909612\n",
      "......Outputs 5.2069 5.8284 3.0831 5.8284 5.5596 2.9756 6.8404 5.3016 2.2089 3.1474\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.1566560432457796 [r2] 0.9896624994158745\n",
      "......Outputs 4.9324 5.5141 2.9159 5.5141 5.2742 2.7258 6.4832 4.9717 2.1015 3.0173\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.197838066526283 [r2] 0.9835130257783905\n",
      "......Outputs 4.8135 5.5739 2.8024 5.5739 5.1364 2.7084 6.4852 4.9414 1.9979 3.0498\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.21099491230988152 [r2] 0.9812472394794826\n",
      "......Outputs 5.1741 6.0437 3.0084 6.0437 5.5153 3.0417 6.9997 5.3966 2.1342 3.2387\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.24884084897505343 [r2] 0.9739165766917479\n",
      "......Outputs 5.3467 5.9257 3.1741 5.9257 5.7077 3.0867 6.9879 5.4470 2.2805 3.1853\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.2417187688768162 [r2] 0.9753882787232804\n",
      "......Outputs 4.8515 5.3525 2.8902 5.3525 5.2119 2.6406 6.3065 4.8446 2.0995 2.9208\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.2578737178347529 [r2] 0.9719885620158675\n",
      "......Outputs 4.7166 5.5541 2.7599 5.5541 5.0999 2.6460 6.3946 4.8548 1.9722 3.0330\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.2443008696499205 [r2] 0.9748596530556076\n",
      "......Outputs 5.2138 6.1857 3.0344 6.1857 5.6165 3.1005 7.1608 5.4996 2.1449 3.3440\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.21091383606342132 [r2] 0.9812616484650301\n",
      "......Outputs 5.2629 5.7265 3.1210 5.7265 5.5651 3.0363 6.8889 5.3814 2.2177 3.1249\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.19577015683034396 [r2] 0.9838558858990463\n",
      "......Outputs 4.8432 5.3575 2.8849 5.3575 5.1774 2.6413 6.3022 4.8603 2.0608 2.8697\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.14655357533571617 [r2] 0.990952802378915\n",
      "......Outputs 4.9283 5.8994 2.9082 5.8994 5.3571 2.7903 6.6647 5.0736 2.0790 3.1111\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.14478457118946553 [r2] 0.9911698961909446\n",
      "......Outputs 5.1736 6.0002 3.0445 6.0002 5.5399 3.0355 6.9923 5.3721 2.1813 3.2867\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.09846364701132601 [r2] 0.9959161190240713\n",
      "......Outputs 5.0514 5.4921 2.9750 5.4921 5.3230 2.8655 6.6248 5.1567 2.1260 3.0519\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.09371883327319722 [r2] 0.9963002277766236\n",
      "......Outputs 4.9660 5.6069 2.9175 5.6069 5.2902 2.7528 6.5456 5.0697 2.0772 2.9881\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.07667691810789526 [r2] 0.9975234303944924\n",
      "......Outputs 5.0801 5.9454 2.9896 5.9454 5.4444 2.8870 6.8012 5.2233 2.1336 3.1587\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.06678941677210734 [r2] 0.9981209577596791\n",
      "......Outputs 5.0875 5.7558 3.0001 5.7558 5.4116 2.9247 6.7555 5.1985 2.1489 3.1613\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.059453070068247814 [r2] 0.99851108531757\n",
      "......Outputs 5.0088 5.5936 2.9416 5.5936 5.3307 2.8377 6.6132 5.1076 2.1045 3.0528\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.054106008751705424 [best r2] 0.9987668604193279\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.052415768572366156 [r2] 0.9988427021205525\n",
      "......Outputs 5.0320 5.7607 2.9532 5.7607 5.3636 2.8464 6.6921 5.1681 2.1062 3.0846\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052415768572366156 [best r2] 0.9988427021205525\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.05323962528995125 [r2] 0.9988060360304432\n",
      "......Outputs 5.0804 5.8202 2.9899 5.8202 5.3999 2.8956 6.7648 5.2118 2.1317 3.1429\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052415768572366156 [best r2] 0.9988427021205525\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.047313356872712334 [r2] 0.9990570497817602\n",
      "......Outputs 5.0533 5.6945 2.9731 5.6945 5.3741 2.8721 6.6930 5.1489 2.1224 3.1056\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.047313356872712334 [best r2] 0.9990570497817602\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.04688988518963418 [r2] 0.9990738537350421\n",
      "......Outputs 5.0254 5.6883 2.9519 5.6883 5.3554 2.8458 6.6624 5.1266 2.1080 3.0805\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04688988518963418 [best r2] 0.9990738537350421\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.04534056974535828 [r2] 0.999134045273236\n",
      "......Outputs 5.0499 5.7764 2.9658 5.7764 5.3801 2.8692 6.7220 5.1734 2.1180 3.1110\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04534056974535828 [best r2] 0.999134045273236\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.046187172061813456 [r2] 0.9991014050259162\n",
      "......Outputs 5.0689 5.7616 2.9796 5.7616 5.3927 2.8845 6.7335 5.1833 2.1277 3.1217\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04534056974535828 [best r2] 0.999134045273236\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.0441260454380198 [r2] 0.9991798160685588\n",
      "......Outputs 5.0471 5.6993 2.9682 5.6993 5.3684 2.8627 6.6831 5.1461 2.1186 3.0986\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0441260454380198 [best r2] 0.9991798160685588\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.043739052736943666 [r2] 0.9991941392803444\n",
      "......Outputs 5.0335 5.7220 2.9582 5.7220 5.3607 2.8523 6.6810 5.1405 2.1107 3.0950\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.043739052736943666 [best r2] 0.9991941392803444\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.635720068679527 [r2] -8.052231389107101\n",
      "......Outputs -0.0472 -0.0401 -0.0502 -0.0401 -0.0484 -0.0353 -0.0512 -0.0414 -0.0559 -0.0420\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.635720068679527 [best r2] -8.052231389107101\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.5920642891191026 [r2] -4.4351238227350995\n",
      "......Outputs 0.4943 0.7867 0.6919 0.7867 0.8001 0.9733 0.5806 0.8921 0.5571 0.8383\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.5920642891191026 [best r2] -4.4351238227350995\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6565097570137963 [r2] -1.9726520071037008\n",
      "......Outputs 2.6643 4.3335 3.5413 4.3335 4.4597 5.5427 3.1842 4.5909 3.0246 4.6453\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6565097570137963 [best r2] -1.9726520071037008\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.887971082622352 [r2] -0.5014536050547491\n",
      "......Outputs 3.6100 6.7223 4.2651 6.7223 7.0414 6.7798 5.1327 6.2307 3.8698 5.2031\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.887971082622352 [best r2] -0.5014536050547491\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8160033183305277 [r2] -0.3891671868150912\n",
      "......Outputs 2.3910 4.9177 2.3823 4.9177 5.2758 3.8225 4.0624 4.0811 2.3522 2.5703\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8160033183305277 [best r2] -0.3891671868150912\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.50500280025449 [r2] 0.04589569185315756\n",
      "......Outputs 2.7570 6.1580 2.3203 6.1580 6.7226 3.8309 5.4086 4.6354 2.5247 2.3319\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.50500280025449 [best r2] 0.04589569185315756\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.3978891836077894 [r2] 0.17687322310526643\n",
      "......Outputs 3.4371 7.5851 2.5753 7.5851 8.2320 4.2383 7.1349 5.6445 2.9784 2.5767\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.3978891836077894 [best r2] 0.17687322310526643\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2504164291793274 [r2] 0.34138656844176407\n",
      "......Outputs 3.1699 6.1730 2.2622 6.1730 6.6633 3.4217 6.4508 5.0049 2.5469 2.3518\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2504164291793274 [best r2] 0.34138656844176407\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0921810886555743 [r2] 0.49752952758809343\n",
      "......Outputs 3.5424 5.9411 2.4455 5.9411 6.3816 3.4192 6.8421 5.2600 2.6552 2.6354\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0921810886555743 [best r2] 0.49752952758809343\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9883003663990848 [r2] 0.588566970437517\n",
      "......Outputs 4.2561 6.1834 2.8935 6.1834 6.6508 3.7484 7.6907 5.8529 2.9421 3.2039\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9883003663990848 [best r2] 0.588566970437517\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8776305837689732 [r2] 0.6755522705534799\n",
      "......Outputs 4.3181 5.5142 2.8587 5.5142 5.8924 3.3651 7.2304 5.4403 2.6940 3.1254\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8776305837689732 [best r2] 0.6755522705534799\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7779467337897851 [r2] 0.7450699891939869\n",
      "......Outputs 4.7067 5.5196 3.0535 5.5196 5.9142 3.2853 7.4194 5.4224 2.6343 3.1688\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7779467337897851 [best r2] 0.7450699891939869\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6960029777775026 [r2] 0.7959467875847119\n",
      "......Outputs 4.9876 5.6089 3.1850 5.6089 5.9383 3.1663 7.4579 5.3625 2.5217 3.1080\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6960029777775026 [best r2] 0.7959467875847119\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.6249054821293533 [r2] 0.8355060573164741\n",
      "......Outputs 5.0151 5.5114 3.1814 5.5114 5.6764 2.9523 7.1360 5.1732 2.3366 2.9703\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6249054821293533 [best r2] 0.8355060573164741\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5652042977706239 [r2] 0.8654349887815671\n",
      "......Outputs 5.2094 5.7267 3.3253 5.7267 5.8023 2.9305 7.1318 5.2626 2.3009 3.0254\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5652042977706239 [best r2] 0.8654349887815671\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.5101082940645506 [r2] 0.8903910516751388\n",
      "......Outputs 5.1065 5.6816 3.2066 5.6816 5.6869 2.7795 6.8194 5.1698 2.1748 2.9794\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5101082940645506 [best r2] 0.8903910516751388\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4644331368878889 [r2] 0.9091410657693333\n",
      "......Outputs 5.1266 5.7823 3.1668 5.7823 5.8576 2.7444 6.7591 5.2441 2.1274 3.0641\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4644331368878889 [best r2] 0.9091410657693333\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.42299153011977014 [r2] 0.9246324177824489\n",
      "......Outputs 5.0346 5.6745 3.0457 5.6745 5.7906 2.6770 6.5464 5.2014 2.0417 3.0887\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.42299153011977014 [best r2] 0.9246324177824489\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.3867510058738768 [r2] 0.9369936756352603\n",
      "......Outputs 5.0493 5.7133 3.0173 5.7133 5.8022 2.6836 6.5285 5.2642 2.0075 3.1522\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3867510058738768 [best r2] 0.9369936756352603\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3535878608636298 [r2] 0.947335746294266\n",
      "......Outputs 5.0144 5.6869 2.9485 5.6869 5.7011 2.6693 6.4783 5.2308 1.9540 3.1220\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3535878608636298 [best r2] 0.947335746294266\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.32484736142286386 [r2] 0.9555491645431814\n",
      "......Outputs 5.0303 5.7362 2.9274 5.7362 5.6797 2.6991 6.5232 5.2575 1.9307 3.1188\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.32484736142286386 [best r2] 0.9555491645431814\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2995282237986687 [r2] 0.9622082726110125\n",
      "......Outputs 5.0088 5.7268 2.8804 5.7268 5.6060 2.7186 6.5251 5.2234 1.9109 3.0981\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2995282237986687 [best r2] 0.9622082726110125\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.27753787060861684 [r2] 0.9675536577705657\n",
      "......Outputs 5.0124 5.7301 2.8577 5.7301 5.5775 2.7667 6.5602 5.2092 1.9061 3.1117\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.27753787060861684 [best r2] 0.9675536577705657\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.25822335763343346 [r2] 0.9719125515314235\n",
      "......Outputs 5.0321 5.7287 2.8414 5.7287 5.5543 2.8056 6.5940 5.2156 1.9142 3.1358\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.25822335763343346 [best r2] 0.9719125515314235\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.24115230740240046 [r2] 0.975503497381004\n",
      "......Outputs 5.0230 5.7154 2.8269 5.7154 5.5122 2.8198 6.6044 5.2121 1.9191 3.1339\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24115230740240046 [best r2] 0.975503497381004\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.22610073850599 [r2] 0.9784659755105534\n",
      "......Outputs 5.0232 5.7292 2.8209 5.7292 5.4952 2.8394 6.6331 5.2028 1.9392 3.1357\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.22610073850599 [best r2] 0.9784659755105534\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.21295130350102062 [r2] 0.980897867795957\n",
      "......Outputs 5.0342 5.7359 2.8288 5.7359 5.4796 2.8642 6.6564 5.2010 1.9689 3.1481\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.21295130350102062 [best r2] 0.980897867795957\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.20097649450295063 [r2] 0.9829857905586581\n",
      "......Outputs 5.0288 5.7274 2.8311 5.7274 5.4630 2.8670 6.6691 5.1921 1.9864 3.1446\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20097649450295063 [best r2] 0.9829857905586581\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.19041959108595957 [r2] 0.9847262915183026\n",
      "......Outputs 5.0129 5.7264 2.8331 5.7264 5.4484 2.8648 6.6769 5.1816 1.9961 3.1389\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19041959108595957 [best r2] 0.9847262915183026\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.18076981718633456 [r2] 0.9862350994340389\n",
      "......Outputs 5.0191 5.7454 2.8421 5.7454 5.4360 2.8752 6.6961 5.1827 2.0164 3.1440\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18076981718633456 [best r2] 0.9862350994340389\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.17216800334362362 [r2] 0.9875139196109652\n",
      "......Outputs 5.0371 5.7536 2.8612 5.7536 5.4411 2.8816 6.7211 5.1943 2.0424 3.1505\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17216800334362362 [best r2] 0.9875139196109652\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.1639005237207786 [r2] 0.9886842870119197\n",
      "......Outputs 5.0338 5.7205 2.8665 5.7205 5.4324 2.8740 6.7097 5.1752 2.0532 3.1506\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1639005237207786 [best r2] 0.9886842870119197\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.1563283239657945 [r2] 0.9897057055967151\n",
      "......Outputs 5.0065 5.7082 2.8607 5.7082 5.4024 2.8591 6.6773 5.1370 2.0478 3.1404\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1563283239657945 [best r2] 0.9897057055967151\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.15042579994688826 [r2] 0.990468397988033\n",
      "......Outputs 4.9955 5.7282 2.8656 5.7282 5.3947 2.8609 6.6782 5.1329 2.0492 3.1316\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15042579994688826 [best r2] 0.990468397988033\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.14519550430287925 [r2] 0.9911197011817607\n",
      "......Outputs 5.0484 5.7735 2.9012 5.7735 5.4209 2.8861 6.7481 5.1830 2.0857 3.1518\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14519550430287925 [best r2] 0.9911197011817607\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.14311878743876308 [r2] 0.9913719125751073\n",
      "......Outputs 5.0897 5.7794 2.9363 5.7794 5.4524 2.8923 6.7504 5.2112 2.1146 3.1669\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14311878743876308 [best r2] 0.9913719125751073\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.13821039175438604 [r2] 0.9919535811289012\n",
      "......Outputs 4.9946 5.6805 2.8993 5.6805 5.3929 2.8366 6.6430 5.1335 2.0660 3.1191\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13821039175438604 [best r2] 0.9919535811289012\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.13903950916223112 [r2] 0.9918567514093473\n",
      "......Outputs 4.9384 5.6611 2.8657 5.6611 5.3409 2.8109 6.6420 5.0996 2.0391 3.1002\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13821039175438604 [best r2] 0.9919535811289012\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.1365014218823401 [r2] 0.992151338638127\n",
      "......Outputs 5.0665 5.8360 2.9472 5.8360 5.4287 2.9054 6.8075 5.2221 2.1061 3.1653\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1365014218823401 [best r2] 0.992151338638127\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.1405960854155994 [r2] 0.9916734000672978\n",
      "......Outputs 5.1374 5.8223 3.0026 5.8223 5.4853 2.9335 6.8076 5.2578 2.1621 3.1646\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1365014218823401 [best r2] 0.992151338638127\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.13636773334054306 [r2] 0.9921667049581435\n",
      "......Outputs 4.9630 5.5955 2.8973 5.5955 5.3513 2.7916 6.5850 5.0806 2.0752 3.0896\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13636773334054306 [best r2] 0.9921667049581435\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.13582727843573233 [r2] 0.9922286720144063\n",
      "......Outputs 4.9228 5.6699 2.8671 5.6699 5.3041 2.7883 6.6339 5.0690 2.0378 3.1175\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13582727843573233 [best r2] 0.9922286720144063\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.13717516538906527 [r2] 0.992073668607015\n",
      "......Outputs 5.1073 5.9110 2.9959 5.9110 5.4607 2.9361 6.8965 5.2687 2.1402 3.1997\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13582727843573233 [best r2] 0.9922286720144063\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.128355099003228 [r2] 0.9930601913975325\n",
      "......Outputs 5.1222 5.7367 3.0047 5.7367 5.4490 2.9128 6.7284 5.2197 2.1653 3.1396\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.128355099003228 [best r2] 0.9930601913975325\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.12991256349276342 [r2] 0.9928907539399456\n",
      "......Outputs 4.9455 5.5786 2.8719 5.5786 5.3061 2.7805 6.5211 5.0319 2.0625 3.0807\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.128355099003228 [best r2] 0.9930601913975325\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.11617844865805103 [r2] 0.9943144512605381\n",
      "......Outputs 4.9945 5.7997 2.9051 5.7997 5.3658 2.8400 6.7248 5.1396 2.0715 3.1739\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11617844865805103 [best r2] 0.9943144512605381\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.1236615861498971 [r2] 0.9935584428200305\n",
      "......Outputs 5.1317 5.8765 3.0195 5.8765 5.4830 2.9475 6.8740 5.2850 2.1638 3.1923\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11617844865805103 [best r2] 0.9943144512605381\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.10872794376226821 [r2] 0.9950202952886944\n",
      "......Outputs 5.0391 5.6177 2.9619 5.6177 5.3770 2.8599 6.6211 5.1355 2.1325 3.0890\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10872794376226821 [best r2] 0.9950202952886944\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.11170779603106946 [r2] 0.9947436024087372\n",
      "......Outputs 4.9467 5.6413 2.8666 5.6413 5.2987 2.7777 6.5726 5.0324 2.0606 3.1046\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10872794376226821 [best r2] 0.9950202952886944\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.1032031747149591 [r2] 0.9955135032733631\n",
      "......Outputs 5.0605 5.8751 2.9552 5.8751 5.4106 2.8990 6.8153 5.2080 2.1127 3.1956\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1032031747149591 [best r2] 0.9955135032733631\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.10626112551484886 [r2] 0.9952436909698427\n",
      "......Outputs 5.1273 5.7939 3.0184 5.7939 5.4650 2.9390 6.8086 5.2573 2.1696 3.1508\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1032031747149591 [best r2] 0.9955135032733631\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.09932386958250373 [r2] 0.9958444500888706\n",
      "......Outputs 4.9965 5.5844 2.9299 5.5844 5.3361 2.8232 6.5684 5.0923 2.1066 3.0760\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09932386958250373 [best r2] 0.9958444500888706\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.09735384154750547 [r2] 0.996007660851046\n",
      "......Outputs 4.9644 5.7101 2.8909 5.7101 5.3159 2.8109 6.6343 5.0814 2.0704 3.1322\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09735384154750547 [best r2] 0.996007660851046\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.09916599823695385 [r2] 0.9958576497534058\n",
      "......Outputs 5.1027 5.8846 2.9869 5.8846 5.4435 2.9256 6.8573 5.2445 2.1401 3.1972\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09735384154750547 [best r2] 0.996007660851046\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.09511103551177888 [r2] 0.996189490392569\n",
      "......Outputs 5.1072 5.7297 3.0102 5.7297 5.4420 2.9106 6.7405 5.2204 2.1637 3.1220\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09511103551177888 [best r2] 0.996189490392569\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.09817909890275245 [r2] 0.9959396887685363\n",
      "......Outputs 4.9619 5.5956 2.9094 5.5956 5.3099 2.7934 6.5419 5.0597 2.0863 3.0720\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09511103551177888 [best r2] 0.996189490392569\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.09197680083727633 [r2] 0.996436491160953\n",
      "......Outputs 4.9854 5.7609 2.9086 5.7609 5.3326 2.8296 6.6870 5.1127 2.0778 3.1537\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.10483050413212865 [r2] 0.9953708997140999\n",
      "......Outputs 5.1403 5.8849 3.0111 5.8849 5.4729 2.9485 6.8874 5.2753 2.1601 3.1864\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.0958470795129882 [r2] 0.9961302847447523\n",
      "......Outputs 5.0935 5.6876 3.0039 5.6876 5.4238 2.8938 6.6923 5.1895 2.1586 3.1005\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.10870879685158455 [r2] 0.9950220489789209\n",
      "......Outputs 4.9293 5.5875 2.8940 5.5875 5.2821 2.7753 6.5108 5.0226 2.0672 3.0734\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.09923278318967065 [r2] 0.9958520684084615\n",
      "......Outputs 4.9991 5.8024 2.9254 5.8024 5.3516 2.8498 6.7325 5.1380 2.0826 3.1657\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.12158908098095576 [r2] 0.9937725479438562\n",
      "......Outputs 5.1767 5.8981 3.0462 5.8981 5.5076 2.9767 6.9217 5.3138 2.1825 3.1830\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.10714070025457527 [r2] 0.9951646245416506\n",
      "......Outputs 5.0742 5.6360 3.0015 5.6360 5.4060 2.8715 6.6386 5.1569 2.1523 3.0748\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.12858572392120096 [r2] 0.993035230477976\n",
      "......Outputs 4.8916 5.5663 2.8757 5.5663 5.2511 2.7464 6.4750 4.9839 2.0435 3.0604\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.11483497788926447 [r2] 0.9944451846991315\n",
      "......Outputs 5.0228 5.8535 2.9412 5.8535 5.3757 2.8700 6.7919 5.1701 2.0858 3.1842\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.1421786818467441 [r2] 0.9914848910729726\n",
      "......Outputs 5.2207 5.9115 3.0712 5.9115 5.5428 3.0065 6.9585 5.3489 2.2002 3.1905\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.12164102878812119 [r2] 0.9937672255648621\n",
      "......Outputs 5.0467 5.5783 2.9885 5.5783 5.3763 2.8504 6.5674 5.1155 2.1387 3.0542\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.14359525183120386 [r2] 0.9913143684952275\n",
      "......Outputs 4.8630 5.5632 2.8598 5.5632 5.2292 2.7284 6.4510 4.9658 2.0269 3.0539\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.12743884930065888 [r2] 0.993158915990774\n",
      "......Outputs 5.0566 5.9192 2.9590 5.9192 5.4146 2.8956 6.8635 5.2191 2.0994 3.2114\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.1481253088172885 [r2] 0.9907577060208732\n",
      "......Outputs 5.2269 5.8901 3.0750 5.8901 5.5528 3.0093 6.9495 5.3451 2.2069 3.1911\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.12713114268657036 [r2] 0.9931919122936352\n",
      "......Outputs 4.9987 5.5174 2.9600 5.5174 5.3301 2.8158 6.4996 5.0709 2.1158 3.0273\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.13835685229149317 [r2] 0.9919365186312196\n",
      "......Outputs 4.8724 5.6141 2.8663 5.6141 5.2412 2.7358 6.4928 4.9908 2.0267 3.0614\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.12878843261745734 [r2] 0.993013253976444\n",
      "......Outputs 5.1064 5.9730 2.9883 5.9730 5.4652 2.9283 6.9317 5.2680 2.1186 3.2305\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.1316626345249213 [r2] 0.9926979244378369\n",
      "......Outputs 5.1995 5.8050 3.0582 5.8050 5.5178 2.9838 6.8700 5.3054 2.1894 3.1718\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.12147090913178403 [r2] 0.993784646923562\n",
      "......Outputs 4.9644 5.4948 2.9298 5.4948 5.2939 2.7917 6.4593 5.0412 2.0901 3.0117\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.11647693753877629 [r2] 0.9942851987927577\n",
      "......Outputs 4.9203 5.6990 2.8871 5.6990 5.2791 2.7728 6.5788 5.0522 2.0442 3.0879\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.12029125381885272 [r2] 0.9939047805815834\n",
      "......Outputs 5.1382 5.9717 3.0083 5.9717 5.4861 2.9507 6.9481 5.2908 2.1399 3.2343\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.10884498181301369 [r2] 0.9950095689088451\n",
      "......Outputs 5.1469 5.7264 3.0332 5.7264 5.4716 2.9445 6.7762 5.2420 2.1703 3.1398\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.11040286958571643 [r2] 0.9948656914510339\n",
      "......Outputs 4.9454 5.5250 2.9177 5.5250 5.2759 2.7789 6.4711 5.0324 2.0761 3.0213\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.09688199212753046 [r2] 0.9960462667811768\n",
      "......Outputs 4.9690 5.7674 2.9134 5.7674 5.3136 2.8074 6.6646 5.1024 2.0633 3.1198\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.1090024502151476 [r2] 0.9949951189334906\n",
      "......Outputs 5.1528 5.9317 3.0188 5.9317 5.4846 2.9549 6.9289 5.2852 2.1509 3.2192\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.09255443241230481 [r2] 0.9963915916045708\n",
      "......Outputs 5.1070 5.6782 3.0112 5.6782 5.4296 2.9094 6.7090 5.1984 2.1542 3.1138\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.1007709887350506 [r2] 0.9957224777191973\n",
      "......Outputs 4.9365 5.5617 2.9133 5.5617 5.2761 2.7763 6.4938 5.0410 2.0728 3.0319\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09197680083727633 [best r2] 0.996436491160953\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.08664432333603292 [r2] 0.9968377116389193\n",
      "......Outputs 4.9955 5.7955 2.9310 5.7955 5.3399 2.8333 6.7101 5.1357 2.0791 3.1332\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08664432333603292 [best r2] 0.9968377116389193\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.1027235109655998 [r2] 0.9955551106916039\n",
      "......Outputs 5.1585 5.8956 3.0244 5.8956 5.4853 2.9580 6.9066 5.2808 2.1578 3.2006\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08664432333603292 [best r2] 0.9968377116389193\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.08681955027905802 [r2] 0.9968249080664\n",
      "......Outputs 5.0893 5.6636 3.0023 5.6636 5.4115 2.8931 6.6774 5.1753 2.1467 3.0978\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08664432333603292 [best r2] 0.9968377116389193\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.09880380139788673 [r2] 0.9958878537797412\n",
      "......Outputs 4.9315 5.5854 2.9107 5.5854 5.2713 2.7751 6.5050 5.0391 2.0695 3.0410\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08664432333603292 [best r2] 0.9968377116389193\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.08649723588883769 [r2] 0.9968484391303346\n",
      "......Outputs 5.0045 5.8061 2.9365 5.8061 5.3485 2.8440 6.7300 5.1468 2.0850 3.1404\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.10627406518910042 [r2] 0.9952425325248057\n",
      "......Outputs 5.1679 5.8894 3.0299 5.8894 5.4934 2.9626 6.9086 5.2895 2.1661 3.1934\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.0911331556095691 [r2] 0.9965015629960324\n",
      "......Outputs 5.0872 5.6598 3.0020 5.6598 5.4085 2.8871 6.6661 5.1726 2.1495 3.0910\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.10741097890636146 [r2] 0.9951401978358005\n",
      "......Outputs 4.9176 5.5787 2.9027 5.5787 5.2577 2.7640 6.4916 5.0246 2.0637 3.0380\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.09573185031197512 [r2] 0.9961395836438315\n",
      "......Outputs 4.9993 5.8019 2.9316 5.8019 5.3410 2.8423 6.7308 5.1439 2.0806 3.1390\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.12049511378109742 [r2] 0.9938841036983354\n",
      "......Outputs 5.1899 5.9096 3.0388 5.9096 5.5131 2.9776 6.9391 5.3127 2.1742 3.1950\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.10546716867816879 [r2] 0.99531450136732\n",
      "......Outputs 5.1032 5.6652 3.0124 5.6652 5.4218 2.8955 6.6722 5.1820 2.1585 3.0898\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.1272876398849237 [r2] 0.9931751405977802\n",
      "......Outputs 4.8895 5.5459 2.8923 5.5459 5.2389 2.7449 6.4487 4.9926 2.0541 3.0271\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.11598389206695947 [r2] 0.9943334777630539\n",
      "......Outputs 4.9711 5.7860 2.9164 5.7860 5.3212 2.8271 6.7110 5.1219 2.0666 3.1350\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.14643462828808007 [r2] 0.9909674823446828\n",
      "......Outputs 5.2219 5.9578 3.0522 5.9578 5.5472 3.0024 7.0018 5.3519 2.1857 3.2119\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.13148898045326388 [r2] 0.9927171736186143\n",
      "......Outputs 5.1413 5.6832 3.0347 5.6832 5.4559 2.9211 6.7013 5.2106 2.1808 3.0943\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.15706386645830436 [r2] 0.9896086060555147\n",
      "......Outputs 4.8538 5.4912 2.8785 5.4912 5.2109 2.7187 6.3763 4.9511 2.0428 3.0056\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.14691854822325592 [r2] 0.9909076844974368\n",
      "......Outputs 4.9183 5.7569 2.8875 5.7569 5.2822 2.7958 6.6604 5.0783 2.0382 3.1259\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08649723588883769 [best r2] 0.9968484391303346\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. Index of best model is 0\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run all tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}