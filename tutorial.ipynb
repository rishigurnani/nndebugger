{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# nndebugger functions\n",
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Remove all 'importlib' statements\n",
    "1. Clean up arguments passed into DebugSession\n",
    "1. Add ReadMe to repo\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Change overfit small batch to an R2 requirement\n",
    "1. Clear outputs\n",
    "1. Delete this cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "len(data_df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3380"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(MyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : MyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "data_set = train_X\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here. The correct line is 'return x.view(data.num_graphs,)'\n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-80c0c8fa0ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 134.40209197998047\n",
      "..last epoch zero_data_loss 134.75465393066406\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-66eab23cba70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    158\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    159\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    161\u001b[0m                 )\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import importlib\n",
    "importlib.reload(dl_debug)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'nndebugger.dl_debug' from '/data/rgur/nndebugger/nndebugger/dl_debug.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "..Epoch 0\n",
      "....Outputs -0.0215 -0.0118 -0.0215 -0.0048 -0.0261\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "....Loss: 5.080452065966185\n",
      "....R2: -23.640635320447466\n",
      "..Epoch 1\n",
      "....Outputs 0.0494 0.0489 0.0772 0.0489 0.0267\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 5.013669021778016\n",
      "....R2: -22.997087142660575\n",
      "..Epoch 2\n",
      "....Outputs 0.1281 0.1659 0.0844 0.1281 0.1160\n",
      "....Labels  5.7012 5.3739 5.0497 5.6991 2.9694\n",
      "....Loss: 4.939931536366104\n",
      "....R2: -22.296412314037998\n",
      "..Epoch 3\n",
      "....Outputs 0.2705 0.1530 0.1956 0.2201 0.2201\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "....Loss: 4.853417574776702\n",
      "....R2: -21.487568024988658\n",
      "..Epoch 4\n",
      "....Outputs 0.2944 0.3309 0.4002 0.3309 0.2392\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 4.747480226046666\n",
      "....R2: -20.51659281772587\n",
      "..Epoch 5\n",
      "....Outputs 0.5635 0.4170 0.3469 0.4701 0.4701\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "....Loss: 4.614684483414844\n",
      "....R2: -19.329712146052287\n",
      "..Epoch 6\n",
      "....Outputs 0.7672 0.6455 0.4823 0.6455 0.5710\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "....Loss: 4.448245296205006\n",
      "....R2: -17.889680347866094\n",
      "..Epoch 7\n",
      "....Outputs 1.0250 0.8664 0.8664 0.6520 0.7640\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "....Loss: 4.2390397056813205\n",
      "....R2: -16.154663459375776\n",
      "..Epoch 8\n",
      "....Outputs 0.8617 1.1400 1.3457 1.1400 1.0063\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "....Loss: 3.9801818324969487\n",
      "....R2: -14.12352530757627\n",
      "..Epoch 9\n",
      "....Outputs 1.4796 1.3076 1.4796 1.1209 1.7480\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "....Loss: 3.6597889784133883\n",
      "....R2: -11.786723029312201\n",
      "..Epoch 10\n",
      "....Outputs 1.4409 1.8985 2.2447 1.6792 1.8985\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "....Loss: 3.2675646994269916\n",
      "....R2: -9.192849042435022\n",
      "..Epoch 11\n",
      "....Outputs 1.8339 2.4141 2.4141 2.8514 2.1354\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "....Loss: 2.7926936941155636\n",
      "....R2: -6.445498763018043\n",
      "..Epoch 12\n",
      "....Outputs 3.0429 3.0429 2.3081 3.5855 2.6846\n",
      "....Labels  5.7012 5.6991 5.0497 5.3739 2.9694\n",
      "....Loss: 2.2323890877199135\n",
      "....R2: -3.757589696359796\n",
      "..Epoch 13\n",
      "....Outputs 3.8012 3.3304 3.8012 2.8759 4.4634\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "....Loss: 1.6060330403968386\n",
      "....R2: -1.4623858747760048\n",
      "..Epoch 14\n",
      "....Outputs 3.5428 4.6960 4.0406 4.6960 5.4793\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 1.043656140521598\n",
      "....R2: -0.03982932092082869\n",
      "..Epoch 15\n",
      "....Outputs 4.7193 5.6982 4.2834 6.5630 5.6982\n",
      "....Labels  2.9694 5.7012 5.0497 5.3739 5.6991\n",
      "....Loss: 1.0062983767804468\n",
      "....R2: 0.03327990430376149\n",
      "..Epoch 16\n",
      "....Outputs 7.5070 5.0037 6.6696 5.2224 6.6696\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "....Loss: 1.517124967533742\n",
      "....R2: -1.1973028088422462\n",
      "..Epoch 17\n",
      "....Outputs 7.3538 7.3538 8.0388 5.5404 5.4236\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 1.9408354812463864\n",
      "....R2: -2.596040974914758\n",
      "..Epoch 18\n",
      "....Outputs 5.8075 5.3273 7.6319 8.1057 7.6319\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "....Loss: 2.0523108599887667\n",
      "....R2: -3.020994266150706\n",
      "..Epoch 19\n",
      "....Outputs 5.0242 7.8231 7.5613 5.8381 7.5613\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "....Loss: 1.885192316803471\n",
      "....R2: -2.3928020311658744\n",
      "..Epoch 20\n",
      "....Outputs 5.6987 7.3279 7.2601 7.2601 4.6043\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "....Loss: 1.534874732531883\n",
      "....R2: -1.2490184559631805\n",
      "..Epoch 21\n",
      "....Outputs 6.8283 6.8283 4.1390 5.4584 6.7357\n",
      "....Labels  5.6991 5.7012 2.9694 5.0497 5.3739\n",
      "....Loss: 1.089509099081137\n",
      "....R2: -0.13320610568624192\n",
      "..Epoch 22\n",
      "....Outputs 3.6880 5.1734 6.1308 6.3539 6.3539\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 0.6259785382839211\n",
      "....R2: 0.6259186204164158\n",
      "..Epoch 23\n",
      "....Outputs 5.8956 5.8956 3.2816 5.5692 4.8909\n",
      "....Labels  5.7012 5.6991 2.9694 5.3739 5.0497\n",
      "....Loss: 0.21782274928121567\n",
      "....R2: 0.9547046470598217\n",
      "..Epoch 24\n",
      "....Outputs 5.0852 2.9332 5.4907 5.4907 4.6419\n",
      "....Labels  5.3739 2.9694 5.6991 5.7012 5.0497\n",
      "....Loss: 0.2602817599222531\n",
      "....R2: 0.935325263139317\n",
      "..Epoch 25\n",
      "....Outputs 5.1622 4.6919 4.4444 2.6527 5.1622\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "....Loss: 0.5496508042500964\n",
      "....R2: 0.7115829546656631\n",
      "..Epoch 26\n",
      "....Outputs 2.4368 4.9173 4.3922 4.9173 4.3076\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 0.7776838162598665\n",
      "....R2: 0.4226311872985996\n",
      "..Epoch 27\n",
      "....Outputs 4.7562 4.7562 2.2794 4.1827 4.2336\n",
      "....Labels  5.7012 5.6991 2.9694 5.3739 5.0497\n",
      "....Loss: 0.9320385800734489\n",
      "....R2: 0.17069370538010387\n",
      "..Epoch 28\n",
      "....Outputs 4.6732 4.0559 4.2207 4.6732 2.1727\n",
      "....Labels  5.6991 5.3739 5.0497 5.7012 2.9694\n",
      "....Loss: 1.0166623613130887\n",
      "....R2: 0.013264699770997757\n",
      "..Epoch 29\n",
      "....Outputs 4.6623 4.2654 2.1107 4.6623 4.0027\n",
      "....Labels  5.6991 5.0497 2.9694 5.7012 5.3739\n",
      "....Loss: 1.0379604719533329\n",
      "....R2: -0.028510723274304928\n",
      "..Epoch 30\n",
      "....Outputs 4.7164 4.7164 4.3627 2.0872 4.0152\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "....Loss: 1.0031842555908077\n",
      "....R2: 0.03925392891668833\n",
      "..Epoch 31\n",
      "....Outputs 4.5070 4.8277 2.0964 4.8277 4.0853\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "....Loss: 0.9208095341767175\n",
      "....R2: 0.1905559869530954\n",
      "..Epoch 32\n",
      "....Outputs 4.9881 4.2055 4.6912 4.9881 2.1336\n",
      "....Labels  5.7012 5.3739 5.0497 5.6991 2.9694\n",
      "....Loss: 0.8007962481326989\n",
      "....R2: 0.3878029228624127\n",
      "..Epoch 33\n",
      "....Outputs 5.1879 4.3675 4.9060 2.1948 5.1879\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "....Loss: 0.6570222262058802\n",
      "....R2: 0.5878955799668228\n",
      "..Epoch 34\n",
      "....Outputs 5.4150 2.2767 4.5616 5.4150 5.1395\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 0.5119465910391128\n",
      "....R2: 0.7497946982275603\n",
      "..Epoch 35\n",
      "....Outputs 4.7759 5.6545 2.3729 5.6545 5.3760\n",
      "....Labels  5.3739 5.6991 2.9694 5.7012 5.0497\n",
      "....Loss: 0.40597337694917773\n",
      "....R2: 0.8426588651260527\n",
      "..Epoch 36\n",
      "....Outputs 5.8884 5.5967 2.4757 5.8884 4.9961\n",
      "....Labels  5.6991 5.0497 2.9694 5.7012 5.3739\n",
      "....Loss: 0.3889897833894648\n",
      "....R2: 0.8555480009181589\n",
      "..Epoch 37\n",
      "....Outputs 2.5771 5.7815 6.0970 6.0970 5.2061\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "....Loss: 0.4544262125467813\n",
      "....R2: 0.8028603762262058\n",
      "..Epoch 38\n",
      "....Outputs 2.6693 6.2610 5.3892 5.9122 6.2610\n",
      "....Labels  2.9694 5.6991 5.3739 5.0497 5.7012\n",
      "....Loss: 0.5410073895980966\n",
      "....R2: 0.7205825151131524\n",
      "..Epoch 39\n",
      "....Outputs 6.3654 2.7447 5.5314 5.9767 6.3654\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 0.6032531068566157\n",
      "....R2: 0.652586789992126\n",
      "..Epoch 40\n",
      "....Outputs 6.4020 6.4020 5.9704 2.7987 5.6243\n",
      "....Labels  5.7012 5.6991 5.0497 2.9694 5.3739\n",
      "....Loss: 0.6204325437296898\n",
      "....R2: 0.6325177704921824\n",
      "..Epoch 41\n",
      "....Outputs 5.6637 6.3717 6.3717 2.8296 5.8981\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "....Loss: 0.587410309215137\n",
      "....R2: 0.6705949104892616\n",
      "..Epoch 42\n",
      "....Outputs 2.8393 5.6556 5.7715 6.2837 6.2837\n",
      "....Labels  2.9694 5.3739 5.0497 5.7012 5.6991\n",
      "....Loss: 0.5095610518690512\n",
      "....R2: 0.7521210394056296\n",
      "..Epoch 43\n",
      "....Outputs 5.6090 6.1527 6.1527 2.8320 5.6072\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "....Loss: 0.39864747741429307\n",
      "....R2: 0.848286144188629\n",
      "..Epoch 44\n",
      "....Outputs 5.4231 2.8136 5.9966 5.9966 5.5367\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "....Loss: 0.27051596508754994\n",
      "....R2: 0.9301392940581772\n",
      "..Epoch 45\n",
      "....Outputs 2.7907 5.2361 5.4524 5.8327 5.8327\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "....Loss: 0.1469701532296497\n",
      "....R2: 0.9793792016574303\n",
      "..Epoch 46\n",
      "....Outputs 5.6766 5.0609 5.6766 5.3684 2.7692\n",
      "....Labels  5.7012 5.0497 5.6991 5.3739 2.9694\n",
      "....Loss: 0.09095278174835743\n",
      "....R2: 0.9921026905568282\n",
      "Verified that a small batch can be overfit since the R2 was greater than 0.99\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x) # Spoiler! The \"bug\" is here.\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "..Epoch 0\n",
      "....Outputs -0.2129 -0.2110 -0.2108 -0.2121 -0.2108\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "....Loss: 5.270354459014804\n",
      "....R2: -25.517149731526693\n",
      "..Epoch 1\n",
      "....Outputs 0.1781 0.1784 0.1827 0.1819 0.1819\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "....Loss: 4.88613753975193\n",
      "....R2: -21.791796180172497\n",
      "..Epoch 2\n",
      "....Outputs 0.5725 0.5667 0.5741 0.5669 0.5725\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "....Loss: 4.505401760057725\n",
      "....R2: -18.378235741519422\n",
      "..Epoch 3\n",
      "....Outputs 0.9643 0.9562 0.9669 0.9643 0.9569\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "....Loss: 4.125180558385563\n",
      "....R2: -15.24550127125686\n",
      "..Epoch 4\n",
      "....Outputs 1.3496 1.3604 1.3510 1.3604 1.3639\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 3.7432067375029527\n",
      "....R2: -12.376264207770964\n",
      "..Epoch 5\n",
      "....Outputs 1.7634 1.7678 1.7634 1.7496 1.7517\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "....Loss: 3.3578128465001713\n",
      "....R2: -9.763664563424774\n",
      "..Epoch 6\n",
      "....Outputs 2.1806 2.1754 2.1610 2.1754 2.1582\n",
      "....Labels  5.3739 5.6991 2.9694 5.7012 5.0497\n",
      "....Loss: 2.9684801480974126\n",
      "....R2: -7.412313924193882\n",
      "..Epoch 7\n",
      "....Outputs 2.5976 2.5976 2.6036 2.5764 2.5800\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "....Loss: 2.576658536973032\n",
      "....R2: -5.338126516254333\n",
      "..Epoch 8\n",
      "....Outputs 3.0297 3.0083 3.0364 3.0039 3.0297\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 2.1869400397629355\n",
      "....R2: -3.565842505640081\n",
      "..Epoch 9\n",
      "....Outputs 3.4699 3.4435 3.4771 3.4391 3.4699\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "....Loss: 1.80913889765971\n",
      "....R2: -2.124575308238751\n",
      "..Epoch 10\n",
      "....Outputs 3.9218 3.8779 3.9143 3.8813 3.9143\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "....Loss: 1.462311776593525\n",
      "....R2: -1.0413952894381695\n",
      "..Epoch 11\n",
      "....Outputs 4.3641 4.3565 4.3145 4.3565 4.3140\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "....Loss: 1.1816321002242858\n",
      "....R2: -0.33294356622276666\n",
      "..Epoch 12\n",
      "....Outputs 4.7337 4.7944 4.7382 4.7870 4.7870\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "....Loss: 1.0211144209552023\n",
      "....R2: 0.0046036581086930095\n",
      "..Epoch 13\n",
      "....Outputs 5.1375 5.1929 5.1261 5.1929 5.1996\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "....Loss: 1.0201819086550845\n",
      "....R2: 0.006420937908448554\n",
      "..Epoch 14\n",
      "....Outputs 5.4764 5.5583 5.5583 5.4961 5.5639\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "....Loss: 1.1454724292542535\n",
      "....R2: -0.2526117288731222\n",
      "..Epoch 15\n",
      "....Outputs 5.7685 5.7973 5.8700 5.8661 5.8661\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "....Loss: 1.3187198852427144\n",
      "....R2: -0.6601689432845921\n",
      "..Epoch 16\n",
      "....Outputs 6.1023 5.9895 6.1042 6.1023 6.0275\n",
      "....Labels  5.7012 2.9694 5.3739 5.6991 5.0497\n",
      "....Loss: 1.4787545055278846\n",
      "....R2: -1.0875617634959371\n",
      "..Epoch 17\n",
      "....Outputs 6.2595 6.1331 6.2599 6.2599 6.1800\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "....Loss: 1.5935826681219114\n",
      "....R2: -1.4243560254034633\n",
      "..Epoch 18\n",
      "....Outputs 6.2008 6.3400 6.2559 6.3400 6.3372\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "....Loss: 1.6519075724305503\n",
      "....R2: -1.6050655063252606\n",
      "..Epoch 19\n",
      "....Outputs 6.2623 6.1999 6.3496 6.3496 6.3442\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "....Loss: 1.6547785286364032\n",
      "....R2: -1.6141285775054173\n",
      "..Epoch 20\n",
      "....Outputs 6.2991 6.2093 6.1405 6.2913 6.2991\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "....Loss: 1.6099590982252854\n",
      "....R2: -1.4744396367306574\n",
      "..Epoch 21\n",
      "....Outputs 6.0344 6.2003 6.2003 6.1088 6.1901\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "....Loss: 1.528564199664819\n",
      "....R2: -1.2305632052474\n",
      "..Epoch 22\n",
      "....Outputs 6.0651 5.8933 6.0528 5.9724 6.0651\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "....Loss: 1.4232079263214457\n",
      "....R2: -0.9336767856548587\n",
      "..Epoch 23\n",
      "....Outputs 5.8905 5.9048 5.8114 5.9048 5.7281\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "....Loss: 1.3070066022173514\n",
      "....R2: -0.630807761040566\n",
      "..Epoch 24\n",
      "....Outputs 5.7299 5.6359 5.7137 5.7299 5.5490\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "....Loss: 1.1929131155775066\n",
      "....R2: -0.35851615696962225\n",
      "..Epoch 25\n",
      "....Outputs 5.5494 5.4548 5.3648 5.5494 5.5314\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "....Loss: 1.0929175887768707\n",
      "....R2: -0.14030764549115915\n",
      "..Epoch 26\n",
      "....Outputs 5.3513 5.3709 5.1830 5.2759 5.3709\n",
      "....Labels  5.3739 5.7012 2.9694 5.0497 5.6991\n",
      "....Loss: 1.0166961303464703\n",
      "....R2: 0.013199103225032038\n",
      "..Epoch 27\n",
      "....Outputs 5.2009 5.0096 5.1798 5.2009 5.1053\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 0.9697239194835898\n",
      "....R2: 0.10227479211911028\n",
      "..Epoch 28\n",
      "....Outputs 4.9479 5.0443 5.0443 4.8496 5.0218\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 0.9518012876266648\n",
      "....R2: 0.1351520814884114\n",
      "..Epoch 29\n",
      "....Outputs 4.7062 4.9048 4.9048 4.8074 4.8809\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "....Loss: 0.9574497475646612\n",
      "....R2: 0.1248567335325711\n",
      "..Epoch 30\n",
      "....Outputs 4.7849 4.7849 4.7597 4.6862 4.5820\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "....Loss: 0.9782997852684331\n",
      "....R2: 0.0863263648364857\n",
      "..Epoch 31\n",
      "....Outputs 4.5858 4.6861 4.6596 4.4782 4.6861\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "....Loss: 1.0058590194553376\n",
      "....R2: 0.03412394323831658\n",
      "..Epoch 32\n",
      "....Outputs 4.5067 4.6090 4.6090 4.5812 4.3954\n",
      "....Labels  5.0497 5.7012 5.6991 5.3739 2.9694\n",
      "....Loss: 1.0332282771778993\n",
      "....R2: -0.019153928428184974\n",
      "..Epoch 33\n",
      "....Outputs 4.5536 4.4489 4.5246 4.3333 4.5536\n",
      "....Labels  5.6991 5.0497 5.3739 2.9694 5.7012\n",
      "....Loss: 1.0556094061070689\n",
      "....R2: -0.0637846183295987\n",
      "..Epoch 34\n",
      "....Outputs 4.5194 4.2914 4.4890 4.4118 4.5194\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "....Loss: 1.0701410197977288\n",
      "....R2: -0.0932744623280981\n",
      "..Epoch 35\n",
      "....Outputs 4.2684 4.3943 4.4734 4.5052 4.5052\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "....Loss: 1.075517070969966\n",
      "....R2: -0.10428662081944107\n",
      "..Epoch 36\n",
      "....Outputs 4.5097 4.4765 4.2630 4.5097 4.3951\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "....Loss: 1.0716244529866004\n",
      "....R2: -0.09630756490943515\n",
      "..Epoch 37\n",
      "....Outputs 4.5311 4.4125 4.5311 4.2736 4.4965\n",
      "....Labels  5.7012 5.0497 5.6991 2.9694 5.3739\n",
      "....Loss: 1.0592435412017562\n",
      "....R2: -0.07112178019245463\n",
      "..Epoch 38\n",
      "....Outputs 4.5677 4.4446 4.5677 4.5315 4.2981\n",
      "....Labels  5.6991 5.0497 5.7012 5.3739 2.9694\n",
      "....Loss: 1.0398286561852093\n",
      "....R2: -0.032216372993752795\n",
      "..Epoch 39\n",
      "....Outputs 4.3346 4.5795 4.6172 4.4893 4.6172\n",
      "....Labels  2.9694 5.3739 5.6991 5.0497 5.7012\n",
      "....Loss: 1.0153267642610395\n",
      "....R2: 0.015855554555313733\n",
      "..Epoch 40\n",
      "....Outputs 4.6773 4.3808 4.5445 4.6773 4.6382\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 0.9880076395272256\n",
      "....R2: 0.068103283849558\n",
      "..Epoch 41\n",
      "....Outputs 4.7458 4.7051 4.7458 4.4344 4.6077\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "....Loss: 0.9602737305223342\n",
      "....R2: 0.11968661820707172\n",
      "..Epoch 42\n",
      "....Outputs 4.4930 4.8199 4.6765 4.8199 4.7777\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "....Loss: 0.9344349985411322\n",
      "....R2: 0.1664236165720887\n",
      "..Epoch 43\n",
      "....Outputs 4.7483 4.5542 4.8535 4.8972 4.8972\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "....Loss: 0.9124415444240616\n",
      "....R2: 0.20520109151747745\n",
      "..Epoch 44\n",
      "....Outputs 4.8206 4.6153 4.9749 4.9749 4.9297\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "....Loss: 0.8956232526912535\n",
      "....R2: 0.2342307977397502\n",
      "..Epoch 45\n",
      "....Outputs 4.6740 5.0506 5.0040 5.0506 4.8908\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "....Loss: 0.8844931142807458\n",
      "....R2: 0.25314532784628785\n",
      "..Epoch 46\n",
      "....Outputs 5.0738 4.9567 5.1219 5.1219 4.7281\n",
      "....Labels  5.3739 5.0497 5.6991 5.7012 2.9694\n",
      "....Loss: 0.8786935167351506\n",
      "....R2: 0.2629073877656378\n",
      "..Epoch 47\n",
      "....Outputs 5.1371 5.1867 5.1867 4.7755 5.0161\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "....Loss: 0.8771186451512513\n",
      "....R2: 0.2655472862685422\n",
      "..Epoch 48\n",
      "....Outputs 5.1921 5.0672 5.2430 4.8146 5.2430\n",
      "....Labels  5.3739 5.0497 5.7012 2.9694 5.6991\n",
      "....Loss: 0.8781773914713464\n",
      "....R2: 0.2637730816375501\n",
      "..Epoch 49\n",
      "....Outputs 4.8440 5.2897 5.2897 5.2373 5.1086\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 0.8801255986683406\n",
      "....R2: 0.26050287026648355\n",
      "..Epoch 50\n",
      "....Outputs 5.2718 5.3256 5.3256 4.8628 5.1394\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "....Loss: 0.8813699059042295\n",
      "....R2: 0.2584104183602649\n",
      "..Epoch 51\n",
      "....Outputs 5.1592 4.8708 5.2952 5.3504 5.3504\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "....Loss: 0.8806926967139145\n",
      "....R2: 0.25954961348227645\n",
      "..Epoch 52\n",
      "....Outputs 4.8680 5.3642 5.1681 5.3642 5.3075\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 0.8773702466297642\n",
      "....R2: 0.2651258280142328\n",
      "..Epoch 53\n",
      "....Outputs 5.3675 5.1666 4.8551 5.3093 5.3675\n",
      "....Labels  5.7012 5.0497 2.9694 5.3739 5.6991\n",
      "....Loss: 0.8711981945972809\n",
      "....R2: 0.2754287118498292\n",
      "..Epoch 54\n",
      "....Outputs 4.8330 5.1556 5.3613 5.3613 5.3016\n",
      "....Labels  2.9694 5.0497 5.6991 5.7012 5.3739\n",
      "....Loss: 0.8624347316538002\n",
      "....R2: 0.28993245742979445\n",
      "..Epoch 55\n",
      "....Outputs 4.8030 5.3469 5.1364 5.3469 5.2857\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "....Loss: 0.8516913850174591\n",
      "....R2: 0.3075129227151182\n",
      "..Epoch 56\n",
      "....Outputs 5.3259 4.7666 5.1107 5.3259 5.2631\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "....Loss: 0.8397893488807904\n",
      "....R2: 0.3267320397835276\n",
      "..Epoch 57\n",
      "....Outputs 5.0800 5.2999 5.2999 4.7256 5.2356\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "....Loss: 0.827605282346831\n",
      "....R2: 0.3461265963529243\n",
      "..Epoch 58\n",
      "....Outputs 5.2049 5.2708 5.0463 5.2708 4.6816\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 0.8159261870869206\n",
      "....R2: 0.3644511695710426\n",
      "..Epoch 59\n",
      "....Outputs 5.1729 5.2404 5.0111 5.2404 4.6362\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "....Loss: 0.8053347012616638\n",
      "....R2: 0.380844134202009\n",
      "..Epoch 60\n",
      "....Outputs 4.5910 5.1411 4.9761 5.2102 5.2102\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "....Loss: 0.7961440005928722\n",
      "....R2: 0.3948954731163046\n",
      "..Epoch 61\n",
      "....Outputs 5.1817 4.9428 4.5473 5.1817 5.1110\n",
      "....Labels  5.6991 5.0497 2.9694 5.7012 5.3739\n",
      "....Loss: 0.788383607296328\n",
      "....R2: 0.406634412012873\n",
      "..Epoch 62\n",
      "....Outputs 4.5063 5.0840 5.1563 4.9125 5.1563\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "....Loss: 0.7818378144469686\n",
      "....R2: 0.416446716726914\n",
      "..Epoch 63\n",
      "....Outputs 5.0611 4.8861 4.4689 5.1350 5.1350\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "....Loss: 0.7761215493138863\n",
      "....R2: 0.4249486192286157\n",
      "..Epoch 64\n",
      "....Outputs 5.1186 4.8645 4.4357 5.1186 5.0430\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "....Loss: 0.7707703195195509\n",
      "....R2: 0.4328509887099864\n",
      "..Epoch 65\n",
      "....Outputs 5.1075 5.1075 4.4072 4.8481 5.0303\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "....Loss: 0.7653299658493009\n",
      "....R2: 0.4408290101284039\n",
      "..Epoch 66\n",
      "....Outputs 5.1020 4.3836 5.0231 5.1020 4.8371\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "....Loss: 0.7594243241605707\n",
      "....R2: 0.4494253595286216\n",
      "..Epoch 67\n",
      "....Outputs 4.8316 4.3646 5.1022 5.1022 5.0217\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "....Loss: 0.7528020649175265\n",
      "....R2: 0.4589856507543808\n",
      "..Epoch 68\n",
      "....Outputs 4.3502 5.1077 5.1077 5.0256 4.8314\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "....Loss: 0.7453537711626617\n",
      "....R2: 0.469638350686278\n",
      "..Epoch 69\n",
      "....Outputs 4.3399 5.1182 5.1182 5.0345 4.8359\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "....Loss: 0.7371055889992654\n",
      "....R2: 0.48131152442656855\n",
      "..Epoch 70\n",
      "....Outputs 5.1331 5.0477 5.1331 4.3331 4.8447\n",
      "....Labels  5.7012 5.3739 5.6991 2.9694 5.0497\n",
      "....Loss: 0.7281969206821635\n",
      "....R2: 0.49377356102508474\n",
      "..Epoch 71\n",
      "....Outputs 5.0646 4.3292 4.8570 5.1516 5.1516\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "....Loss: 0.7188392252065524\n",
      "....R2: 0.5067004451836541\n",
      "..Epoch 72\n",
      "....Outputs 4.8720 5.0843 4.3273 5.1729 5.1729\n",
      "....Labels  5.0497 5.3739 2.9694 5.6991 5.7012\n",
      "....Loss: 0.7092763336397143\n",
      "....R2: 0.5197381139204343\n",
      "..Epoch 73\n",
      "....Outputs 5.1059 4.3268 5.1962 4.8888 5.1962\n",
      "....Labels  5.3739 2.9694 5.6991 5.0497 5.7012\n",
      "....Loss: 0.6997382509090874\n",
      "....R2: 0.532568024320061\n",
      "..Epoch 74\n",
      "....Outputs 5.2204 5.2204 5.1286 4.9066 4.3267\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "....Loss: 0.6904065291474478\n",
      "....R2: 0.5449522688924873\n",
      "..Epoch 75\n",
      "....Outputs 5.2448 4.3264 5.2448 4.9245 5.1513\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "....Loss: 0.6813897269558676\n",
      "....R2: 0.5567605987931172\n",
      "..Epoch 76\n",
      "....Outputs 4.9416 5.2684 5.2684 5.1734 4.3252\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "....Loss: 0.672714418738343\n",
      "....R2: 0.5679751923231801\n",
      "..Epoch 77\n",
      "....Outputs 4.9573 5.1940 5.2907 4.3223 5.2907\n",
      "....Labels  5.0497 5.3739 5.6991 2.9694 5.7012\n",
      "....Loss: 0.6643308882544706\n",
      "....R2: 0.5786761196244187\n",
      "..Epoch 78\n",
      "....Outputs 4.9709 4.3174 5.3108 5.2125 5.3108\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "....Loss: 0.6561326194418153\n",
      "....R2: 0.5890107749939276\n",
      "..Epoch 79\n",
      "....Outputs 5.3285 5.2285 4.9820 4.3101 5.3285\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "....Loss: 0.6479847717880002\n",
      "....R2: 0.5991547099874049\n",
      "..Epoch 80\n",
      "....Outputs 4.3001 5.2416 5.3433 5.3433 4.9902\n",
      "....Labels  2.9694 5.3739 5.6991 5.7012 5.0497\n",
      "....Loss: 0.6397516531332227\n",
      "....R2: 0.6092760726364082\n",
      "..Epoch 81\n",
      "....Outputs 5.2517 5.3551 4.2873 4.9955 5.3551\n",
      "....Labels  5.3739 5.7012 2.9694 5.0497 5.6991\n",
      "....Loss: 0.631322849904499\n",
      "....R2: 0.619503894335939\n",
      "..Epoch 82\n",
      "....Outputs 4.2719 5.3640 5.2589 5.3640 4.9979\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 0.6226304914059624\n",
      "....R2: 0.6299094673857443\n",
      "..Epoch 83\n",
      "....Outputs 4.2539 5.3700 5.3700 4.9974 5.2632\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "....Loss: 0.6136554378296871\n",
      "....R2: 0.6405021093360053\n",
      "..Epoch 84\n",
      "....Outputs 5.3735 5.2649 4.2336 5.3735 4.9945\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "....Loss: 0.6044249859310139\n",
      "....R2: 0.6512357270549429\n",
      "..Epoch 85\n",
      "....Outputs 5.2644 4.9894 5.3748 4.2115 5.3748\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "....Loss: 0.5950013827059412\n",
      "....R2: 0.6620260908269009\n",
      "..Epoch 86\n",
      "....Outputs 5.3745 5.2623 4.1879 4.9826 5.3745\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "....Loss: 0.5854658899168672\n",
      "....R2: 0.6727720778197708\n",
      "..Epoch 87\n",
      "....Outputs 5.3729 4.9747 4.1633 5.3729 5.2589\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "....Loss: 0.5759004891271116\n",
      "....R2: 0.6833772752035558\n",
      "..Epoch 88\n",
      "....Outputs 4.9662 5.3707 5.2549 4.1380 5.3707\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "....Loss: 0.5663741530170224\n",
      "....R2: 0.6937655586343884\n",
      "..Epoch 89\n",
      "....Outputs 4.1127 4.9575 5.3684 5.3684 5.2507\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "....Loss: 0.5569303059970027\n",
      "....R2: 0.7038928583370871\n",
      "..Epoch 90\n",
      "....Outputs 4.0875 5.2469 4.9492 5.3664 5.3664\n",
      "....Labels  2.9694 5.3739 5.0497 5.7012 5.6991\n",
      "....Loss: 0.5475844005519553\n",
      "....R2: 0.7137474675316153\n",
      "..Epoch 91\n",
      "....Outputs 5.3652 5.2438 5.3652 4.9417 4.0630\n",
      "....Labels  5.7012 5.3739 5.6991 5.0497 2.9694\n",
      "....Loss: 0.5383240437764117\n",
      "....R2: 0.7233473948911301\n",
      "..Epoch 92\n",
      "....Outputs 5.3650 5.3650 4.9353 5.2419 4.0393\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "....Loss: 0.5291171454989575\n",
      "....R2: 0.7327296174492257\n",
      "..Epoch 93\n",
      "....Outputs 4.0166 5.3662 5.2413 5.3662 4.9302\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "....Loss: 0.5199211493185412\n",
      "....R2: 0.7419391210138823\n",
      "..Epoch 94\n",
      "....Outputs 4.9267 5.3689 5.3689 5.2422 3.9951\n",
      "....Labels  5.0497 5.7012 5.6991 5.3739 2.9694\n",
      "....Loss: 0.5106929952297559\n",
      "....R2: 0.7510185674810826\n",
      "..Epoch 95\n",
      "....Outputs 4.9248 5.2447 5.3732 3.9748 5.3732\n",
      "....Labels  5.0497 5.3739 5.6991 2.9694 5.7012\n",
      "....Loss: 0.5013988715199819\n",
      "....R2: 0.759998527256113\n",
      "..Epoch 96\n",
      "....Outputs 5.2488 4.9245 5.3790 3.9557 5.3790\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "....Loss: 0.4920203515472403\n",
      "....R2: 0.7688928732169522\n",
      "..Epoch 97\n",
      "....Outputs 3.9377 4.9257 5.3863 5.2543 5.3863\n",
      "....Labels  2.9694 5.0497 5.7012 5.3739 5.6991\n",
      "....Loss: 0.48255574148993924\n",
      "....R2: 0.7776986135647359\n",
      "..Epoch 98\n",
      "....Outputs 5.3948 5.2611 3.9206 4.9282 5.3948\n",
      "....Labels  5.6991 5.3739 2.9694 5.0497 5.7012\n",
      "....Loss: 0.47301997227167913\n",
      "....R2: 0.7863975967490175\n",
      "..Epoch 99\n",
      "....Outputs 5.4044 5.2690 3.9043 5.4044 4.9318\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "....Loss: 0.46343866732676053\n",
      "....R2: 0.7949632468827685\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The maximum R2 over 100 epochs was not greater than 0.99",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3d84f2f421bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 193\u001b[0;31m                                of data. The maximum R2 over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not greater than {k.DL_DBG_SUFFICIENT_R2_SMALL_BATCH}''')\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the R2 was greater than {k.DL_DBG_SUFFICIENT_R2_SMALL_BATCH}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The maximum R2 over 100 epochs was not greater than 0.99"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not mix information from separate instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graphnet.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph_batch2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data to illustrate the point\n",
    "\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "\n",
    "data_set_ = train_X_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=2)\n",
    "        x = x - x_mean[:, :, None] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0) # Spoiler! this is the bug.\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data\n",
    "The capacity of your architecture should be just large enough to overfit the training data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this cell should return the integer 1\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run all tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_all_tests=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}