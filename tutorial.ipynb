{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim, reshape\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Remove all 'importlib' statements\n",
    "1. Consider using `trainer` function for all tests in `dl_debug`\n",
    "1. Clean up arguments passed into DebugSession\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Try using all the polymers for \"chart dependencies\" instead of a small sample\n",
    "1. Change FFNet to MyNet\n",
    "1. Change overfit small batch to an R2 requirement\n",
    "1. Delete this cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "len(data_df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3380"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class FFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(FFNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : FFNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "data_set = {}\n",
    "data_set['train'] = train_X\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here. The correct line is 'return x.view(data.num_graphs,)'\n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-80c0c8fa0ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 134.40209197998047\n",
      "..last epoch zero_data_loss 134.75465393066406\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-66eab23cba70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    159\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    160\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    162\u001b[0m                 )\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 5.164145787847168\n",
      "....Outputs -0.1179 -0.0832 -0.0910 -0.1023 -0.1179\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 1\n",
      "....Loss: 5.1002019730011945\n",
      "....Outputs -0.0351 -0.0503 -0.0165 -0.0337 -0.0503\n",
      "....Labels  2.9694 5.7012 5.3739 5.0497 5.6991\n",
      "..Epoch 2\n",
      "....Loss: 5.040911384104809\n",
      "....Outputs 0.0199 0.0257 0.0519 0.0095 0.0095\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 3\n",
      "....Loss: 4.973188421236432\n",
      "....Outputs 0.0799 0.1340 0.0778 0.0778 0.0908\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 4.888306674583275\n",
      "....Outputs 0.2393 0.1645 0.1709 0.1514 0.1645\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "..Epoch 5\n",
      "....Loss: 4.7807606964063485\n",
      "....Outputs 0.2755 0.2755 0.2396 0.2719 0.3727\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 6\n",
      "....Loss: 4.644630629981769\n",
      "....Outputs 0.5403 0.4175 0.3498 0.4175 0.4008\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 4.474223379589342\n",
      "....Outputs 0.4866 0.7494 0.5966 0.5966 0.5624\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 8\n",
      "....Loss: 4.262562752986111\n",
      "....Outputs 1.0087 0.7640 0.6550 0.8208 0.8208\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 9\n",
      "....Loss: 4.000683249294005\n",
      "....Outputs 1.1004 0.8625 1.3298 1.0138 1.1004\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 10\n",
      "....Loss: 3.6791838931022047\n",
      "....Outputs 1.4460 1.4460 1.3233 1.7261 1.1165\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "..Epoch 11\n",
      "....Loss: 3.2884296161716975\n",
      "....Outputs 1.7023 1.8692 2.2155 1.8692 1.4256\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 12\n",
      "....Loss: 2.8176726352501023\n",
      "....Outputs 2.3882 2.8105 2.3882 2.1640 1.8034\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 13\n",
      "....Loss: 2.2646333826378098\n",
      "....Outputs 2.2582 2.7181 3.0192 3.5265 3.0192\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "..Epoch 14\n",
      "....Loss: 1.6494165198046378\n",
      "....Outputs 4.3795 2.8022 3.3662 3.7759 3.7759\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "..Epoch 15\n",
      "....Loss: 1.0886559799013982\n",
      "....Outputs 4.6727 4.0771 3.4411 5.3692 4.6727\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.0137416690620507\n",
      "....Outputs 4.7595 5.6851 6.4372 5.6851 4.1538\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 17\n",
      "....Loss: 1.5032213905818341\n",
      "....Outputs 6.6780 4.8544 7.3817 5.2751 6.6780\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 18\n",
      "....Loss: 1.9383025814360197\n",
      "....Outputs 7.3862 5.4967 7.3862 7.9418 5.3938\n",
      "....Labels  5.7012 2.9694 5.6991 5.3739 5.0497\n",
      "..Epoch 19\n",
      "....Loss: 2.0708371055563553\n",
      "....Outputs 7.6840 5.4178 7.6840 8.0510 5.6893\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "..Epoch 20\n",
      "....Loss: 1.922636734027545\n",
      "....Outputs 5.7587 7.8103 7.6210 7.6210 5.1292\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.5886829834496452\n",
      "....Outputs 7.3226 7.3511 7.3226 4.7229 5.6589\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 22\n",
      "....Loss: 1.1536622344057446\n",
      "....Outputs 6.8872 4.2626 5.4514 6.8872 6.7891\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 23\n",
      "....Loss: 0.6942094990871401\n",
      "....Outputs 5.1924 6.4081 6.4081 3.8045 6.2042\n",
      "....Labels  5.0497 5.6991 5.7012 2.9694 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 0.27488628496386613\n",
      "....Outputs 4.9270 3.3841 5.6515 5.9386 5.9386\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 25\n",
      "....Loss: 0.21981452876476146\n",
      "....Outputs 4.6885 5.5193 5.1660 5.5193 3.0193\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "Verified that a small batch can be overfit since the RMSE was less than 0.22499709319526628\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x) # Spoiler! The \"bug\" is here.\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 4.805500439356576\n",
      "....Outputs 0.2629 0.2636 0.2640 0.2640 0.2620\n",
      "....Labels  5.3739 5.0497 5.6991 5.7012 2.9694\n",
      "..Epoch 1\n",
      "....Loss: 4.435932796055207\n",
      "....Outputs 0.6406 0.6434 0.6434 0.6420 0.6415\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 2\n",
      "....Loss: 4.065092697306583\n",
      "....Outputs 1.0243 1.0260 1.0221 1.0260 1.0225\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 3\n",
      "....Loss: 3.689960887050353\n",
      "....Outputs 1.4155 1.4155 1.4135 1.4100 1.4102\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 3.3075887871990797\n",
      "....Outputs 1.8080 1.8161 1.8138 1.8161 1.8091\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 5\n",
      "....Loss: 2.9168664772983215\n",
      "....Outputs 2.2281 2.2307 2.2216 2.2307 2.2195\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 6\n",
      "....Loss: 2.519406243113757\n",
      "....Outputs 2.6488 2.6607 2.6607 2.6577 2.6456\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 7\n",
      "....Loss: 2.1208775868573455\n",
      "....Outputs 3.1025 3.1058 3.1058 3.0903 3.0862\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 8\n",
      "....Loss: 1.7338260778610044\n",
      "....Outputs 3.5432 3.5601 3.5637 3.5637 3.5387\n",
      "....Labels  2.9694 5.3739 5.6991 5.7012 5.0497\n",
      "..Epoch 9\n",
      "....Loss: 1.3835805710305429\n",
      "....Outputs 4.0294 4.0255 4.0015 4.0294 3.9984\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 10\n",
      "....Loss: 1.1180571792359077\n",
      "....Outputs 4.4945 4.4560 4.4945 4.4567 4.4900\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 11\n",
      "....Loss: 1.00505095651603\n",
      "....Outputs 4.9460 4.9408 4.9008 4.8938 4.9460\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 12\n",
      "....Loss: 1.0690141585637287\n",
      "....Outputs 5.3663 5.3600 5.3131 5.2978 5.3663\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 13\n",
      "....Loss: 1.2396269023942597\n",
      "....Outputs 5.7335 5.6471 5.7256 5.6721 5.7335\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 14\n",
      "....Loss: 1.425915270215542\n",
      "....Outputs 5.9219 5.9570 6.0164 6.0264 6.0264\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 15\n",
      "....Loss: 1.5722411349266978\n",
      "....Outputs 6.1546 6.2313 6.1096 6.2187 6.2313\n",
      "....Labels  5.0497 5.7012 2.9694 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.6558170832447898\n",
      "....Outputs 6.2084 6.3300 6.3453 6.2625 6.3453\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 17\n",
      "....Loss: 1.6739146545193468\n",
      "....Outputs 6.3570 6.3751 6.3751 6.2873 6.2253\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 18\n",
      "....Loss: 1.6344405823016586\n",
      "....Outputs 6.3327 6.1722 6.2410 6.3118 6.3327\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 19\n",
      "....Loss: 1.5503934668459405\n",
      "....Outputs 6.2088 6.2322 6.1378 6.2322 6.0632\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 20\n",
      "....Loss: 1.4369855872528479\n",
      "....Outputs 6.0883 5.9919 6.0624 6.0883 5.9124\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.3102363411814306\n",
      "....Outputs 5.9146 5.9146 5.8169 5.7335 5.8865\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 22\n",
      "....Loss: 1.186149180221001\n",
      "....Outputs 5.7238 5.6936 5.6252 5.5385 5.7238\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "..Epoch 23\n",
      "....Loss: 1.0796563150895606\n",
      "....Outputs 5.5267 5.3381 5.4276 5.5267 5.4948\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 1.002628269979226\n",
      "....Outputs 5.1413 5.3329 5.2993 5.3329 5.2333\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 25\n",
      "....Loss: 0.9609766076046345\n",
      "....Outputs 5.0496 5.1147 4.9554 5.1497 5.1497\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 26\n",
      "....Loss: 0.9526929657880722\n",
      "....Outputs 4.9468 4.9833 4.8824 4.9833 4.7859\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 27\n",
      "....Loss: 0.9691682958438876\n",
      "....Outputs 4.7356 4.8376 4.7998 4.6368 4.8376\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 28\n",
      "....Loss: 0.9992042295878756\n",
      "....Outputs 4.7155 4.5105 4.6120 4.6764 4.7155\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 29\n",
      "....Loss: 1.0326671688537492\n",
      "....Outputs 4.6184 4.5780 4.4083 4.6184 4.5130\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 30\n",
      "....Loss: 1.0621343432259394\n",
      "....Outputs 4.4388 4.5468 4.3304 4.5468 4.5050\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "..Epoch 31\n",
      "....Loss: 1.0829944386185344\n",
      "....Outputs 4.4570 4.3891 4.5003 4.5003 4.2762\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 32\n",
      "....Loss: 1.0929372219858227\n",
      "....Outputs 4.2448 4.4777 4.3629 4.4777 4.4329\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 33\n",
      "....Loss: 1.0913923739163534\n",
      "....Outputs 4.2344 4.4777 4.4311 4.4777 4.3585\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 34\n",
      "....Loss: 1.079070630212774\n",
      "....Outputs 4.4982 4.4982 4.3742 4.4499 4.2433\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 35\n",
      "....Loss: 1.0576429989131608\n",
      "....Outputs 4.4077 4.5371 4.5371 4.2690 4.4869\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 36\n",
      "....Loss: 1.0294969860030931\n",
      "....Outputs 4.5919 4.4566 4.5397 4.3093 4.5919\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 37\n",
      "....Loss: 0.9975241171041463\n",
      "....Outputs 4.3614 4.6598 4.6056 4.6598 4.5182\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 38\n",
      "....Loss: 0.9648815255743177\n",
      "....Outputs 4.4226 4.7379 4.6815 4.7379 4.5895\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "..Epoch 39\n",
      "....Loss: 0.9346764323893912\n",
      "....Outputs 4.8229 4.4898 4.7645 4.8229 4.6677\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 40\n",
      "....Loss: 0.9095701368781034\n",
      "....Outputs 4.9117 4.5600 4.8512 4.7494 4.9117\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 41\n",
      "....Loss: 0.8913522060370198\n",
      "....Outputs 4.8315 5.0010 4.9384 5.0010 4.6300\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 42\n",
      "....Loss: 0.8806075182249623\n",
      "....Outputs 4.6968 4.9108 5.0228 5.0875 5.0875\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 43\n",
      "....Loss: 0.8766271244373088\n",
      "....Outputs 5.1014 5.1680 4.9842 5.1680 4.7576\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 44\n",
      "....Loss: 0.8776314539680883\n",
      "....Outputs 5.2400 5.0492 4.8097 5.2400 5.1714\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 45\n",
      "....Loss: 0.8812335587719614\n",
      "....Outputs 5.3011 5.3011 5.2306 5.1035 4.8511\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 46\n",
      "....Loss: 0.8849773700967407\n",
      "....Outputs 5.3495 5.1453 4.8803 5.3495 5.2771\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 47\n",
      "....Loss: 0.8867955447648992\n",
      "....Outputs 5.3843 4.8963 5.1738 5.3843 5.3101\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 48\n",
      "....Loss: 0.8852987472699397\n",
      "....Outputs 5.4051 4.8990 5.3292 5.4051 5.1885\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 49\n",
      "....Loss: 0.879891822082931\n",
      "....Outputs 5.1900 5.3347 4.8889 5.4125 5.4125\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 50\n",
      "....Loss: 0.8707346839028719\n",
      "....Outputs 5.4073 4.8669 5.1792 5.4073 5.3278\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 51\n",
      "....Loss: 0.8585994689028964\n",
      "....Outputs 5.1577 5.3100 4.8347 5.3912 5.3912\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 52\n",
      "....Loss: 0.8446523644820416\n",
      "....Outputs 5.2833 5.1275 5.3662 5.3662 4.7942\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 53\n",
      "....Loss: 0.8302037761057399\n",
      "....Outputs 5.3347 5.2500 4.7476 5.3347 5.0908\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 54\n",
      "....Loss: 0.8164592524941318\n",
      "....Outputs 4.6971 5.2989 5.0500 5.2989 5.2125\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 55\n",
      "....Loss: 0.804320131162967\n",
      "....Outputs 4.6449 5.2613 5.1732 5.2613 5.0073\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 56\n",
      "....Loss: 0.7942532438404672\n",
      "....Outputs 5.2240 4.9650 5.2240 4.5930 5.1343\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 57\n",
      "....Loss: 0.7862669646672308\n",
      "....Outputs 5.1892 5.0978 4.5433 5.1892 4.9251\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 58\n",
      "....Loss: 0.7799800469218372\n",
      "....Outputs 4.4972 5.1585 5.1585 5.0654 4.8891\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 59\n",
      "....Loss: 0.7747558378336324\n",
      "....Outputs 4.8585 5.0384 5.1334 4.4559 5.1334\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "..Epoch 60\n",
      "....Loss: 0.7698637345733651\n",
      "....Outputs 5.1146 4.4201 5.0179 4.8340 5.1146\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 61\n",
      "....Loss: 0.7646262602921209\n",
      "....Outputs 5.0045 4.8164 5.1030 5.1030 4.3904\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 62\n",
      "....Loss: 0.7585283025290552\n",
      "....Outputs 5.0987 5.0987 4.9984 4.8058 4.3668\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 63\n",
      "....Loss: 0.7512755673035985\n",
      "....Outputs 4.8021 4.3492 5.1016 5.1016 4.9994\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 64\n",
      "....Loss: 0.7428097977065513\n",
      "....Outputs 4.3372 4.8049 5.1112 5.1112 5.0072\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "..Epoch 65\n",
      "....Loss: 0.7332799742171616\n",
      "....Outputs 5.0210 5.1270 5.1270 4.3300 4.8136\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 66\n",
      "....Loss: 0.7229888975206399\n",
      "....Outputs 4.3268 5.1479 5.1479 5.0400 4.8271\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 67\n",
      "....Loss: 0.7123198013647741\n",
      "....Outputs 5.1729 4.8446 4.3267 5.0630 5.1729\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "..Epoch 68\n",
      "....Loss: 0.7016596890357047\n",
      "....Outputs 4.3285 5.0889 4.8647 5.2007 5.2007\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 69\n",
      "....Loss: 0.6913264815873218\n",
      "....Outputs 5.1164 4.8863 4.3313 5.2300 5.2300\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "..Epoch 70\n",
      "....Loss: 0.6815181720561068\n",
      "....Outputs 5.2597 5.2597 4.3338 4.9082 5.1441\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 71\n",
      "....Loss: 0.6722888115758315\n",
      "....Outputs 5.2886 4.3352 4.9292 5.2886 5.1710\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 72\n",
      "....Loss: 0.6635549962148646\n",
      "....Outputs 5.3154 5.1960 4.3345 5.3154 4.9482\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 73\n",
      "....Loss: 0.6551304986770649\n",
      "....Outputs 4.3311 5.3395 4.9646 5.3395 5.2181\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 74\n",
      "....Loss: 0.6467787196429693\n",
      "....Outputs 4.9775 5.3600 5.3600 5.2368 4.3244\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "..Epoch 75\n",
      "....Loss: 0.6382689998949966\n",
      "....Outputs 5.3766 4.3142 4.9865 5.3766 5.2514\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 76\n",
      "....Loss: 0.6294232014946849\n",
      "....Outputs 5.2620 4.3003 4.9916 5.3890 5.3890\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "..Epoch 77\n",
      "....Loss: 0.6201467672186717\n",
      "....Outputs 5.3974 5.2685 5.3974 4.9927 4.2829\n",
      "....Labels  5.7012 5.3739 5.6991 5.0497 2.9694\n",
      "..Epoch 78\n",
      "....Loss: 0.6104376160039681\n",
      "....Outputs 5.2712 5.4019 5.4019 4.9901 4.2621\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "..Epoch 79\n",
      "....Loss: 0.6003735422596815\n",
      "....Outputs 5.4032 5.2705 4.2386 4.9844 5.4032\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "..Epoch 80\n",
      "....Loss: 0.5900861987088514\n",
      "....Outputs 4.2128 4.9760 5.2672 5.4018 5.4018\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 81\n",
      "....Loss: 0.579726321213472\n",
      "....Outputs 5.3984 4.9658 5.3984 4.1854 5.2619\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 82\n",
      "....Loss: 0.5694292361727803\n",
      "....Outputs 5.3938 5.3938 4.9545 5.2554 4.1570\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 83\n",
      "....Loss: 0.5592894323530254\n",
      "....Outputs 5.3888 5.2485 5.3888 4.9429 4.1283\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 84\n",
      "....Loss: 0.5493464076008651\n",
      "....Outputs 5.2420 4.0998 5.3841 5.3841 4.9316\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "..Epoch 85\n",
      "....Loss: 0.5395842252696949\n",
      "....Outputs 5.3804 4.0721 4.9213 5.2364 5.3804\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 86\n",
      "....Loss: 0.5299442175533777\n",
      "....Outputs 5.2323 5.3782 5.3782 4.9126 4.0455\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 87\n",
      "....Loss: 0.5203444634319783\n",
      "....Outputs 4.9057 5.3779 4.0204 5.3779 5.2302\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 88\n",
      "....Loss: 0.5107023322052716\n",
      "....Outputs 5.3797 3.9970 5.3797 4.9011 5.2302\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 89\n",
      "....Loss: 0.5009533317191649\n",
      "....Outputs 3.9753 5.3838 4.8988 5.3838 5.2325\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 90\n",
      "....Loss: 0.4910637727982867\n",
      "....Outputs 4.8987 5.3902 3.9553 5.3902 5.2371\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 91\n",
      "....Loss: 0.4810351777854053\n",
      "....Outputs 4.9007 5.2437 5.3985 5.3985 3.9368\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 92\n",
      "....Loss: 0.47089934315351106\n",
      "....Outputs 5.4087 5.4087 4.9045 3.9196 5.2522\n",
      "....Labels  5.7012 5.6991 5.0497 2.9694 5.3739\n",
      "..Epoch 93\n",
      "....Loss: 0.46070993592763615\n",
      "....Outputs 4.9098 3.9034 5.4202 5.2620 5.4202\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "..Epoch 94\n",
      "....Loss: 0.4505271102673805\n",
      "....Outputs 5.4327 4.9162 5.4327 3.8880 5.2728\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 95\n",
      "....Loss: 0.44040510696872376\n",
      "....Outputs 5.2842 5.4457 5.4457 3.8729 4.9232\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 96\n",
      "....Loss: 0.4303809206165061\n",
      "....Outputs 5.4588 3.8578 5.4588 4.9304 5.2956\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 97\n",
      "....Loss: 0.4204706731076811\n",
      "....Outputs 3.8425 5.4714 5.4714 5.3066 4.9373\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 98\n",
      "....Loss: 0.4106677544339954\n",
      "....Outputs 4.9437 3.8266 5.3170 5.4834 5.4834\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 99\n",
      "....Loss: 0.4009496926039271\n",
      "....Outputs 3.8100 5.4943 4.9492 5.4943 5.3262\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3d84f2f421bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 191\u001b[0;31m                                of data. The minimum RMSE over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not less than {self.EPSILON}''')\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the RMSE was less than {self.EPSILON}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not mix information from separate instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graphnet.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph_batch2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# data to illustrate the point\n",
    "\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()\n",
    "polymer_smiles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[*]C(C#N)=C([*])c1ccccc1',\n",
       " '[*]CCCCOC(=O)C(=O)O[*]',\n",
       " '[*]CC(CCl)(CCl)C(=O)O[*]',\n",
       " '[*]c1[nH]c([*])c(C(=O)O)c1C']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# feature_array = np.zeros((N_DATA_, MAX_N_ATOMS, N_FEATURES_))\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "# for ind, smiles in enumerate(polymer_smiles):\n",
    "#     feature_array[ind, ].append(featurize_smiles_by_atom(smiles))\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "# for smiles,data in zip(polymer_smiles,train_X_):\n",
    "#     data.num_atoms = Chem.MolFromSmiles(smiles).GetNumAtoms()\n",
    "data_set_ = {'train': train_X_}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=2)\n",
    "        x = x - x_mean[:, :, None] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 0.5427 0.4155 0.3948 0.4339\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: 0.542738676071167\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0)\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.1925 -0.2155 -0.2169 -0.1876\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.19253747165203094\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Data is getting mixed between instances in the same batch.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-64c781c65389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_chart_dependencies=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest_model_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_chart_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_choose_model_size_by_overfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mchart_dependencies\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# print(data.x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is getting mixed between instances in the same batch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished charting dependencies. Data is not getting mixed between instances in the same batch.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is getting mixed between instances in the same batch."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data\n",
    "The capacity of your architecture should be just large enough to overfit the training data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run all tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_all_tests=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}