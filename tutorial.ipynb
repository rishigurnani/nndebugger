{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Remove all 'importlib' statements\n",
    "1. Consider using `trainer` function for all tests in `dl_debug`\n",
    "1. Clean up arguments passed into DebugSession\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Try using all the polymers for \"chart dependencies\" instead of a small sample\n",
    "1. Change FFNet to MyNet\n",
    "1. Change overfit small batch to an R2 requirement\n",
    "1. Delete this cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "len(data_df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3380"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class FFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(FFNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : FFNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "data_set = {}\n",
    "data_set['train'] = train_X\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here. The correct line is 'return x.view(data.num_graphs,)'\n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-80c0c8fa0ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 134.40209197998047\n",
      "..last epoch zero_data_loss 134.75465393066406\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-66eab23cba70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    159\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    160\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    162\u001b[0m                 )\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 5.164145787847168\n",
      "....Outputs -0.1179 -0.0832 -0.0910 -0.1023 -0.1179\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 1\n",
      "....Loss: 5.1002019730011945\n",
      "....Outputs -0.0351 -0.0503 -0.0165 -0.0337 -0.0503\n",
      "....Labels  2.9694 5.7012 5.3739 5.0497 5.6991\n",
      "..Epoch 2\n",
      "....Loss: 5.040911384104809\n",
      "....Outputs 0.0199 0.0257 0.0519 0.0095 0.0095\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 3\n",
      "....Loss: 4.973188421236432\n",
      "....Outputs 0.0799 0.1340 0.0778 0.0778 0.0908\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 4.888306674583275\n",
      "....Outputs 0.2393 0.1645 0.1709 0.1514 0.1645\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "..Epoch 5\n",
      "....Loss: 4.7807606964063485\n",
      "....Outputs 0.2755 0.2755 0.2396 0.2719 0.3727\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 6\n",
      "....Loss: 4.644630629981769\n",
      "....Outputs 0.5403 0.4175 0.3498 0.4175 0.4008\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 4.474223379589342\n",
      "....Outputs 0.4866 0.7494 0.5966 0.5966 0.5624\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 8\n",
      "....Loss: 4.262562752986111\n",
      "....Outputs 1.0087 0.7640 0.6550 0.8208 0.8208\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 9\n",
      "....Loss: 4.000683249294005\n",
      "....Outputs 1.1004 0.8625 1.3298 1.0138 1.1004\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 10\n",
      "....Loss: 3.6791838931022047\n",
      "....Outputs 1.4460 1.4460 1.3233 1.7261 1.1165\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "..Epoch 11\n",
      "....Loss: 3.2884296161716975\n",
      "....Outputs 1.7023 1.8692 2.2155 1.8692 1.4256\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 12\n",
      "....Loss: 2.8176726352501023\n",
      "....Outputs 2.3882 2.8105 2.3882 2.1640 1.8034\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 13\n",
      "....Loss: 2.2646333826378098\n",
      "....Outputs 2.2582 2.7181 3.0192 3.5265 3.0192\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "..Epoch 14\n",
      "....Loss: 1.6494165198046378\n",
      "....Outputs 4.3795 2.8022 3.3662 3.7759 3.7759\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "..Epoch 15\n",
      "....Loss: 1.0886559799013982\n",
      "....Outputs 4.6727 4.0771 3.4411 5.3692 4.6727\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.0137416690620507\n",
      "....Outputs 4.7595 5.6851 6.4372 5.6851 4.1538\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 17\n",
      "....Loss: 1.5032213905818341\n",
      "....Outputs 6.6780 4.8544 7.3817 5.2751 6.6780\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 18\n",
      "....Loss: 1.9383025814360197\n",
      "....Outputs 7.3862 5.4967 7.3862 7.9418 5.3938\n",
      "....Labels  5.7012 2.9694 5.6991 5.3739 5.0497\n",
      "..Epoch 19\n",
      "....Loss: 2.0708371055563553\n",
      "....Outputs 7.6840 5.4178 7.6840 8.0510 5.6893\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "..Epoch 20\n",
      "....Loss: 1.922636734027545\n",
      "....Outputs 5.7587 7.8103 7.6210 7.6210 5.1292\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.5886829834496452\n",
      "....Outputs 7.3226 7.3511 7.3226 4.7229 5.6589\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 22\n",
      "....Loss: 1.1536622344057446\n",
      "....Outputs 6.8872 4.2626 5.4514 6.8872 6.7891\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 23\n",
      "....Loss: 0.6942094990871401\n",
      "....Outputs 5.1924 6.4081 6.4081 3.8045 6.2042\n",
      "....Labels  5.0497 5.6991 5.7012 2.9694 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 0.27488628496386613\n",
      "....Outputs 4.9270 3.3841 5.6515 5.9386 5.9386\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 25\n",
      "....Loss: 0.21981452876476146\n",
      "....Outputs 4.6885 5.5193 5.1660 5.5193 3.0193\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "Verified that a small batch can be overfit since the RMSE was less than 0.22499709319526628\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x) # Spoiler! The \"bug\" is here.\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 4.805500439356576\n",
      "....Outputs 0.2629 0.2636 0.2640 0.2640 0.2620\n",
      "....Labels  5.3739 5.0497 5.6991 5.7012 2.9694\n",
      "..Epoch 1\n",
      "....Loss: 4.435932796055207\n",
      "....Outputs 0.6406 0.6434 0.6434 0.6420 0.6415\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 2\n",
      "....Loss: 4.065092697306583\n",
      "....Outputs 1.0243 1.0260 1.0221 1.0260 1.0225\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 3\n",
      "....Loss: 3.689960887050353\n",
      "....Outputs 1.4155 1.4155 1.4135 1.4100 1.4102\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 3.3075887871990797\n",
      "....Outputs 1.8080 1.8161 1.8138 1.8161 1.8091\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 5\n",
      "....Loss: 2.9168664772983215\n",
      "....Outputs 2.2281 2.2307 2.2216 2.2307 2.2195\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 6\n",
      "....Loss: 2.519406243113757\n",
      "....Outputs 2.6488 2.6607 2.6607 2.6577 2.6456\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 7\n",
      "....Loss: 2.1208775868573455\n",
      "....Outputs 3.1025 3.1058 3.1058 3.0903 3.0862\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 8\n",
      "....Loss: 1.7338260778610044\n",
      "....Outputs 3.5432 3.5601 3.5637 3.5637 3.5387\n",
      "....Labels  2.9694 5.3739 5.6991 5.7012 5.0497\n",
      "..Epoch 9\n",
      "....Loss: 1.3835805710305429\n",
      "....Outputs 4.0294 4.0255 4.0015 4.0294 3.9984\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 10\n",
      "....Loss: 1.1180571792359077\n",
      "....Outputs 4.4945 4.4560 4.4945 4.4567 4.4900\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 11\n",
      "....Loss: 1.00505095651603\n",
      "....Outputs 4.9460 4.9408 4.9008 4.8938 4.9460\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 12\n",
      "....Loss: 1.0690141585637287\n",
      "....Outputs 5.3663 5.3600 5.3131 5.2978 5.3663\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 13\n",
      "....Loss: 1.2396269023942597\n",
      "....Outputs 5.7335 5.6471 5.7256 5.6721 5.7335\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 14\n",
      "....Loss: 1.425915270215542\n",
      "....Outputs 5.9219 5.9570 6.0164 6.0264 6.0264\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 15\n",
      "....Loss: 1.5722411349266978\n",
      "....Outputs 6.1546 6.2313 6.1096 6.2187 6.2313\n",
      "....Labels  5.0497 5.7012 2.9694 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.6558170832447898\n",
      "....Outputs 6.2084 6.3300 6.3453 6.2625 6.3453\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 17\n",
      "....Loss: 1.6739146545193468\n",
      "....Outputs 6.3570 6.3751 6.3751 6.2873 6.2253\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 18\n",
      "....Loss: 1.6344405823016586\n",
      "....Outputs 6.3327 6.1722 6.2410 6.3118 6.3327\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 19\n",
      "....Loss: 1.5503934668459405\n",
      "....Outputs 6.2088 6.2322 6.1378 6.2322 6.0632\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 20\n",
      "....Loss: 1.4369855872528479\n",
      "....Outputs 6.0883 5.9919 6.0624 6.0883 5.9124\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.3102363411814306\n",
      "....Outputs 5.9146 5.9146 5.8169 5.7335 5.8865\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 22\n",
      "....Loss: 1.186149180221001\n",
      "....Outputs 5.7238 5.6936 5.6252 5.5385 5.7238\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "..Epoch 23\n",
      "....Loss: 1.0796563150895606\n",
      "....Outputs 5.5267 5.3381 5.4276 5.5267 5.4948\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 1.002628269979226\n",
      "....Outputs 5.1413 5.3329 5.2993 5.3329 5.2333\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 25\n",
      "....Loss: 0.9609766076046345\n",
      "....Outputs 5.0496 5.1147 4.9554 5.1497 5.1497\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 26\n",
      "....Loss: 0.9526929657880722\n",
      "....Outputs 4.9468 4.9833 4.8824 4.9833 4.7859\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 27\n",
      "....Loss: 0.9691682958438876\n",
      "....Outputs 4.7356 4.8376 4.7998 4.6368 4.8376\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 28\n",
      "....Loss: 0.9992042295878756\n",
      "....Outputs 4.7155 4.5105 4.6120 4.6764 4.7155\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 29\n",
      "....Loss: 1.0326671688537492\n",
      "....Outputs 4.6184 4.5780 4.4083 4.6184 4.5130\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 30\n",
      "....Loss: 1.0621343432259394\n",
      "....Outputs 4.4388 4.5468 4.3304 4.5468 4.5050\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "..Epoch 31\n",
      "....Loss: 1.0829944386185344\n",
      "....Outputs 4.4570 4.3891 4.5003 4.5003 4.2762\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 32\n",
      "....Loss: 1.0929372219858227\n",
      "....Outputs 4.2448 4.4777 4.3629 4.4777 4.4329\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 33\n",
      "....Loss: 1.0913923739163534\n",
      "....Outputs 4.2344 4.4777 4.4311 4.4777 4.3585\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 34\n",
      "....Loss: 1.079070630212774\n",
      "....Outputs 4.4982 4.4982 4.3742 4.4499 4.2433\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 35\n",
      "....Loss: 1.0576429989131608\n",
      "....Outputs 4.4077 4.5371 4.5371 4.2690 4.4869\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 36\n",
      "....Loss: 1.0294969860030931\n",
      "....Outputs 4.5919 4.4566 4.5397 4.3093 4.5919\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 37\n",
      "....Loss: 0.9975241171041463\n",
      "....Outputs 4.3614 4.6598 4.6056 4.6598 4.5182\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 38\n",
      "....Loss: 0.9648815255743177\n",
      "....Outputs 4.4226 4.7379 4.6815 4.7379 4.5895\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "..Epoch 39\n",
      "....Loss: 0.9346764323893912\n",
      "....Outputs 4.8229 4.4898 4.7645 4.8229 4.6677\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 40\n",
      "....Loss: 0.9095701368781034\n",
      "....Outputs 4.9117 4.5600 4.8512 4.7494 4.9117\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 41\n",
      "....Loss: 0.8913522060370198\n",
      "....Outputs 4.8315 5.0010 4.9384 5.0010 4.6300\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 42\n",
      "....Loss: 0.8806075182249623\n",
      "....Outputs 4.6968 4.9108 5.0228 5.0875 5.0875\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 43\n",
      "....Loss: 0.8766271244373088\n",
      "....Outputs 5.1014 5.1680 4.9842 5.1680 4.7576\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 44\n",
      "....Loss: 0.8776314539680883\n",
      "....Outputs 5.2400 5.0492 4.8097 5.2400 5.1714\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 45\n",
      "....Loss: 0.8812335587719614\n",
      "....Outputs 5.3011 5.3011 5.2306 5.1035 4.8511\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 46\n",
      "....Loss: 0.8849773700967407\n",
      "....Outputs 5.3495 5.1453 4.8803 5.3495 5.2771\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 47\n",
      "....Loss: 0.8867955447648992\n",
      "....Outputs 5.3843 4.8963 5.1738 5.3843 5.3101\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 48\n",
      "....Loss: 0.8852987472699397\n",
      "....Outputs 5.4051 4.8990 5.3292 5.4051 5.1885\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 49\n",
      "....Loss: 0.879891822082931\n",
      "....Outputs 5.1900 5.3347 4.8889 5.4125 5.4125\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 50\n",
      "....Loss: 0.8707346839028719\n",
      "....Outputs 5.4073 4.8669 5.1792 5.4073 5.3278\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 51\n",
      "....Loss: 0.8585994689028964\n",
      "....Outputs 5.1577 5.3100 4.8347 5.3912 5.3912\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 52\n",
      "....Loss: 0.8446523644820416\n",
      "....Outputs 5.2833 5.1275 5.3662 5.3662 4.7942\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 53\n",
      "....Loss: 0.8302037761057399\n",
      "....Outputs 5.3347 5.2500 4.7476 5.3347 5.0908\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 54\n",
      "....Loss: 0.8164592524941318\n",
      "....Outputs 4.6971 5.2989 5.0500 5.2989 5.2125\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 55\n",
      "....Loss: 0.804320131162967\n",
      "....Outputs 4.6449 5.2613 5.1732 5.2613 5.0073\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 56\n",
      "....Loss: 0.7942532438404672\n",
      "....Outputs 5.2240 4.9650 5.2240 4.5930 5.1343\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 57\n",
      "....Loss: 0.7862669646672308\n",
      "....Outputs 5.1892 5.0978 4.5433 5.1892 4.9251\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 58\n",
      "....Loss: 0.7799800469218372\n",
      "....Outputs 4.4972 5.1585 5.1585 5.0654 4.8891\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 59\n",
      "....Loss: 0.7747558378336324\n",
      "....Outputs 4.8585 5.0384 5.1334 4.4559 5.1334\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "..Epoch 60\n",
      "....Loss: 0.7698637345733651\n",
      "....Outputs 5.1146 4.4201 5.0179 4.8340 5.1146\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 61\n",
      "....Loss: 0.7646262602921209\n",
      "....Outputs 5.0045 4.8164 5.1030 5.1030 4.3904\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 62\n",
      "....Loss: 0.7585283025290552\n",
      "....Outputs 5.0987 5.0987 4.9984 4.8058 4.3668\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 63\n",
      "....Loss: 0.7512755673035985\n",
      "....Outputs 4.8021 4.3492 5.1016 5.1016 4.9994\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 64\n",
      "....Loss: 0.7428097977065513\n",
      "....Outputs 4.3372 4.8049 5.1112 5.1112 5.0072\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "..Epoch 65\n",
      "....Loss: 0.7332799742171616\n",
      "....Outputs 5.0210 5.1270 5.1270 4.3300 4.8136\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 66\n",
      "....Loss: 0.7229888975206399\n",
      "....Outputs 4.3268 5.1479 5.1479 5.0400 4.8271\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 67\n",
      "....Loss: 0.7123198013647741\n",
      "....Outputs 5.1729 4.8446 4.3267 5.0630 5.1729\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "..Epoch 68\n",
      "....Loss: 0.7016596890357047\n",
      "....Outputs 4.3285 5.0889 4.8647 5.2007 5.2007\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 69\n",
      "....Loss: 0.6913264815873218\n",
      "....Outputs 5.1164 4.8863 4.3313 5.2300 5.2300\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "..Epoch 70\n",
      "....Loss: 0.6815181720561068\n",
      "....Outputs 5.2597 5.2597 4.3338 4.9082 5.1441\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 71\n",
      "....Loss: 0.6722888115758315\n",
      "....Outputs 5.2886 4.3352 4.9292 5.2886 5.1710\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 72\n",
      "....Loss: 0.6635549962148646\n",
      "....Outputs 5.3154 5.1960 4.3345 5.3154 4.9482\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 73\n",
      "....Loss: 0.6551304986770649\n",
      "....Outputs 4.3311 5.3395 4.9646 5.3395 5.2181\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 74\n",
      "....Loss: 0.6467787196429693\n",
      "....Outputs 4.9775 5.3600 5.3600 5.2368 4.3244\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "..Epoch 75\n",
      "....Loss: 0.6382689998949966\n",
      "....Outputs 5.3766 4.3142 4.9865 5.3766 5.2514\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 76\n",
      "....Loss: 0.6294232014946849\n",
      "....Outputs 5.2620 4.3003 4.9916 5.3890 5.3890\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "..Epoch 77\n",
      "....Loss: 0.6201467672186717\n",
      "....Outputs 5.3974 5.2685 5.3974 4.9927 4.2829\n",
      "....Labels  5.7012 5.3739 5.6991 5.0497 2.9694\n",
      "..Epoch 78\n",
      "....Loss: 0.6104376160039681\n",
      "....Outputs 5.2712 5.4019 5.4019 4.9901 4.2621\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "..Epoch 79\n",
      "....Loss: 0.6003735422596815\n",
      "....Outputs 5.4032 5.2705 4.2386 4.9844 5.4032\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "..Epoch 80\n",
      "....Loss: 0.5900861987088514\n",
      "....Outputs 4.2128 4.9760 5.2672 5.4018 5.4018\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 81\n",
      "....Loss: 0.579726321213472\n",
      "....Outputs 5.3984 4.9658 5.3984 4.1854 5.2619\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 82\n",
      "....Loss: 0.5694292361727803\n",
      "....Outputs 5.3938 5.3938 4.9545 5.2554 4.1570\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 83\n",
      "....Loss: 0.5592894323530254\n",
      "....Outputs 5.3888 5.2485 5.3888 4.9429 4.1283\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 84\n",
      "....Loss: 0.5493464076008651\n",
      "....Outputs 5.2420 4.0998 5.3841 5.3841 4.9316\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "..Epoch 85\n",
      "....Loss: 0.5395842252696949\n",
      "....Outputs 5.3804 4.0721 4.9213 5.2364 5.3804\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 86\n",
      "....Loss: 0.5299442175533777\n",
      "....Outputs 5.2323 5.3782 5.3782 4.9126 4.0455\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 87\n",
      "....Loss: 0.5203444634319783\n",
      "....Outputs 4.9057 5.3779 4.0204 5.3779 5.2302\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 88\n",
      "....Loss: 0.5107023322052716\n",
      "....Outputs 5.3797 3.9970 5.3797 4.9011 5.2302\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 89\n",
      "....Loss: 0.5009533317191649\n",
      "....Outputs 3.9753 5.3838 4.8988 5.3838 5.2325\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 90\n",
      "....Loss: 0.4910637727982867\n",
      "....Outputs 4.8987 5.3902 3.9553 5.3902 5.2371\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 91\n",
      "....Loss: 0.4810351777854053\n",
      "....Outputs 4.9007 5.2437 5.3985 5.3985 3.9368\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 92\n",
      "....Loss: 0.47089934315351106\n",
      "....Outputs 5.4087 5.4087 4.9045 3.9196 5.2522\n",
      "....Labels  5.7012 5.6991 5.0497 2.9694 5.3739\n",
      "..Epoch 93\n",
      "....Loss: 0.46070993592763615\n",
      "....Outputs 4.9098 3.9034 5.4202 5.2620 5.4202\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "..Epoch 94\n",
      "....Loss: 0.4505271102673805\n",
      "....Outputs 5.4327 4.9162 5.4327 3.8880 5.2728\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 95\n",
      "....Loss: 0.44040510696872376\n",
      "....Outputs 5.2842 5.4457 5.4457 3.8729 4.9232\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 96\n",
      "....Loss: 0.4303809206165061\n",
      "....Outputs 5.4588 3.8578 5.4588 4.9304 5.2956\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 97\n",
      "....Loss: 0.4204706731076811\n",
      "....Outputs 3.8425 5.4714 5.4714 5.3066 4.9373\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 98\n",
      "....Loss: 0.4106677544339954\n",
      "....Outputs 4.9437 3.8266 5.3170 5.4834 5.4834\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 99\n",
      "....Loss: 0.4009496926039271\n",
      "....Outputs 3.8100 5.4943 4.9492 5.4943 5.3262\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3d84f2f421bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 191\u001b[0;31m                                of data. The minimum RMSE over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not less than {self.EPSILON}''')\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the RMSE was less than {self.EPSILON}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not mix information from separate instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graphnet.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph_batch2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# data to illustrate the point\n",
    "\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()\n",
    "polymer_smiles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[*]C(C#N)=C([*])c1ccccc1',\n",
       " '[*]CCCCOC(=O)C(=O)O[*]',\n",
       " '[*]CC(CCl)(CCl)C(=O)O[*]',\n",
       " '[*]c1[nH]c([*])c(C(=O)O)c1C']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# feature_array = np.zeros((N_DATA_, MAX_N_ATOMS, N_FEATURES_))\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "# for ind, smiles in enumerate(polymer_smiles):\n",
    "#     feature_array[ind, ].append(featurize_smiles_by_atom(smiles))\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "# for smiles,data in zip(polymer_smiles,train_X_):\n",
    "#     data.num_atoms = Chem.MolFromSmiles(smiles).GetNumAtoms()\n",
    "data_set_ = {'train': train_X_}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=2)\n",
    "        x = x - x_mean[:, :, None] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 0.5427 0.4155 0.3948 0.4339\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: 0.542738676071167\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0)\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.1925 -0.2155 -0.2169 -0.1876\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.19253747165203094\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Data is getting mixed between instances in the same batch.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-64c781c65389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_chart_dependencies=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest_model_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_chart_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_choose_model_size_by_overfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mchart_dependencies\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# print(data.x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is getting mixed between instances in the same batch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished charting dependencies. Data is not getting mixed between instances in the same batch.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is getting mixed between instances in the same batch."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data\n",
    "The capacity of your architecture should be just large enough to overfit the training data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.567168941972658 [r2] -7.786489497257955\n",
      "......Outputs 0.0509 0.0604 0.0698 0.0604 0.0365 0.0808 0.0562 0.0560 0.0603 0.0559\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.567168941972658 [best r2] -7.786489497257955\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.6914496056835606 [r2] -4.740042775357589\n",
      "......Outputs 0.4901 0.7204 0.5924 0.7204 0.6978 0.9185 0.5859 0.6776 0.5902 0.8029\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.6914496056835606 [best r2] -4.740042775357589\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6362727976802036 [r2] -1.9275339331338874\n",
      "......Outputs 2.2987 3.6586 2.9156 3.6586 3.7082 4.9065 2.7765 3.5014 2.7815 4.1027\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6362727976802036 [best r2] -1.9275339331338874\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.9236150263941838 [r2] -0.558682152943532\n",
      "......Outputs 3.7625 6.7959 4.3306 6.7959 7.0080 7.4083 5.3082 5.8823 4.2955 5.7877\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.9236150263941838 [best r2] -0.558682152943532\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.7950152251140765 [r2] -0.3572426985890702\n",
      "......Outputs 2.5121 4.9889 2.3927 4.9889 5.2675 4.2389 4.2223 3.8400 2.6463 3.0045\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.7950152251140765 [best r2] -0.3572426985890702\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4919902496522939 [r2] 0.06232311363706988\n",
      "......Outputs 2.7075 5.8985 2.1561 5.8985 6.3257 3.9480 5.2281 4.0928 2.6299 2.5479\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4919902496522939 [best r2] 0.06232311363706988\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.3917735094040196 [r2] 0.1840597205728095\n",
      "......Outputs 3.4236 7.5073 2.4406 7.5073 8.0536 4.3822 6.9646 5.1626 3.0969 2.7837\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.3917735094040196 [best r2] 0.1840597205728095\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2256870612254656 [r2] 0.36717963921579255\n",
      "......Outputs 3.2638 6.3146 2.2285 6.3146 6.7205 3.6075 6.4349 4.6986 2.6882 2.5156\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2256870612254656 [best r2] 0.36717963921579255\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0657333215255484 [r2] 0.5215700814150481\n",
      "......Outputs 3.6024 5.9163 2.4711 5.9163 6.2405 3.5407 6.5752 4.8912 2.6830 2.7398\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0657333215255484 [best r2] 0.5215700814150481\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9554351185848157 [r2] 0.6154758314476064\n",
      "......Outputs 4.3713 6.1758 3.0675 6.1758 6.4800 3.8999 7.3263 5.5783 2.9350 3.2929\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9554351185848157 [best r2] 0.6154758314476064\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8383488555408521 [r2] 0.7039460959502375\n",
      "......Outputs 4.4724 5.5500 3.0417 5.5500 5.7952 3.4394 6.9911 5.2050 2.6548 3.1863\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8383488555408521 [best r2] 0.7039460959502375\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7378780947578538 [r2] 0.7706543663512719\n",
      "......Outputs 4.9178 5.6890 3.2650 5.6890 5.8854 3.3181 7.2591 5.2617 2.5885 3.2166\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7378780947578538 [best r2] 0.7706543663512719\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6566257309701073 [r2] 0.818382775086618\n",
      "......Outputs 5.1606 5.7398 3.3744 5.7398 5.8341 3.1371 7.2626 5.2183 2.4652 3.1452\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6566257309701073 [best r2] 0.818382775086618\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.5846334186097973 [r2] 0.8560245268907364\n",
      "......Outputs 5.2115 5.6220 3.3940 5.6220 5.6194 2.9578 7.0233 5.1118 2.3371 3.0607\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5846334186097973 [best r2] 0.8560245268907364\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5248905914803061 [r2] 0.883946346434663\n",
      "......Outputs 5.3452 5.7883 3.4782 5.7883 5.7245 2.9334 7.0293 5.2348 2.3029 3.1215\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5248905914803061 [best r2] 0.883946346434663\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.46867271961341694 [r2] 0.9074746810671842\n",
      "......Outputs 5.1842 5.7046 3.3147 5.7046 5.6699 2.8224 6.8219 5.0999 2.2081 3.0800\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.46867271961341694 [best r2] 0.9074746810671842\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.42194671504320497 [r2] 0.9250042830698607\n",
      "......Outputs 5.1415 5.7856 3.2695 5.7856 5.7917 2.8215 6.7887 5.1739 2.1750 3.1574\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.42194671504320497 [best r2] 0.9250042830698607\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.3802179179254348 [r2] 0.9391043319092589\n",
      "......Outputs 4.9956 5.6403 3.1726 5.6403 5.7159 2.7827 6.6235 5.1108 2.1028 3.1906\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3802179179254348 [best r2] 0.9391043319092589\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.3447647716964154 [r2] 0.949931220300668\n",
      "......Outputs 4.9807 5.7051 3.1379 5.7051 5.7369 2.7951 6.6155 5.1599 2.0635 3.2442\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3447647716964154 [best r2] 0.949931220300668\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3129202946190174 [r2] 0.9587533480687177\n",
      "......Outputs 4.9458 5.6941 3.0783 5.6941 5.6490 2.7687 6.5796 5.1205 2.0171 3.2226\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3129202946190174 [best r2] 0.9587533480687177\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.28589731106143584 [r2] 0.9655696543763258\n",
      "......Outputs 4.9653 5.7217 3.0437 5.7217 5.6081 2.7702 6.5912 5.1204 1.9976 3.2058\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.28589731106143584 [best r2] 0.9655696543763258\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2623500739102462 [r2] 0.9710076363982334\n",
      "......Outputs 4.9780 5.7363 3.0061 5.7363 5.5850 2.7681 6.6095 5.1059 1.9848 3.1827\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2623500739102462 [best r2] 0.9710076363982334\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.24136408137454055 [r2] 0.9754604540480722\n",
      "......Outputs 4.9777 5.7035 2.9674 5.7035 5.5352 2.7653 6.6063 5.0982 1.9757 3.1742\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24136408137454055 [best r2] 0.9754604540480722\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.2233014041727192 [r2] 0.9789958965349694\n",
      "......Outputs 4.9996 5.7179 2.9425 5.7179 5.5178 2.7857 6.6384 5.1288 1.9805 3.1885\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2233014041727192 [best r2] 0.9789958965349694\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.2070789501466385 [r2] 0.9819368640786688\n",
      "......Outputs 5.0155 5.7172 2.9158 5.7172 5.4940 2.8030 6.6543 5.1403 1.9900 3.2033\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2070789501466385 [best r2] 0.9819368640786688\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.1922049430665851 [r2] 0.984438539814104\n",
      "......Outputs 5.0164 5.7154 2.8892 5.7154 5.4634 2.8068 6.6567 5.1314 1.9937 3.1953\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1922049430665851 [best r2] 0.984438539814104\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.17949862221850096 [r2] 0.986428011603099\n",
      "......Outputs 5.0226 5.7261 2.8675 5.7261 5.4503 2.8141 6.6731 5.1274 2.0009 3.1793\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17949862221850096 [best r2] 0.986428011603099\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.16821961804174554 [r2] 0.9880800474680693\n",
      "......Outputs 5.0318 5.7437 2.8669 5.7437 5.4375 2.8363 6.6946 5.1400 2.0160 3.1701\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16821961804174554 [best r2] 0.9880800474680693\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1582919198108691 [r2] 0.9894454740107144\n",
      "......Outputs 5.0432 5.7538 2.8778 5.7538 5.4382 2.8544 6.7215 5.1529 2.0336 3.1543\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1582919198108691 [best r2] 0.9894454740107144\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.14884317346546805 [r2] 0.9906679064695023\n",
      "......Outputs 5.0337 5.7255 2.8794 5.7255 5.4203 2.8553 6.7134 5.1326 2.0427 3.1392\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14884317346546805 [best r2] 0.9906679064695023\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.14098179668419497 [r2] 0.9916276510145845\n",
      "......Outputs 5.0019 5.6941 2.8697 5.6941 5.3835 2.8471 6.6852 5.1026 2.0441 3.1338\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14098179668419497 [best r2] 0.9916276510145845\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.1350323565273119 [r2] 0.9923193684033809\n",
      "......Outputs 4.9920 5.7234 2.8698 5.7234 5.3609 2.8490 6.6925 5.1088 2.0481 3.1278\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1350323565273119 [best r2] 0.9923193684033809\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.13064835335691305 [r2] 0.9928099961704172\n",
      "......Outputs 5.0421 5.7971 2.9033 5.7971 5.3977 2.8777 6.7499 5.1675 2.0674 3.1348\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13064835335691305 [best r2] 0.9928099961704172\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.1334301254283252 [r2] 0.9925005566402331\n",
      "......Outputs 5.1039 5.8086 2.9580 5.8086 5.4592 2.9092 6.7873 5.2178 2.0982 3.1498\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13064835335691305 [best r2] 0.9928099961704172\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.1295307520261282 [r2] 0.9929324805106438\n",
      "......Outputs 5.0098 5.6232 2.9441 5.6232 5.3869 2.8655 6.6569 5.1099 2.0957 3.1110\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.13874234554981907 [r2] 0.9918915226945585\n",
      "......Outputs 4.8798 5.6107 2.8645 5.6107 5.2944 2.8070 6.5783 4.9994 2.0618 3.0823\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.13454170258615028 [r2] 0.9923750837235579\n",
      "......Outputs 5.0299 5.8861 2.9164 5.8861 5.3897 2.8871 6.7719 5.1896 2.0882 3.1583\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.14635747944839525 [r2] 0.9909769973712583\n",
      "......Outputs 5.2094 5.8954 3.0478 5.8954 5.5198 2.9656 6.9029 5.3488 2.1430 3.1639\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.13074686687087902 [r2] 0.9927991490442051\n",
      "......Outputs 4.9709 5.5334 2.9594 5.5334 5.3396 2.8257 6.5960 5.0520 2.1091 3.0192\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.1303855739449413 [r2] 0.9928388902945877\n",
      "......Outputs 4.8783 5.6355 2.8616 5.6355 5.3030 2.7879 6.5873 4.9858 2.0836 3.0907\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.1252536821958186 [r2] 0.9933915098803044\n",
      "......Outputs 5.1373 5.9436 2.9775 5.9436 5.4887 2.9405 6.8818 5.2851 2.1388 3.2471\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1252536821958186 [best r2] 0.9933915098803044\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.1104031955557323 [r2] 0.9948656611323815\n",
      "......Outputs 5.1235 5.7066 3.0119 5.7066 5.4190 2.9087 6.7376 5.2326 2.1350 3.0950\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1104031955557323 [best r2] 0.9948656611323815\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.11180612489911186 [r2] 0.9947343446287049\n",
      "......Outputs 4.9094 5.5913 2.8991 5.5913 5.2851 2.7839 6.5327 5.0137 2.0909 3.0033\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1104031955557323 [best r2] 0.9948656611323815\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.09688267093121489 [r2] 0.9960462113773186\n",
      "......Outputs 5.0186 5.8451 2.9290 5.8451 5.3972 2.8661 6.7390 5.1451 2.1203 3.1903\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09688267093121489 [best r2] 0.9960462113773186\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.10198091672197669 [r2] 0.9956191431322571\n",
      "......Outputs 5.1567 5.8204 3.0118 5.8204 5.4802 2.9397 6.8474 5.2551 2.1463 3.1894\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09688267093121489 [best r2] 0.9960462113773186\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.09033166204645983 [r2] 0.9965628281763895\n",
      "......Outputs 4.9969 5.5893 2.9553 5.5893 5.3456 2.8392 6.6128 5.1103 2.1097 3.0242\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09033166204645983 [best r2] 0.9965628281763895\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.08952608205444905 [r2] 0.9966238603804549\n",
      "......Outputs 4.9466 5.7137 2.9059 5.7137 5.3160 2.8154 6.6118 5.0885 2.0989 3.0897\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08952608205444905 [best r2] 0.9966238603804549\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.089789258032075 [r2] 0.9966039818200423\n",
      "......Outputs 5.1177 5.8863 2.9761 5.8863 5.4469 2.9143 6.8242 5.2379 2.1376 3.2103\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08952608205444905 [best r2] 0.9966238603804549\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.08322959290648316 [r2] 0.9970820571498199\n",
      "......Outputs 5.1060 5.6997 2.9997 5.6997 5.4277 2.9007 6.7448 5.1967 2.1385 3.1007\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08322959290648316 [best r2] 0.9970820571498199\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.08669522114239504 [r2] 0.9968339952773796\n",
      "......Outputs 4.9415 5.6088 2.9209 5.6088 5.3059 2.8109 6.5755 5.0588 2.0998 3.0323\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08322959290648316 [best r2] 0.9970820571498199\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.07912171183965351 [r2] 0.9973629850579421\n",
      "......Outputs 5.0081 5.8057 2.9250 5.8057 5.3580 2.8554 6.7066 5.1448 2.1109 3.1509\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.09044464191456292 [r2] 0.9965542249013511\n",
      "......Outputs 5.1596 5.8466 3.0093 5.8466 5.4691 2.9339 6.8420 5.2696 2.1480 3.1802\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.08046202032496791 [r2] 0.9972728871668606\n",
      "......Outputs 5.0444 5.6353 2.9843 5.6353 5.3798 2.8650 6.6619 5.1396 2.1274 3.0592\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.09137807441260833 [r2] 0.9964827337486563\n",
      "......Outputs 4.9132 5.6432 2.8990 5.6432 5.2816 2.8006 6.5685 5.0329 2.0901 3.0646\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.08616702975168057 [r2] 0.9968724555822734\n",
      "......Outputs 5.0657 5.8618 2.9471 5.8618 5.3998 2.8909 6.7759 5.2051 2.1181 3.1805\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.1021645706027314 [r2] 0.9956033502595523\n",
      "......Outputs 5.1910 5.8239 3.0376 5.8239 5.4940 2.9466 6.8476 5.2966 2.1555 3.1529\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.09525067890152313 [r2] 0.9961782928884526\n",
      "......Outputs 4.9872 5.5807 2.9647 5.5807 5.3351 2.8308 6.5946 5.0756 2.1172 3.0382\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.11076896694155626 [r2] 0.9948315841262131\n",
      "......Outputs 4.8835 5.6463 2.8694 5.6463 5.2584 2.7873 6.5570 5.0065 2.0779 3.0829\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.11198147105115076 [r2] 0.9947178153656421\n",
      "......Outputs 5.1292 5.9314 2.9694 5.9314 5.4501 2.9258 6.8483 5.2807 2.1255 3.2058\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.12852593090415176 [r2] 0.9930417062786426\n",
      "......Outputs 5.2276 5.8215 3.0731 5.8215 5.5201 2.9605 6.8598 5.3243 2.1667 3.1394\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.12784700483080025 [r2] 0.993115025172621\n",
      "......Outputs 4.9135 5.5012 2.9366 5.5012 5.2833 2.7912 6.5107 4.9891 2.0995 3.0084\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.1406116738846619 [r2] 0.9916715535558429\n",
      "......Outputs 4.8440 5.6524 2.8428 5.6524 5.2362 2.7767 6.5387 4.9852 2.0643 3.0946\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.15081287224904147 [r2] 0.9904192818669865\n",
      "......Outputs 5.2046 6.0296 3.0066 6.0296 5.5222 2.9726 6.9419 5.3597 2.1398 3.2431\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.15208108274109516 [r2] 0.9902574726751856\n",
      "......Outputs 5.2392 5.7704 3.0971 5.7704 5.5271 2.9636 6.8388 5.3177 2.1718 3.1253\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.16049192881860413 [r2] 0.9891500525716359\n",
      "......Outputs 4.8288 5.4218 2.8941 5.4218 5.2154 2.7418 6.4130 4.9233 2.0778 2.9651\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.14862645242364572 [r2] 0.9906950624162222\n",
      "......Outputs 4.8698 5.7397 2.8515 5.7397 5.2756 2.7957 6.5958 5.0442 2.0741 3.1336\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.17011753489935377 [r2] 0.9878095593983696\n",
      "......Outputs 5.2794 6.0730 3.0518 6.0730 5.5962 3.0144 7.0312 5.4092 2.1658 3.2824\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.13846391757908777 [r2] 0.9919240342046024\n",
      "......Outputs 5.1557 5.6196 3.0561 5.6196 5.4393 2.9106 6.7179 5.2217 2.1460 3.0685\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.15699144295313092 [r2] 0.9896181869690529\n",
      "......Outputs 4.7988 5.4591 2.8597 5.4591 5.1902 2.7223 6.3985 4.9370 2.0606 2.9517\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.12341782541231469 [r2] 0.9935838128835492\n",
      "......Outputs 4.9914 5.9157 2.9126 5.9157 5.3965 2.8616 6.7536 5.1763 2.1068 3.1936\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.14221714246365397 [r2] 0.9914802836222142\n",
      "......Outputs 5.2615 5.9418 3.0657 5.9418 5.5679 2.9984 6.9652 5.3465 2.1661 3.2555\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.10955607708572576 [r2] 0.9949441499282952\n",
      "......Outputs 5.0203 5.5070 2.9800 5.5070 5.3316 2.8408 6.5744 5.0958 2.1066 3.0096\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.1162560004636481 [r2] 0.9943068582561672\n",
      "......Outputs 4.8731 5.6222 2.8781 5.6222 5.2506 2.7660 6.5073 5.0310 2.0717 3.0105\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.10266295042118975 [r2] 0.9955603501069038\n",
      "......Outputs 5.1127 5.9744 2.9745 5.9744 5.4625 2.9111 6.8532 5.2557 2.1326 3.2263\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.09816570543372752 [r2] 0.9959407964980629\n",
      "......Outputs 5.1765 5.7619 3.0315 5.7619 5.4736 2.9338 6.8197 5.2367 2.1451 3.1797\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.0918782543177463 [r2] 0.9964441231564731\n",
      "......Outputs 4.9548 5.5252 2.9316 5.5252 5.2991 2.8052 6.5406 5.0563 2.0896 3.0054\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.08153974619843114 [r2] 0.9971993428191473\n",
      "......Outputs 4.9598 5.7584 2.9140 5.7584 5.3248 2.8210 6.6371 5.1221 2.0933 3.0868\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.08922403459239929 [r2] 0.996646603119368\n",
      "......Outputs 5.1490 5.9178 3.0046 5.9178 5.4681 2.9273 6.8586 5.2590 2.1400 3.2159\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.07450195532919482 [r2] 0.9976619349872378\n",
      "......Outputs 5.0998 5.6727 3.0030 5.6727 5.4124 2.8897 6.7215 5.1687 2.1274 3.1178\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07450195532919482 [best r2] 0.9976619349872378\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.08117172724019532 [r2] 0.997224566566651\n",
      "......Outputs 4.9397 5.5905 2.9209 5.5905 5.2961 2.8035 6.5610 5.0670 2.0890 3.0283\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07450195532919482 [best r2] 0.9976619349872378\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.069904084083267 [r2] 0.9979416162702905\n",
      "......Outputs 5.0156 5.8158 2.9391 5.8158 5.3654 2.8547 6.7133 5.1673 2.1078 3.1299\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.069904084083267 [best r2] 0.9979416162702905\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.08238720414291843 [r2] 0.9971408247812775\n",
      "......Outputs 5.1545 5.8579 3.0143 5.8579 5.4652 2.9265 6.8385 5.2477 2.1419 3.1899\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.069904084083267 [best r2] 0.9979416162702905\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.06917407842305526 [r2] 0.9979843830365042\n",
      "......Outputs 5.0572 5.6427 2.9860 5.6427 5.3800 2.8658 6.6707 5.1390 2.1208 3.0853\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.07838281587268837 [r2] 0.9974120077977597\n",
      "......Outputs 4.9380 5.6315 2.9149 5.6315 5.2927 2.8056 6.5764 5.0716 2.0899 3.0477\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.0707182195497979 [r2] 0.9978933912828148\n",
      "......Outputs 5.0476 5.8354 2.9512 5.8354 5.3865 2.8735 6.7488 5.1873 2.1135 3.1491\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.08461535970615454 [r2] 0.996984081155665\n",
      "......Outputs 5.1622 5.8342 3.0236 5.8342 5.4714 2.9293 6.8320 5.2518 2.1446 3.1723\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.07343907913406868 [r2] 0.9977281707546478\n",
      "......Outputs 5.0351 5.6278 2.9804 5.6278 5.3634 2.8528 6.6448 5.1236 2.1187 3.0688\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.0860900770956241 [r2] 0.9968780392813394\n",
      "......Outputs 4.9227 5.6362 2.9050 5.6362 5.2791 2.7990 6.5699 5.0577 2.0860 3.0547\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.08004304333110039 [r2] 0.997301214139004\n",
      "......Outputs 5.0630 5.8500 2.9549 5.8500 5.3965 2.8829 6.7686 5.2013 2.1141 3.1599\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.09978338818260055 [r2] 0.9958059101140074\n",
      "......Outputs 5.1909 5.8502 3.0414 5.8502 5.4950 2.9424 6.8561 5.2799 2.1525 3.1710\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.08806276512448002 [r2] 0.9967333253917278\n",
      "......Outputs 5.0271 5.6110 2.9847 5.6110 5.3581 2.8467 6.6293 5.1134 2.1217 3.0583\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.11010418903988883 [r2] 0.9948934342754577\n",
      "......Outputs 4.8804 5.5975 2.8819 5.5975 5.2440 2.7783 6.5271 5.0156 2.0754 3.0462\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.10189615661496743 [r2] 0.9956264222899498\n",
      "......Outputs 5.0586 5.8684 2.9422 5.8684 5.3927 2.8861 6.7780 5.2063 2.1086 3.1691\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.135552008142996 [r2] 0.9922601391551067\n",
      "......Outputs 5.2520 5.9160 3.0715 5.9160 5.5464 2.9777 6.9263 5.3423 2.1696 3.1927\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.1195421254496811 [r2] 0.9939804616250822\n",
      "......Outputs 5.0345 5.5830 3.0031 5.5830 5.3690 2.8515 6.6214 5.1082 2.1342 3.0541\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.15811410547232982 [r2] 0.9894691731604297\n",
      "......Outputs 4.7940 5.5033 2.8442 5.5033 5.1805 2.7331 6.4310 4.9358 2.0564 3.0160\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.1433916483247012 [r2] 0.9913389817228145\n",
      "......Outputs 5.0192 5.8875 2.9113 5.8875 5.3668 2.8716 6.7623 5.1953 2.0910 3.1747\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.1961504619002661 [r2] 0.9837931015395538\n",
      "......Outputs 5.3501 6.0496 3.1152 6.0496 5.6325 3.0347 7.0536 5.4429 2.1885 3.2368\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.1695743818848333 [r2] 0.9878872786567432\n",
      "......Outputs 5.0819 5.5535 3.0478 5.5535 5.4064 2.8720 6.6275 5.1182 2.1515 3.0387\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.22221601450330924 [r2] 0.9791995874284399\n",
      "......Outputs 4.6883 5.3683 2.8004 5.3683 5.1034 2.6759 6.2935 4.8184 2.0317 2.9507\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.642011675707765 [r2] -8.076819470438192\n",
      "......Outputs -0.0684 -0.0502 -0.0778 -0.0502 -0.0501 -0.0612 -0.0630 -0.0746 -0.0689 -0.0786\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.642011675707765 [best r2] -8.076819470438192\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.519057995418072 [r2] -4.216438473259185\n",
      "......Outputs 0.5923 0.8245 0.5988 0.8245 0.8869 1.1491 0.6124 0.8064 0.6365 0.8612\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.519057995418072 [best r2] -4.216438473259185\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6227463912092017 [r2] -1.8975693368869857\n",
      "......Outputs 3.0974 4.6626 3.5035 4.6626 4.9329 6.2121 3.4467 4.7094 3.3816 4.9824\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6227463912092017 [best r2] -1.8975693368869857\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.85620184720819 [r2] -0.4513482778293403\n",
      "......Outputs 3.7367 6.5296 3.7606 6.5296 6.9827 6.6745 5.1043 5.7848 3.8411 4.8968\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.85620184720819 [best r2] -0.4513482778293403\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8147785749770953 [r2] -0.3872940629675379\n",
      "......Outputs 2.4970 4.8040 2.0436 4.8040 5.2529 3.8096 4.1168 3.8199 2.3751 2.3979\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8147785749770953 [best r2] -0.3872940629675379\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4994158920340177 [r2] 0.052966242429975274\n",
      "......Outputs 3.0094 6.1994 2.1147 6.1994 6.8358 4.0088 5.6672 4.6646 2.6809 2.3419\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4994158920340177 [best r2] 0.052966242429975274\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.369184768594499 [r2] 0.21033050900818784\n",
      "......Outputs 3.6880 7.4364 2.4068 7.4364 8.1293 4.3416 7.2343 5.7188 3.0888 2.6463\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.369184768594499 [best r2] 0.21033050900818784\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2198852927431239 [r2] 0.37315634877424053\n",
      "......Outputs 3.3837 6.0235 2.1221 6.0235 6.5022 3.4143 6.4631 4.9915 2.6133 2.4202\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2198852927431239 [best r2] 0.37315634877424053\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0569506638926756 [r2] 0.5294230260863547\n",
      "......Outputs 3.8875 5.9058 2.5510 5.9058 6.2752 3.4860 6.9142 5.3877 2.7562 2.9157\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0569506638926756 [best r2] 0.5294230260863547\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9365427221776575 [r2] 0.6305323421071445\n",
      "......Outputs 4.6243 6.0516 3.1112 6.0516 6.3859 3.6903 7.5722 5.8962 2.9511 3.4667\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9365427221776575 [best r2] 0.6305323421071445\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8195545274697362 [r2] 0.7170713362160185\n",
      "......Outputs 4.5773 5.4424 2.9840 5.4424 5.7079 3.1600 7.1569 5.3525 2.6139 3.1334\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8195545274697362 [best r2] 0.7170713362160185\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7261465021132911 [r2] 0.777889167331694\n",
      "......Outputs 5.0693 5.6150 3.3021 5.6150 5.9528 3.1466 7.5011 5.4337 2.5963 3.1262\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7261465021132911 [best r2] 0.777889167331694\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6404641271650708 [r2] 0.8272130812269106\n",
      "......Outputs 5.1257 5.5159 3.3673 5.5159 5.7170 2.9357 7.2714 5.1895 2.4412 2.9730\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6404641271650708 [best r2] 0.8272130812269106\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.572344406610701 [r2] 0.862013650057009\n",
      "......Outputs 5.1988 5.5626 3.4748 5.5626 5.6164 2.8382 7.1163 5.1378 2.3651 2.9460\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.572344406610701 [best r2] 0.862013650057009\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5090995268216786 [r2] 0.8908241384876118\n",
      "......Outputs 5.2207 5.6949 3.4290 5.6949 5.6967 2.7845 7.0152 5.1663 2.3035 2.9763\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5090995268216786 [best r2] 0.8908241384876118\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.4556506662911328 [r2] 0.9125448759346795\n",
      "......Outputs 5.1074 5.7025 3.2391 5.7025 5.7293 2.7069 6.8095 5.1373 2.2200 2.9966\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4556506662911328 [best r2] 0.9125448759346795\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4088075954113815 [r2] 0.929602189311816\n",
      "......Outputs 5.0845 5.7341 3.1949 5.7341 5.8066 2.7376 6.6909 5.1962 2.1785 3.0839\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4088075954113815 [best r2] 0.929602189311816\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.36819821452121987 [r2] 0.9428936244282393\n",
      "......Outputs 4.9939 5.6662 3.0980 5.6662 5.7531 2.7145 6.5374 5.1946 2.1155 3.1300\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.36819821452121987 [best r2] 0.9428936244282393\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.33306920938841134 [r2] 0.9532705990943033\n",
      "......Outputs 4.9701 5.6830 3.0336 5.6830 5.7371 2.7146 6.5052 5.2362 2.0758 3.1673\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.33306920938841134 [best r2] 0.9532705990943033\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3021976405758525 [r2] 0.9615316658923431\n",
      "......Outputs 4.9441 5.6964 2.9926 5.6964 5.6336 2.7138 6.4779 5.2376 2.0433 3.1628\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3021976405758525 [best r2] 0.9615316658923431\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.2757752685032223 [r2] 0.9679644729749859\n",
      "......Outputs 4.9493 5.7203 2.9643 5.7203 5.5771 2.7301 6.4940 5.2369 2.0307 3.1388\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2757752685032223 [best r2] 0.9679644729749859\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.25322592099990415 [r2] 0.9729891930098756\n",
      "......Outputs 4.9675 5.7296 2.9348 5.7296 5.5479 2.7505 6.5288 5.2358 2.0252 3.1312\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.25322592099990415 [best r2] 0.9729891930098756\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.23387496795762072 [r2] 0.9769596685241652\n",
      "......Outputs 4.9749 5.7214 2.9142 5.7214 5.4995 2.7701 6.5576 5.2211 2.0253 3.1463\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.23387496795762072 [best r2] 0.9769596685241652\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.2174009727902396 [r2] 0.9800912401928847\n",
      "......Outputs 4.9952 5.7264 2.9082 5.7264 5.4987 2.7967 6.6111 5.2151 2.0347 3.1605\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2174009727902396 [best r2] 0.9800912401928847\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.20311059875059823 [r2] 0.9826225353501077\n",
      "......Outputs 5.0017 5.7205 2.9038 5.7205 5.4743 2.8169 6.6431 5.1941 2.0449 3.1562\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20311059875059823 [best r2] 0.9826225353501077\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.19056314864755092 [r2] 0.984703253102384\n",
      "......Outputs 5.0027 5.7208 2.9061 5.7208 5.4539 2.8253 6.6651 5.1807 2.0516 3.1508\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19056314864755092 [best r2] 0.984703253102384\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.17956712950137288 [r2] 0.9864176498803597\n",
      "......Outputs 5.0103 5.7281 2.9117 5.7281 5.4446 2.8438 6.7000 5.1727 2.0618 3.1499\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17956712950137288 [best r2] 0.9864176498803597\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.16958364562709058 [r2] 0.9878859551998357\n",
      "......Outputs 5.0078 5.7254 2.9062 5.7254 5.4374 2.8536 6.7140 5.1631 2.0722 3.1501\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16958364562709058 [best r2] 0.9878859551998357\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.16080071239493923 [r2] 0.9891082622019997\n",
      "......Outputs 5.0071 5.7340 2.9104 5.7340 5.4325 2.8572 6.7158 5.1653 2.0846 3.1491\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16080071239493923 [best r2] 0.9891082622019997\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.15264530095232576 [r2] 0.9901850493604083\n",
      "......Outputs 5.0071 5.7317 2.9146 5.7317 5.4289 2.8569 6.7128 5.1596 2.0950 3.1412\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15264530095232576 [best r2] 0.9901850493604083\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.14525725799373312 [r2] 0.9911121457433221\n",
      "......Outputs 5.0134 5.7320 2.9176 5.7320 5.4236 2.8569 6.7149 5.1498 2.1009 3.1373\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14525725799373312 [best r2] 0.9911121457433221\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.13842274756432812 [r2] 0.991928836007222\n",
      "......Outputs 5.0167 5.7294 2.9201 5.7294 5.4217 2.8552 6.7113 5.1375 2.1048 3.1374\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13842274756432812 [best r2] 0.991928836007222\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.13196673564616634 [r2] 0.992664154282806\n",
      "......Outputs 5.0193 5.7312 2.9243 5.7312 5.4170 2.8543 6.7052 5.1339 2.1120 3.1396\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13196673564616634 [best r2] 0.992664154282806\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.12602023004742235 [r2] 0.9933103749414356\n",
      "......Outputs 5.0259 5.7391 2.9269 5.7391 5.4151 2.8519 6.7091 5.1381 2.1166 3.1382\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12602023004742235 [best r2] 0.9933103749414356\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.12040214571834298 [r2] 0.9938935375030447\n",
      "......Outputs 5.0356 5.7363 2.9310 5.7363 5.4156 2.8533 6.7111 5.1390 2.1183 3.1349\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12040214571834298 [best r2] 0.9938935375030447\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.11506754485528915 [r2] 0.9944226623853382\n",
      "......Outputs 5.0362 5.7340 2.9310 5.7340 5.4145 2.8523 6.7109 5.1384 2.1194 3.1332\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11506754485528915 [best r2] 0.9944226623853382\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.11000373280284426 [r2] 0.9949027482237021\n",
      "......Outputs 5.0336 5.7344 2.9329 5.7344 5.4086 2.8556 6.7071 5.1360 2.1208 3.1318\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11000373280284426 [best r2] 0.9949027482237021\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.10524570392496643 [r2] 0.995334158353187\n",
      "......Outputs 5.0347 5.7403 2.9382 5.7403 5.4079 2.8612 6.7090 5.1408 2.1229 3.1296\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10524570392496643 [best r2] 0.995334158353187\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.10078768594133634 [r2] 0.995721060077275\n",
      "......Outputs 5.0409 5.7413 2.9456 5.7413 5.4078 2.8627 6.7117 5.1477 2.1260 3.1304\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10078768594133634 [best r2] 0.995721060077275\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.09651719507318489 [r2] 0.9960759853032811\n",
      "......Outputs 5.0361 5.7311 2.9431 5.7311 5.4009 2.8563 6.7037 5.1426 2.1236 3.1276\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09651719507318489 [best r2] 0.9960759853032811\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.09235716284812731 [r2] 0.9964069570612074\n",
      "......Outputs 5.0240 5.7299 2.9354 5.7299 5.3916 2.8559 6.6977 5.1385 2.1204 3.1232\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09235716284812731 [best r2] 0.9964069570612074\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.08854671173495124 [r2] 0.9966973228885487\n",
      "......Outputs 5.0379 5.7470 2.9454 5.7470 5.3979 2.8693 6.7145 5.1530 2.1248 3.1259\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08854671173495124 [best r2] 0.9966973228885487\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.0860183487294447 [r2] 0.996883239412669\n",
      "......Outputs 5.0577 5.7497 2.9661 5.7497 5.4134 2.8731 6.7242 5.1656 2.1305 3.1271\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0860183487294447 [best r2] 0.996883239412669\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.08298840760876762 [r2] 0.9970989440573389\n",
      "......Outputs 5.0363 5.7207 2.9594 5.7207 5.3939 2.8542 6.6957 5.1458 2.1217 3.1196\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08298840760876762 [best r2] 0.9970989440573389\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.0829257070432682 [r2] 0.9971033260942616\n",
      "......Outputs 4.9994 5.7006 2.9224 5.7006 5.3551 2.8388 6.6653 5.1153 2.1073 3.1102\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0829257070432682 [best r2] 0.9971033260942616\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.08257019406507941 [r2] 0.9971281096686514\n",
      "......Outputs 5.0381 5.7587 2.9320 5.7587 5.3715 2.8718 6.7183 5.1483 2.1203 3.1198\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.0963062107188083 [r2] 0.9960931221629955\n",
      "......Outputs 5.1384 5.8192 3.0179 5.8192 5.4679 2.9212 6.8019 5.2243 2.1591 3.1423\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.10816989875771561 [r2] 0.9950712806730743\n",
      "......Outputs 5.0785 5.7125 3.0217 5.7125 5.4490 2.8618 6.6965 5.1809 2.1443 3.1243\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.14237771018092538 [r2] 0.9914610346956788\n",
      "......Outputs 4.8512 5.5427 2.8383 5.5427 5.2722 2.7364 6.5067 5.0200 2.0583 3.0704\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.1722950786867867 [r2] 0.9874954811267816\n",
      "......Outputs 4.9097 5.7204 2.7953 5.7204 5.2587 2.8226 6.6224 5.0741 2.0682 3.0921\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.23085555783796233 [r2] 0.9775507461866524\n",
      "......Outputs 5.3866 6.1372 3.1285 6.1372 5.6472 3.1035 7.0952 5.3949 2.2438 3.2322\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.21787987929106792 [r2] 0.9800034306864082\n",
      "......Outputs 5.1955 5.6600 3.1573 5.6600 5.5526 2.8838 6.7430 5.2330 2.2014 3.1142\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.2545342109477944 [r2] 0.9727093697294632\n",
      "......Outputs 4.5819 5.2509 2.7151 5.2509 5.1441 2.5378 6.2564 4.8550 1.9784 2.9331\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.1912743998133374 [r2] 0.9845888539345922\n",
      "......Outputs 4.9688 5.8405 2.8501 5.8405 5.4495 2.8486 6.8020 5.2092 2.0991 3.2149\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.17743474988055577 [r2] 0.9867383183554773\n",
      "......Outputs 5.3044 6.0101 3.0590 6.0101 5.6004 2.9907 7.0143 5.3757 2.2207 3.2479\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.13564121630931594 [r2] 0.992249948439351\n",
      "......Outputs 4.9899 5.4597 2.9482 5.4597 5.2569 2.7471 6.4653 5.0378 2.0950 2.9850\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.10282986068323091 [r2] 0.9955459023337839\n",
      "......Outputs 4.9985 5.6944 2.9353 5.6944 5.4221 2.8064 6.6421 5.0905 2.0967 3.0608\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.0957144641162013 [r2] 0.9961409857239032\n",
      "......Outputs 5.1065 5.9430 2.9860 5.9430 5.5219 2.9368 6.8594 5.2052 2.1613 3.2046\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.06949626454940301 [r2] 0.9979655633811495\n",
      "......Outputs 5.0010 5.6898 2.9424 5.6898 5.3391 2.8520 6.6221 5.1264 2.1196 3.1284\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06949626454940301 [best r2] 0.9979655633811495\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.06512759867444035 [r2] 0.9982133010919245\n",
      "......Outputs 5.0222 5.6753 2.9410 5.6753 5.3910 2.8267 6.6419 5.1721 2.1037 3.0832\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06512759867444035 [best r2] 0.9982133010919245\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.05990851443781492 [r2] 0.9984881860730949\n",
      "......Outputs 5.0951 5.7714 2.9901 5.7714 5.4330 2.8790 6.7504 5.2043 2.1318 3.1238\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05990851443781492 [best r2] 0.9984881860730949\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.053821678993531276 [r2] 0.9987797867847582\n",
      "......Outputs 5.0412 5.7233 2.9638 5.7233 5.3778 2.8650 6.6970 5.1287 2.1205 3.1164\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.053821678993531276 [best r2] 0.9987797867847582\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.052938104319112084 [r2] 0.9988195216902015\n",
      "......Outputs 5.0199 5.7262 2.9439 5.7262 5.4002 2.8549 6.6947 5.1370 2.1112 3.1081\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052938104319112084 [best r2] 0.9988195216902015\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.051444203437685625 [r2] 0.9988852072519959\n",
      "......Outputs 5.0562 5.7582 2.9722 5.7582 5.4115 2.8745 6.7237 5.1867 2.1251 3.1242\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.051444203437685625 [best r2] 0.9988852072519959\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.049440450097422164 [r2] 0.998970358419525\n",
      "......Outputs 5.0498 5.7334 2.9671 5.7334 5.3860 2.8659 6.7026 5.1670 2.1226 3.1106\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.049440450097422164 [best r2] 0.998970358419525\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.04869850582318469 [r2] 0.9990010298450864\n",
      "......Outputs 5.0390 5.7228 2.9568 5.7228 5.3826 2.8596 6.6932 5.1441 2.1163 3.1043\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04869850582318469 [best r2] 0.9990010298450864\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.04806567084087775 [r2] 0.9990268242982261\n",
      "......Outputs 5.0517 5.7441 2.9685 5.7441 5.3929 2.8706 6.7111 5.1579 2.1217 3.1168\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04806567084087775 [best r2] 0.9990268242982261\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.047505874104712444 [r2] 0.9990493604733853\n",
      "......Outputs 5.0525 5.7445 2.9704 5.7445 5.3948 2.8691 6.7115 5.1652 2.1235 3.1137\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.047505874104712444 [best r2] 0.9990493604733853\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.046690343390480345 [r2] 0.9990817194685112\n",
      "......Outputs 5.0431 5.7266 2.9655 5.7266 5.3827 2.8621 6.6941 5.1562 2.1198 3.1070\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046690343390480345 [best r2] 0.9990817194685112\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.046179090237177554 [r2] 0.9991017194704382\n",
      "......Outputs 5.0430 5.7319 2.9640 5.7319 5.3808 2.8642 6.6979 5.1549 2.1189 3.1091\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046179090237177554 [best r2] 0.9991017194704382\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.04610402099043909 [r2] 0.999104637606428\n",
      "......Outputs 5.0521 5.7470 2.9686 5.7470 5.3891 2.8700 6.7134 5.1624 2.1222 3.1126\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04610402099043909 [best r2] 0.999104637606428\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.04567546601042934 [r2] 0.9991212057333528\n",
      "......Outputs 5.0521 5.7385 2.9716 5.7385 5.3862 2.8679 6.7060 5.1608 2.1224 3.1103\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04567546601042934 [best r2] 0.9991212057333528\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.045121255999286264 [r2] 0.9991424023124997\n",
      "......Outputs 5.0417 5.7262 2.9648 5.7262 5.3774 2.8616 6.6928 5.1527 2.1180 3.1055\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.045121255999286264 [best r2] 0.9991424023124997\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.044840007331159164 [r2] 0.9991530601050101\n",
      "......Outputs 5.0443 5.7361 2.9633 5.7361 5.3792 2.8657 6.7019 5.1561 2.1184 3.1088\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.044840007331159164 [best r2] 0.9991530601050101\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.045056049130072405 [r2] 0.9991448792317156\n",
      "......Outputs 5.0560 5.7467 2.9708 5.7467 5.3873 2.8723 6.7141 5.1646 2.1227 3.1127\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.044840007331159164 [best r2] 0.9991530601050101\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.04469745421144267 [r2] 0.9991584366433194\n",
      "......Outputs 5.0520 5.7356 2.9716 5.7356 5.3831 2.8674 6.7036 5.1603 2.1215 3.1078\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04469745421144267 [best r2] 0.9991584366433194\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.04425254485183048 [r2] 0.9991751067635021\n",
      "......Outputs 5.0386 5.7247 2.9633 5.7247 5.3722 2.8606 6.6902 5.1508 2.1164 3.1038\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04425254485183048 [best r2] 0.9991751067635021\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.04417173271631713 [r2] 0.999178116783248\n",
      "......Outputs 5.0443 5.7374 2.9627 5.7374 5.3764 2.8665 6.7026 5.1563 2.1175 3.1090\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.044871420619755735 [r2] 0.9991518730183775\n",
      "......Outputs 5.0611 5.7505 2.9738 5.7505 5.3888 2.8746 6.7187 5.1686 2.1236 3.1132\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.044531462096533025 [r2] 0.9991646756337237\n",
      "......Outputs 5.0541 5.7352 2.9748 5.7352 5.3829 2.8674 6.7037 5.1615 2.1221 3.1069\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.04430360718352622 [r2] 0.999173202001931\n",
      "......Outputs 5.0319 5.7177 2.9603 5.7177 5.3654 2.8570 6.6827 5.1451 2.1139 3.1013\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.04441237612501712 [r2] 0.9991691373064606\n",
      "......Outputs 5.0405 5.7359 2.9587 5.7359 5.3704 2.8659 6.7007 5.1539 2.1153 3.1086\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.04643005256835537 [r2] 0.9990919294489489\n",
      "......Outputs 5.0719 5.7615 2.9790 5.7615 5.3947 2.8808 6.7310 5.1775 2.1266 3.1162\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.046612846412403416 [r2] 0.9990847652763513\n",
      "......Outputs 5.0631 5.7389 2.9838 5.7389 5.3899 2.8704 6.7090 5.1672 2.1257 3.1077\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.047397074367575676 [r2] 0.9990537098677996\n",
      "......Outputs 5.0165 5.6998 2.9542 5.6998 5.3533 2.8478 6.6628 5.1321 2.1086 3.0960\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.048913609956206465 [r2] 0.9989921853362072\n",
      "......Outputs 5.0226 5.7254 2.9431 5.7254 5.3528 2.8592 6.6868 5.1421 2.1070 3.1053\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.054817715013237796 [r2] 0.9987342058088435\n",
      "......Outputs 5.0943 5.7895 2.9857 5.7895 5.4090 2.8948 6.7615 5.1971 2.1324 3.1240\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.06067120221606351 [r2] 0.9984494476188884\n",
      "......Outputs 5.0991 5.7636 3.0126 5.7636 5.4219 2.8850 6.7401 5.1926 2.1404 3.1155\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.06478520766652592 [r2] 0.9982320378977327\n",
      "......Outputs 4.9862 5.6581 2.9489 5.6581 5.3356 2.8268 6.6190 5.1053 2.1011 3.0847\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.07985422276952288 [r2] 0.9973139319265039\n",
      "......Outputs 4.9535 5.6695 2.8914 5.6695 5.2902 2.8279 6.6208 5.0914 2.0786 3.0890\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.09081332147545251 [r2] 0.9965260756143889\n",
      "......Outputs 5.1255 5.8458 2.9765 5.8458 5.4194 2.9205 6.8188 5.2279 2.1350 3.1375\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.1292103478094393 [r2] 0.9929674013668779\n",
      "......Outputs 5.2314 5.8751 3.0992 5.8751 5.5379 2.9489 6.8725 5.2896 2.1900 3.1478\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.1349314502649547 [r2] 0.9923308431975566\n",
      "......Outputs 4.9629 5.5923 2.9815 5.5923 5.3466 2.7999 6.5519 5.0747 2.1083 3.0689\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.20405070544493753 [r2] 0.9824612982855452\n",
      "......Outputs 4.7268 5.4560 2.7527 5.4560 5.1122 2.7132 6.3826 4.9164 1.9948 3.0290\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.21266729205762883 [r2] 0.9809487865389848\n",
      "......Outputs 5.0693 5.8710 2.8606 5.8710 5.3416 2.9253 6.8405 5.2075 2.0859 3.1432\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.32393102553453423 [r2] 0.9557995864456212\n",
      "......Outputs 5.5774 6.2325 3.2449 6.2325 5.8293 3.1486 7.2919 5.5678 2.2972 3.2447\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.3031712137317654 [r2] 0.9612834040895882\n",
      "......Outputs 5.0615 5.5482 3.1254 5.5482 5.4572 2.8079 6.5258 5.1176 2.1805 3.0375\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.40578413383063383 [r2] 0.9306396357680078\n",
      "......Outputs 4.3534 5.0748 2.6119 5.0748 4.8834 2.4842 5.9311 4.5965 1.8953 2.8856\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.3395203065897768 [r2] 0.9514428990601236\n",
      "......Outputs 4.8998 5.8460 2.7734 5.8460 5.3254 2.9045 6.7520 5.1271 2.0390 3.2248\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.37847258536994993 [r2] 0.939662113373988\n",
      "......Outputs 5.6706 6.4043 3.2157 6.4043 5.9455 3.2984 7.4891 5.7112 2.3320 3.3510\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. The optimal capacity is 1\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run all tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_all_tests=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.26398674584925175\n",
      "..last epoch zero_data_loss 14.002456188201904\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 4.918188840078714\n",
      "....Outputs 0.1491 0.1444 0.1491 0.1363 0.1560\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 1\n",
      "....Loss: 4.861387173518942\n",
      "....Outputs 0.2143 0.1915 0.2085 0.2085 0.2008\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "..Epoch 2\n",
      "....Loss: 4.803974477072832\n",
      "....Outputs 0.2729 0.2704 0.2704 0.2444 0.2559\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "..Epoch 3\n",
      "....Loss: 4.738009527580887\n",
      "....Outputs 0.3409 0.3409 0.3183 0.3039 0.3430\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 4\n",
      "....Loss: 4.658560570012228\n",
      "....Outputs 0.3929 0.4257 0.4257 0.4283 0.3762\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "..Epoch 5\n",
      "....Loss: 4.560131973899963\n",
      "....Outputs 0.5328 0.4816 0.4648 0.5334 0.5328\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "..Epoch 6\n",
      "....Loss: 4.436741078987767\n",
      "....Outputs 0.6692 0.5900 0.6692 0.6641 0.5756\n",
      "....Labels  5.7012 5.0497 5.6991 5.3739 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 4.281912202507818\n",
      "....Outputs 0.8295 0.8404 0.7252 0.8404 0.7140\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 8\n",
      "....Loss: 4.089046892508784\n",
      "....Outputs 0.8930 1.0542 1.0366 1.0542 0.8858\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 9\n",
      "....Loss: 3.85084932545325\n",
      "....Outputs 1.2936 1.3194 1.3194 1.0981 1.0985\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "..Epoch 10\n",
      "....Loss: 3.5587217138336666\n",
      "....Outputs 1.6469 1.6469 1.3619 1.3482 1.6082\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 11\n",
      "....Loss: 3.204854507595619\n",
      "....Outputs 1.6500 1.6857 2.0460 1.9912 2.0460\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "..Epoch 12\n",
      "....Loss: 2.778764192490878\n",
      "....Outputs 2.0799 2.5290 2.0134 2.5290 2.4611\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 13\n",
      "....Loss: 2.2708427592323965\n",
      "....Outputs 2.5529 3.1126 2.4517 3.1126 3.0350\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 14\n",
      "....Loss: 1.6848162750689775\n",
      "....Outputs 3.8090 2.9733 3.7282 3.8090 3.1124\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 15\n",
      "....Loss: 1.0700101216452482\n",
      "....Outputs 3.7403 4.5444 4.6263 4.6263 3.5883\n",
      "....Labels  2.9694 5.3739 5.6991 5.7012 5.0497\n",
      "..Epoch 16\n",
      "....Loss: 0.7252332361539681\n",
      "....Outputs 5.4669 5.5483 4.3791 4.2831 5.5483\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "..Epoch 17\n",
      "....Loss: 1.1082600645834113\n",
      "....Outputs 6.4978 4.9999 6.4978 6.4003 4.9220\n",
      "....Labels  5.7012 5.0497 5.6991 5.3739 2.9694\n",
      "..Epoch 18\n",
      "....Loss: 1.6430862856338264\n",
      "....Outputs 5.2404 7.1314 5.6031 7.2727 7.2727\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 19\n",
      "....Loss: 1.9283083415850382\n",
      "....Outputs 7.4873 5.9667 5.2823 7.6920 7.6920\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "..Epoch 20\n",
      "....Loss: 1.9180875224768097\n",
      "....Outputs 6.0753 5.0941 7.7493 7.4790 7.7493\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "..Epoch 21\n",
      "....Loss: 1.6853020093447393\n",
      "....Outputs 7.5343 5.9843 7.2148 4.7611 7.5343\n",
      "....Labels  5.6991 5.0497 5.3739 2.9694 5.7012\n",
      "..Epoch 22\n",
      "....Loss: 1.3144013846701808\n",
      "....Outputs 7.1490 5.7623 7.1490 4.3545 6.7929\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 23\n",
      "....Loss: 0.882371201396374\n",
      "....Outputs 6.3023 6.6834 6.6834 5.4702 3.9289\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "..Epoch 24\n",
      "....Loss: 0.45076331924398305\n",
      "....Outputs 5.1595 6.2057 3.5223 5.8063 6.2057\n",
      "....Labels  5.0497 5.7012 2.9694 5.3739 5.6991\n",
      "..Epoch 25\n",
      "....Loss: 0.1263004766604557\n",
      "....Outputs 5.7614 5.7614 4.8671 5.3523 3.1655\n",
      "....Labels  5.7012 5.6991 5.0497 5.3739 2.9694\n",
      "Verified that a small batch can be overfit since the RMSE was less than 0.22499709319526628\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 0.0217 0.0373 0.0526 0.0373 0.0264\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "....Loss: 0.021699048578739166\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.561857115211121 [r2] -7.766063202708089\n",
      "......Outputs 0.0371 0.0509 0.0437 0.0509 0.0372 0.0416 0.0422 0.0467 0.0435 0.0421\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.561857115211121 [best r2] -7.766063202708089\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.5091220514393817 [r2] -4.187023175176409\n",
      "......Outputs 0.5913 0.8171 0.7904 0.8171 0.8402 1.1025 0.6341 0.9172 0.7245 0.9304\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.5091220514393817 [best r2] -4.187023175176409\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6228279368308294 [r2] -1.8977495203517796\n",
      "......Outputs 2.8505 4.4441 3.7732 4.4441 4.6393 5.8720 3.2884 4.7014 3.4576 5.0184\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6228279368308294 [best r2] -1.8977495203517796\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.869131710710157 [r2] -0.4716381983388156\n",
      "......Outputs 3.4729 6.3715 4.0603 6.3715 6.7645 6.3765 4.9247 5.7948 3.9064 4.8838\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.869131710710157 [best r2] -0.4716381983388156\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.7952207575206838 [r2] -0.3575535297450876\n",
      "......Outputs 2.3547 4.7974 2.3425 4.7974 5.2234 3.6566 4.0363 3.8849 2.4307 2.3886\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.7952207575206838 [best r2] -0.3575535297450876\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.50145513926599 [r2] 0.05038850633122216\n",
      "......Outputs 2.8523 6.3033 2.4510 6.3033 6.9558 3.8499 5.6150 4.7066 2.7253 2.3021\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.50145513926599 [best r2] 0.05038850633122216\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.3664915417315997 [r2] 0.21343406002004495\n",
      "......Outputs 3.4496 7.4016 2.6763 7.4016 8.1072 4.0736 7.0859 5.5611 3.0564 2.5060\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.3664915417315997 [best r2] 0.21343406002004495\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.224720594517234 [r2] 0.3681772163364051\n",
      "......Outputs 3.2315 5.9757 2.3558 5.9757 6.4377 3.2648 6.3361 4.8431 2.5906 2.2658\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.224720594517234 [best r2] 0.3681772163364051\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0643989510396332 [r2] 0.5227673848834631\n",
      "......Outputs 3.8384 6.0199 2.7429 6.0199 6.3926 3.4497 7.0109 5.3491 2.8204 2.8142\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0643989510396332 [best r2] 0.5227673848834631\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9450296807063351 [r2] 0.623805762977059\n",
      "......Outputs 4.4517 6.0558 3.1286 6.0558 6.3303 3.5823 7.5664 5.7434 2.9735 3.3346\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9450296807063351 [best r2] 0.623805762977059\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8345225513413481 [r2] 0.706642365415606\n",
      "......Outputs 4.4751 5.5011 3.0474 5.5011 5.6683 3.1525 7.1828 5.3037 2.6786 3.0880\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8345225513413481 [best r2] 0.706642365415606\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7443841269537174 [r2] 0.7665921552478118\n",
      "......Outputs 5.0007 5.8174 3.3543 5.8174 5.9021 3.1897 7.6683 5.4960 2.7082 3.1843\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7443841269537174 [best r2] 0.7665921552478118\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6611514178481438 [r2] 0.8158706126586492\n",
      "......Outputs 5.0386 5.6223 3.3311 5.6223 5.6306 2.9554 7.3945 5.2365 2.5064 2.9824\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6611514178481438 [best r2] 0.8158706126586492\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.5956163880893252 [r2] 0.850564246363309\n",
      "......Outputs 5.1925 5.6658 3.4528 5.6658 5.5932 2.9020 7.3005 5.2006 2.4381 3.0040\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5956163880893252 [best r2] 0.850564246363309\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5352005772262329 [r2] 0.879342482049186\n",
      "......Outputs 5.2254 5.6890 3.4299 5.6890 5.5862 2.8300 7.1371 5.1532 2.3541 3.0158\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5352005772262329 [best r2] 0.879342482049186\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.48341403137470085 [r2] 0.9015626883057953\n",
      "......Outputs 5.1888 5.6992 3.3069 5.6992 5.6420 2.7585 6.9648 5.1270 2.2736 3.0406\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.48341403137470085 [best r2] 0.9015626883057953\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4367447635457685 [r2] 0.9196517070949145\n",
      "......Outputs 5.1528 5.6944 3.2207 5.6944 5.7061 2.7472 6.8159 5.1636 2.2101 3.1436\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4367447635457685 [best r2] 0.9196517070949145\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.39588318906579445 [r2] 0.9339830644212912\n",
      "......Outputs 5.0897 5.6556 3.1455 5.6556 5.7174 2.7443 6.6684 5.1747 2.1549 3.1994\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.39588318906579445 [best r2] 0.9339830644212912\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.3595119515685054 [r2] 0.9455562656742191\n",
      "......Outputs 5.0447 5.6515 3.0764 5.6515 5.7170 2.7522 6.5890 5.1797 2.1186 3.1941\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3595119515685054 [best r2] 0.9455562656742191\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3275017201812638 [r2] 0.9548197726750163\n",
      "......Outputs 5.0263 5.6812 3.0323 5.6812 5.6849 2.7546 6.5660 5.1807 2.0900 3.1718\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3275017201812638 [best r2] 0.9548197726750163\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.2992901616481978 [r2] 0.9622683217409485\n",
      "......Outputs 5.0024 5.6766 2.9966 5.6766 5.6178 2.7585 6.5360 5.1538 2.0674 3.1561\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2992901616481978 [best r2] 0.9622683217409485\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2750019940426108 [r2] 0.9681438764747\n",
      "......Outputs 5.0157 5.6996 2.9821 5.6996 5.6046 2.7931 6.5533 5.1517 2.0568 3.1705\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2750019940426108 [best r2] 0.9681438764747\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.25338454229411245 [r2] 0.9729553431492267\n",
      "......Outputs 5.0151 5.7016 2.9564 5.7016 5.5727 2.8079 6.5547 5.1399 2.0473 3.1732\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.25338454229411245 [best r2] 0.9729553431492267\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.23489793233426867 [r2] 0.9767576718363167\n",
      "......Outputs 5.0230 5.7099 2.9433 5.7099 5.5502 2.8176 6.5819 5.1627 2.0504 3.1810\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.23489793233426867 [best r2] 0.9767576718363167\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.21856147046333194 [r2] 0.9798781249110743\n",
      "......Outputs 5.0349 5.7146 2.9306 5.7146 5.5417 2.8408 6.6235 5.1701 2.0587 3.1860\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.21856147046333194 [best r2] 0.9798781249110743\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.2039957413090428 [r2] 0.982470745640784\n",
      "......Outputs 5.0353 5.7108 2.9199 5.7108 5.5088 2.8528 6.6389 5.1556 2.0611 3.1857\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2039957413090428 [best r2] 0.982470745640784\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.19109724696558145 [r2] 0.9846173874354907\n",
      "......Outputs 5.0269 5.7121 2.9074 5.7121 5.4866 2.8564 6.6570 5.1463 2.0587 3.1765\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19109724696558145 [best r2] 0.9846173874354907\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.17974442617246325 [r2] 0.9863908154112997\n",
      "......Outputs 5.0304 5.7300 2.9008 5.7300 5.4841 2.8678 6.6889 5.1543 2.0625 3.1719\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17974442617246325 [best r2] 0.9863908154112997\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1698831729243027 [r2] 0.9878431245189231\n",
      "......Outputs 5.0449 5.7497 2.9034 5.7497 5.4888 2.8811 6.7189 5.1740 2.0768 3.1727\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1698831729243027 [best r2] 0.9878431245189231\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16067582107526027 [r2] 0.9891251745058298\n",
      "......Outputs 5.0446 5.7415 2.9000 5.7415 5.4811 2.8770 6.7221 5.1614 2.0803 3.1657\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16067582107526027 [best r2] 0.9891251745058298\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.1518679209352565 [r2] 0.9902847644233903\n",
      "......Outputs 5.0192 5.7119 2.8893 5.7119 5.4469 2.8555 6.6976 5.1272 2.0747 3.1528\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1518679209352565 [best r2] 0.9902847644233903\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.14463189948592348 [r2] 0.9911885086191529\n",
      "......Outputs 4.9945 5.7067 2.8787 5.7067 5.4200 2.8426 6.6882 5.1155 2.0704 3.1431\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14463189948592348 [best r2] 0.9911885086191529\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.13836801755062275 [r2] 0.991935217148999\n",
      "......Outputs 5.0141 5.7442 2.8966 5.7442 5.4282 2.8658 6.7175 5.1484 2.0839 3.1482\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13836801755062275 [best r2] 0.991935217148999\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.13682886913759199 [r2] 0.9921136378560844\n",
      "......Outputs 5.0802 5.7984 2.9458 5.7984 5.4802 2.9049 6.7745 5.2069 2.1177 3.1639\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.13849600532441914 [r2] 0.9919202907086089\n",
      "......Outputs 5.0968 5.7622 2.9705 5.7622 5.4973 2.8948 6.7600 5.1933 2.1307 3.1630\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.13904730437303056 [r2] 0.9918558382858877\n",
      "......Outputs 4.9674 5.6231 2.8836 5.6231 5.3864 2.8043 6.6197 5.0629 2.0743 3.1168\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.16201529493063987 [r2] 0.9889431027874288\n",
      "......Outputs 4.8715 5.6111 2.7840 5.6111 5.2859 2.7771 6.5671 5.0161 2.0207 3.1070\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.1625602175559952 [r2] 0.9888686001181887\n",
      "......Outputs 5.0705 5.9093 2.9005 5.9093 5.4400 2.9401 6.8339 5.2549 2.1071 3.1919\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.19871133441693895 [r2] 0.9833671557623164\n",
      "......Outputs 5.2854 5.9634 3.0846 5.9634 5.6369 3.0250 6.9714 5.3743 2.2248 3.2182\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.17829913739746403 [r2] 0.9866087929485488\n",
      "......Outputs 4.9515 5.4865 2.9101 5.4865 5.3766 2.7666 6.5237 5.0157 2.1144 3.0360\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.18753201893804833 [r2] 0.9851860082227853\n",
      "......Outputs 4.7823 5.5145 2.7622 5.5145 5.2451 2.7131 6.4817 4.9555 2.0150 3.0714\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.16393809697338407 [r2] 0.9886790982929755\n",
      "......Outputs 5.1346 6.0211 2.9475 6.0211 5.5388 2.9850 7.0107 5.3339 2.1220 3.2888\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.14264809697077785 [r2] 0.991428571528016\n",
      "......Outputs 5.1816 5.7691 3.0098 5.7691 5.4632 2.9418 6.8398 5.2470 2.1541 3.1537\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13682886913759199 [best r2] 0.9921136378560844\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.1304813483059091 [r2] 0.9928283660842788\n",
      "......Outputs 4.9098 5.5097 2.8728 5.5097 5.2871 2.7379 6.4703 5.0100 2.0890 2.9891\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1304813483059091 [best r2] 0.9928283660842788\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.10647812101576672 [r2] 0.9952242454486859\n",
      "......Outputs 4.9976 5.8260 2.9148 5.8260 5.4360 2.8404 6.7167 5.1807 2.1202 3.1429\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10647812101576672 [best r2] 0.9952242454486859\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.10218857760060637 [r2] 0.995601283735675\n",
      "......Outputs 5.1322 5.8380 2.9692 5.8380 5.4843 2.9327 6.8511 5.2505 2.1403 3.2212\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10218857760060637 [best r2] 0.995601283735675\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.08628539915748878 [r2] 0.996863856937179\n",
      "......Outputs 5.0221 5.6075 2.9189 5.6075 5.3629 2.8440 6.6333 5.1101 2.1036 3.0769\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08628539915748878 [best r2] 0.996863856937179\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.08079779258395542 [r2] 0.9972500789042362\n",
      "......Outputs 4.9889 5.7155 2.9315 5.7155 5.3618 2.8360 6.6303 5.1314 2.1084 3.0788\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08079779258395542 [best r2] 0.9972500789042362\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.0768561275886016 [r2] 0.9975118403771038\n",
      "......Outputs 5.0714 5.8412 2.9689 5.8412 5.4297 2.8923 6.7688 5.2022 2.1327 3.1612\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0768561275886016 [best r2] 0.9975118403771038\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.07123381771003277 [r2] 0.9978625612327701\n",
      "......Outputs 5.0711 5.7181 2.9488 5.7181 5.4171 2.8798 6.7310 5.1538 2.1239 3.1395\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07123381771003277 [best r2] 0.9978625612327701\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.0684302956564433 [r2] 0.9980274951775707\n",
      "......Outputs 5.0133 5.6645 2.9323 5.6645 5.3753 2.8479 6.6515 5.1217 2.1102 3.0922\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0684302956564433 [best r2] 0.9980274951775707\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.0648501479704894 [r2] 0.9982284917320605\n",
      "......Outputs 5.0268 5.7641 2.9510 5.7641 5.3900 2.8645 6.7039 5.1662 2.1154 3.1169\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0648501479704894 [best r2] 0.9982284917320605\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.06497322065547244 [r2] 0.9982217614108755\n",
      "......Outputs 5.0735 5.7858 2.9699 5.7858 5.4205 2.8870 6.7575 5.1857 2.1250 3.1458\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0648501479704894 [best r2] 0.9982284917320605\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.061422994294706004 [r2] 0.9984107829756342\n",
      "......Outputs 5.0520 5.7060 2.9611 5.7060 5.3981 2.8692 6.6987 5.1465 2.1224 3.1200\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.061422994294706004 [best r2] 0.9984107829756342\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.060570598139228424 [r2] 0.9984545855608935\n",
      "......Outputs 5.0085 5.6985 2.9410 5.6985 5.3678 2.8490 6.6595 5.1286 2.1126 3.1019\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.060570598139228424 [best r2] 0.9984545855608935\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.058489535713074615 [r2] 0.9985589548476054\n",
      "......Outputs 5.0379 5.7651 2.9499 5.7651 5.3928 2.8709 6.7205 5.1660 2.1163 3.1290\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.058489535713074615 [best r2] 0.9985589548476054\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.060574557905370655 [r2] 0.998454383493224\n",
      "......Outputs 5.0849 5.7721 2.9763 5.7721 5.4252 2.8927 6.7562 5.1892 2.1301 3.1406\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.058489535713074615 [best r2] 0.9985589548476054\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.05772374406519355 [r2] 0.9985964424464873\n",
      "......Outputs 5.0496 5.7085 2.9671 5.7085 5.3956 2.8650 6.6880 5.1492 2.1265 3.1096\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.05929305584600362 [r2] 0.9985190891737363\n",
      "......Outputs 4.9970 5.6935 2.9342 5.6935 5.3549 2.8389 6.6473 5.1186 2.1074 3.1006\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.05783187119432003 [r2] 0.9985911792817053\n",
      "......Outputs 5.0360 5.7614 2.9453 5.7614 5.3844 2.8702 6.7218 5.1643 2.1104 3.1321\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.06515135868882828 [r2] 0.9982119971981422\n",
      "......Outputs 5.1079 5.7950 2.9921 5.7950 5.4392 2.9068 6.7803 5.2117 2.1368 3.1418\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.0629738106214418 [r2] 0.9983295203536026\n",
      "......Outputs 5.0655 5.7139 2.9836 5.7139 5.4070 2.8688 6.6907 5.1575 2.1362 3.1066\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.07145945072536196 [r2] 0.997848999119293\n",
      "......Outputs 4.9658 5.6511 2.9182 5.6511 5.3274 2.8176 6.6026 5.0851 2.0986 3.0900\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.07229158459686985 [r2] 0.9977986113163573\n",
      "......Outputs 5.0027 5.7466 2.9194 5.7466 5.3475 2.8575 6.7023 5.1455 2.0940 3.1268\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.09151309449722769 [r2] 0.9964723318563682\n",
      "......Outputs 5.1556 5.8632 3.0149 5.8632 5.4705 2.9419 6.8528 5.2636 2.1492 3.1570\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.09782614231762572 [r2] 0.9959688301552722\n",
      "......Outputs 5.1359 5.7475 3.0341 5.7475 5.4665 2.9009 6.7403 5.2001 2.1690 3.1148\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.1167918389985718 [r2] 0.9942542564974713\n",
      "......Outputs 4.9068 5.5570 2.8880 5.5570 5.2898 2.7708 6.5013 5.0115 2.0873 3.0605\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.13776806549519033 [r2] 0.99200500196642\n",
      "......Outputs 4.8785 5.6576 2.8324 5.6576 5.2417 2.7937 6.5929 5.0545 2.0398 3.1035\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.16181751427452443 [r2] 0.9889700817907416\n",
      "......Outputs 5.2031 5.9941 3.0121 5.9941 5.5013 2.9894 6.9867 5.3484 2.1441 3.1955\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.19863875355931818 [r2] 0.9833793040942536\n",
      "......Outputs 5.3262 5.8867 3.1453 5.8867 5.6150 3.0053 6.9165 5.3452 2.2415 3.1556\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.20341942650665695 [r2] 0.9825696506320055\n",
      "......Outputs 4.8445 5.4177 2.8824 5.4177 5.2712 2.7083 6.3418 4.9316 2.1057 2.9998\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.24738732682918918 [r2] 0.9742204022564634\n",
      "......Outputs 4.6665 5.4689 2.7314 5.4689 5.1050 2.6775 6.3687 4.8881 1.9884 3.0596\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.23703972727045458 [r2] 0.9763318933018441\n",
      "......Outputs 5.1799 6.1212 2.9752 6.1212 5.5503 3.0265 7.1377 5.3955 2.1341 3.2818\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.25084613256068783 [r2] 0.9734944963994994\n",
      "......Outputs 5.4122 5.9642 3.1576 5.9642 5.6717 3.0674 7.0494 5.4158 2.2567 3.1878\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.2212737995145609 [r2] 0.9793756045437034\n",
      "......Outputs 4.8290 5.3340 2.9064 5.3340 5.2018 2.6868 6.2297 4.9141 2.1049 2.9117\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.198197746968621 [r2] 0.9834530228390592\n",
      "......Outputs 4.7628 5.6282 2.8335 5.6282 5.2465 2.7331 6.4547 5.0071 2.0599 3.0662\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.18183869215604645 [r2] 0.9860718370479347\n",
      "......Outputs 5.1956 6.1059 3.0184 6.1059 5.6055 3.0463 7.1250 5.4100 2.1754 3.3288\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.13305811444836585 [r2] 0.9925423161169165\n",
      "......Outputs 5.1572 5.6444 3.0075 5.6444 5.4141 2.9160 6.7444 5.2053 2.1454 3.1392\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.1265352818597451 [r2] 0.9932555814452994\n",
      "......Outputs 4.8765 5.5015 2.8999 5.5015 5.2539 2.7102 6.4092 5.0089 2.0718 2.9699\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.0915416381750572 [r2] 0.9964701308964519\n",
      "......Outputs 4.9939 5.8967 2.9688 5.8967 5.4459 2.8456 6.7512 5.2015 2.1288 3.1372\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.08847252018062102 [r2] 0.9967028550667701\n",
      "......Outputs 5.1439 5.8433 3.0074 5.8433 5.4893 2.9518 6.8658 5.2335 2.1550 3.2194\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.066725725266812 [r2] 0.9981245398231442\n",
      "......Outputs 5.0244 5.5774 2.9351 5.5774 5.3563 2.8501 6.6191 5.0894 2.0972 3.0891\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.06192792840682286 [r2] 0.9983845469313716\n",
      "......Outputs 4.9830 5.7007 2.9408 5.7007 5.3526 2.8261 6.6254 5.1334 2.0931 3.0710\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05772374406519355 [best r2] 0.9985964424464873\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.05750220328997699 [r2] 0.9986071953363961\n",
      "......Outputs 5.0733 5.8518 2.9917 5.8518 5.4181 2.8884 6.7726 5.2198 2.1287 3.1472\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05750220328997699 [best r2] 0.9986071953363961\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.05222043686869995 [r2] 0.998851311581506\n",
      "......Outputs 5.0803 5.7328 2.9750 5.7328 5.4088 2.8804 6.7323 5.1629 2.1222 3.1394\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05222043686869995 [best r2] 0.998851311581506\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.05016128211429438 [r2] 0.9989401156053757\n",
      "......Outputs 5.0227 5.6593 2.9470 5.6593 5.3746 2.8460 6.6511 5.1200 2.1042 3.0982\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05016128211429438 [best r2] 0.9989401156053757\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.04777019502133837 [r2] 0.9990387523978514\n",
      "......Outputs 5.0299 5.7508 2.9629 5.7508 5.3853 2.8633 6.6971 5.1671 2.1146 3.1090\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04777019502133837 [best r2] 0.9990387523978514\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.048720487532941205 [r2] 0.9990001278039411\n",
      "......Outputs 5.0701 5.7866 2.9805 5.7866 5.4050 2.8854 6.7453 5.1889 2.1267 3.1289\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04777019502133837 [best r2] 0.9990387523978514\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.046531518198882266 [r2] 0.9990879562194358\n",
      "......Outputs 5.0600 5.7181 2.9709 5.7181 5.3896 2.8692 6.7018 5.1521 2.1190 3.1180\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046531518198882266 [best r2] 0.9990879562194358\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.0462016632050917 [r2] 0.9991008410723031\n",
      "......Outputs 5.0287 5.7023 2.9587 5.7023 5.3723 2.8522 6.6714 5.1366 2.1106 3.1050\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0462016632050917 [best r2] 0.9991008410723031\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.045605583578550817 [r2] 0.9991238927470227\n",
      "......Outputs 5.0410 5.7533 2.9664 5.7533 5.3865 2.8668 6.7101 5.1627 2.1173 3.1142\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.045605583578550817 [best r2] 0.9991238927470227\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.046602503474262674 [r2] 0.9990851713946911\n",
      "......Outputs 5.0682 5.7606 2.9777 5.7606 5.4010 2.8815 6.7320 5.1747 2.1253 3.1227\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.045605583578550817 [best r2] 0.9991238927470227\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.04529435074103139 [r2] 0.9991358098371131\n",
      "......Outputs 5.0544 5.7204 2.9712 5.7204 5.3858 2.8672 6.6963 5.1536 2.1202 3.1116\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04529435074103139 [best r2] 0.9991358098371131\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.04460826465797775 [r2] 0.9991617918130972\n",
      "......Outputs 5.0329 5.7214 2.9639 5.7214 5.3750 2.8583 6.6836 5.1487 2.1155 3.1073\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04460826465797775 [best r2] 0.9991617918130972\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.04424150625063734 [r2] 0.9991755182440988\n",
      "......Outputs 5.0418 5.7444 2.9661 5.7444 5.3822 2.8680 6.7069 5.1603 2.1192 3.1142\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04424150625063734 [best r2] 0.9991755182440988\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.0446602879779158 [r2] 0.9991598355926318\n",
      "......Outputs 5.0614 5.7467 2.9730 5.7467 5.3919 2.8745 6.7201 5.1660 2.1222 3.1176\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04424150625063734 [best r2] 0.9991755182440988\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.04407742396964595 [r2] 0.9991816225561361\n",
      "......Outputs 5.0559 5.7300 2.9725 5.7300 5.3840 2.8666 6.7014 5.1575 2.1201 3.1108\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04407742396964595 [best r2] 0.9991816225561361\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.04368787625380312 [r2] 0.9991960239572203\n",
      "......Outputs 5.0371 5.7257 2.9654 5.7257 5.3728 2.8583 6.6874 5.1493 2.1153 3.1056\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04368787625380312 [best r2] 0.9991960239572203\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.04345049231031094 [r2] 0.9992047372439511\n",
      "......Outputs 5.0428 5.7406 2.9670 5.7406 5.3798 2.8669 6.7045 5.1587 2.1165 3.1128\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04345049231031094 [best r2] 0.9992047372439511\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.04397003344559158 [r2] 0.9991856054984986\n",
      "......Outputs 5.0619 5.7489 2.9752 5.7489 5.3933 2.8770 6.7213 5.1705 2.1220 3.1178\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04345049231031094 [best r2] 0.9992047372439511\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.732268260713699 [r2] -8.433219766833716\n",
      "......Outputs -0.1236 -0.1160 -0.1052 -0.1160 -0.1262 -0.1265 -0.1215 -0.1121 -0.0955 -0.1097\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.732268260713699 [best r2] -8.433219766833716\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.8311587685976614 [r2] -5.182747935904069\n",
      "......Outputs 0.3140 0.5576 0.5043 0.5576 0.5212 0.7347 0.3982 0.5550 0.4671 0.6587\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.8311587685976614 [best r2] -5.182747935904069\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.7474719870341184 [r2] -2.1797120086905206\n",
      "......Outputs 2.1437 3.6480 3.0487 3.6480 3.5556 4.7194 2.6120 3.5389 2.7309 4.0933\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.7474719870341184 [best r2] -2.1797120086905206\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 2.008470143175146 [r2] -0.6992293511812888\n",
      "......Outputs 3.6640 6.9538 4.6638 6.9538 6.9807 7.3829 5.1985 6.0900 4.3556 5.9431\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.008470143175146 [best r2] -0.6992293511812888\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8712787933584072 [r2] -0.4750210992030415\n",
      "......Outputs 2.3710 5.0847 2.5771 5.0847 5.2474 4.2061 4.1087 3.8965 2.6552 2.9975\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8712787933584072 [best r2] -0.4750210992030415\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.5513090448325386 [r2] -0.013719708760157934\n",
      "......Outputs 2.5254 5.9709 2.3294 5.9709 6.3168 3.9298 5.1264 4.1367 2.6085 2.5088\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.5513090448325386 [best r2] -0.013719708760157934\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.4403347166864526 [r2] 0.12612744851193525\n",
      "......Outputs 3.2307 7.6542 2.6436 7.6542 8.1718 4.4283 7.0238 5.2410 3.0929 2.7601\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4403347166864526 [best r2] 0.12612744851193525\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2695926530973465 [r2] 0.3210308498120954\n",
      "......Outputs 3.0624 6.4671 2.3075 6.4671 6.8450 3.6302 6.5591 4.7237 2.6827 2.4649\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2695926530973465 [best r2] 0.3210308498120954\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.1029560062635742 [r2] 0.48756637348708953\n",
      "......Outputs 3.4163 6.0228 2.4126 6.0228 6.3444 3.4731 6.8351 4.8652 2.6601 2.6849\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.1029560062635742 [best r2] 0.48756637348708953\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9809332065620517 [r2] 0.594678058816664\n",
      "......Outputs 4.2565 6.2698 2.9011 6.2698 6.5573 3.7855 7.7416 5.5349 2.9315 3.2867\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9809332065620517 [best r2] 0.594678058816664\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8572727469536183 [r2] 0.6904297086021713\n",
      "......Outputs 4.3745 5.6625 2.8677 5.6625 5.8090 3.3425 7.3173 5.1554 2.6233 3.1969\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8572727469536183 [best r2] 0.6904297086021713\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7547605538333035 [r2] 0.7600395709154377\n",
      "......Outputs 4.8524 5.8016 3.0804 5.8016 5.8504 3.2515 7.5574 5.2203 2.5656 3.2597\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7547605538333035 [best r2] 0.7600395709154377\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6698823370863872 [r2] 0.8109754144512958\n",
      "......Outputs 5.0712 5.8149 3.1613 5.8149 5.7577 3.0885 7.4667 5.1427 2.4472 3.1531\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6698823370863872 [best r2] 0.8109754144512958\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.5999384289148331 [r2] 0.8483876414792555\n",
      "......Outputs 5.1431 5.7347 3.1927 5.7347 5.5934 2.9380 7.1897 5.0674 2.3379 3.0502\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5999384289148331 [best r2] 0.8483876414792555\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5367949276445124 [r2] 0.878622539261775\n",
      "......Outputs 5.2694 5.8345 3.2638 5.8345 5.7336 2.8966 7.1320 5.1885 2.3103 3.1020\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5367949276445124 [best r2] 0.878622539261775\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.48067127076808663 [r2] 0.9026765329117017\n",
      "......Outputs 5.1530 5.7186 3.1332 5.7186 5.7512 2.7932 6.8743 5.1124 2.2402 3.0876\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.48067127076808663 [best r2] 0.9026765329117017\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.43190338780587223 [r2] 0.9214231776182297\n",
      "......Outputs 5.1601 5.7429 3.0856 5.7429 5.9072 2.7736 6.7819 5.1923 2.2001 3.2132\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.43190338780587223 [best r2] 0.9214231776182297\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.3892965647041517 [r2] 0.9361615426778175\n",
      "......Outputs 5.0848 5.6240 2.9954 5.6240 5.7844 2.7172 6.5939 5.1792 2.1315 3.2459\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3892965647041517 [best r2] 0.9361615426778175\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.35161279058634004 [r2] 0.9479224468493035\n",
      "......Outputs 5.0772 5.6412 2.9588 5.6412 5.7300 2.6914 6.5643 5.2320 2.0931 3.2698\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.35161279058634004 [best r2] 0.9479224468493035\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3188596321626393 [r2] 0.9571727368074845\n",
      "......Outputs 5.0418 5.6454 2.9067 5.6454 5.6174 2.6501 6.5362 5.2152 2.0508 3.2287\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3188596321626393 [best r2] 0.9571727368074845\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.29108108371699265 [r2] 0.9643097814643892\n",
      "......Outputs 5.0409 5.6734 2.8764 5.6734 5.5737 2.6350 6.5462 5.2223 2.0224 3.2124\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.29108108371699265 [best r2] 0.9643097814643892\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2674770405148954 [r2] 0.9698633996025059\n",
      "......Outputs 5.0498 5.7095 2.8541 5.7095 5.5740 2.6522 6.5896 5.2284 2.0101 3.2083\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2674770405148954 [best r2] 0.9698633996025059\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.24735323266758974 [r2] 0.9742275074967902\n",
      "......Outputs 5.0323 5.6894 2.8287 5.6894 5.5275 2.6915 6.6006 5.2107 2.0126 3.2101\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24735323266758974 [best r2] 0.9742275074967902\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.2308201531564933 [r2] 0.9775576314246847\n",
      "......Outputs 5.0472 5.7110 2.8164 5.7110 5.5140 2.7449 6.6451 5.2142 2.0174 3.2164\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2308201531564933 [best r2] 0.9775576314246847\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.21575914703677707 [r2] 0.9803908091239318\n",
      "......Outputs 5.0527 5.7263 2.8099 5.7263 5.4740 2.7875 6.6624 5.2036 2.0198 3.2168\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.21575914703677707 [best r2] 0.9803908091239318\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.20250428967553918 [r2] 0.9827261280607951\n",
      "......Outputs 5.0430 5.7130 2.8051 5.7130 5.4463 2.8184 6.6646 5.1782 2.0166 3.2087\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20250428967553918 [best r2] 0.9827261280607951\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.19094448275004272 [r2] 0.9846419714994885\n",
      "......Outputs 5.0347 5.7203 2.8160 5.7203 5.4409 2.8443 6.6785 5.1716 2.0254 3.2107\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19094448275004272 [best r2] 0.9846419714994885\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.18061314018213304 [r2] 0.9862589497516777\n",
      "......Outputs 5.0282 5.7291 2.8243 5.7291 5.4351 2.8608 6.6899 5.1673 2.0428 3.2031\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18061314018213304 [best r2] 0.9862589497516777\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.17104856289707446 [r2] 0.9876757613279662\n",
      "......Outputs 5.0287 5.7314 2.8332 5.7314 5.4188 2.8699 6.6927 5.1620 2.0559 3.1954\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17104856289707446 [best r2] 0.9876757613279662\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16232454428987408 [r2] 0.9889008524339822\n",
      "......Outputs 5.0233 5.7288 2.8397 5.7288 5.4075 2.8654 6.6976 5.1552 2.0595 3.1866\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16232454428987408 [best r2] 0.9889008524339822\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.1543463509290618 [r2] 0.9899650786385726\n",
      "......Outputs 5.0283 5.7320 2.8472 5.7320 5.4092 2.8704 6.7078 5.1576 2.0666 3.1845\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1543463509290618 [best r2] 0.9899650786385726\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.14691791812926408 [r2] 0.9909077624862429\n",
      "......Outputs 5.0352 5.7401 2.8583 5.7401 5.4116 2.8802 6.7134 5.1601 2.0797 3.1833\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14691791812926408 [best r2] 0.9909077624862429\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.139859329557291 [r2] 0.9917604380202699\n",
      "......Outputs 5.0276 5.7405 2.8645 5.7405 5.4056 2.8709 6.7111 5.1542 2.0901 3.1807\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.139859329557291 [best r2] 0.9917604380202699\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.13334261901097988 [r2] 0.992510390015222\n",
      "......Outputs 5.0260 5.7254 2.8706 5.7254 5.3928 2.8636 6.7068 5.1535 2.0944 3.1799\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13334261901097988 [best r2] 0.992510390015222\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.1271440465549962 [r2] 0.9931905301755392\n",
      "......Outputs 5.0270 5.7264 2.8788 5.7264 5.3886 2.8640 6.7084 5.1573 2.0987 3.1742\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1271440465549962 [best r2] 0.9931905301755392\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.12137641700280273 [r2] 0.9937943129994626\n",
      "......Outputs 5.0358 5.7416 2.8911 5.7416 5.3957 2.8676 6.7137 5.1657 2.1066 3.1719\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12137641700280273 [best r2] 0.9937943129994626\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.11597415837524708 [r2] 0.9943344288238456\n",
      "......Outputs 5.0453 5.7416 2.9048 5.7416 5.4014 2.8698 6.7129 5.1704 2.1148 3.1746\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11597415837524708 [best r2] 0.9943344288238456\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.11070394592903592 [r2] 0.9948376500309651\n",
      "......Outputs 5.0319 5.7292 2.9088 5.7292 5.3867 2.8598 6.7018 5.1597 2.1127 3.1703\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11070394592903592 [best r2] 0.9948376500309651\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.10590947913892246 [r2] 0.9952751186699887\n",
      "......Outputs 5.0172 5.7173 2.9055 5.7173 5.3650 2.8554 6.6916 5.1396 2.1044 3.1641\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10590947913892246 [best r2] 0.9952751186699887\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.1014648061744116 [r2] 0.9956633726841652\n",
      "......Outputs 5.0331 5.7354 2.9129 5.7354 5.3701 2.8709 6.7081 5.1527 2.1113 3.1647\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1014648061744116 [best r2] 0.9956633726841652\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.09919964482952126 [r2] 0.9958548383136315\n",
      "......Outputs 5.0723 5.7737 2.9387 5.7737 5.4141 2.8897 6.7380 5.1945 2.1338 3.1719\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09919964482952126 [best r2] 0.9958548383136315\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.09857556136027429 [r2] 0.9959068302228861\n",
      "......Outputs 5.0698 5.7532 2.9532 5.7532 5.4271 2.8749 6.7183 5.1893 2.1379 3.1703\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.09876777823431665 [r2] 0.9958908517516774\n",
      "......Outputs 4.9859 5.6685 2.9220 5.6685 5.3407 2.8229 6.6539 5.1085 2.0943 3.1499\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.11186799627217803 [r2] 0.9947285151900742\n",
      "......Outputs 4.9421 5.6530 2.8867 5.6530 5.2628 2.8253 6.6349 5.0654 2.0636 3.1295\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.1185959957943149 [r2] 0.9940753691874595\n",
      "......Outputs 5.0757 5.8229 2.9472 5.8229 5.3902 2.9313 6.7803 5.2116 2.1305 3.1560\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.16306926039662348 [r2] 0.9887987769923192\n",
      "......Outputs 5.2466 5.9296 3.0668 5.9296 5.6193 2.9940 6.8988 5.3592 2.2289 3.1967\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.16001900324770837 [r2] 0.9892139019810864\n",
      "......Outputs 5.0062 5.6364 2.9814 5.6364 5.3878 2.7662 6.6019 5.1112 2.1245 3.1305\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.21029414566130958 [r2] 0.9813715977990047\n",
      "......Outputs 4.7221 5.4447 2.8024 5.4447 5.0893 2.6473 6.4120 4.8474 1.9645 3.0913\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.18474284781278769 [r2] 0.9856233894293261\n",
      "......Outputs 5.0361 5.8585 2.9262 5.8585 5.3595 2.9610 6.8829 5.2238 2.0930 3.2133\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.2079830070539447 [r2] 0.9817788011810449\n",
      "......Outputs 5.3615 6.0171 3.1081 6.0171 5.6473 3.1241 7.0425 5.4700 2.2567 3.1927\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.1593367756993327 [r2] 0.9893056771669114\n",
      "......Outputs 4.9770 5.5275 2.9440 5.5275 5.3142 2.7425 6.4086 5.0250 2.1058 3.0209\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.1456678481381302 [r2] 0.9910618291809011\n",
      "......Outputs 4.8441 5.6371 2.8658 5.6371 5.2775 2.6987 6.4946 4.9616 2.0557 3.1252\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.12779408279875232 [r2] 0.993120724038143\n",
      "......Outputs 5.1293 5.9724 3.0064 5.9724 5.4837 2.9983 6.9697 5.2859 2.1925 3.2964\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09857556136027429 [best r2] 0.9959068302228861\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.09508563491620135 [r2] 0.9961915254096115\n",
      "......Outputs 5.0880 5.6745 2.9827 5.6745 5.3904 2.9259 6.7270 5.2083 2.1441 3.1064\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09508563491620135 [best r2] 0.9961915254096115\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.09078774764032507 [r2] 0.9965280319146046\n",
      "......Outputs 4.9626 5.5757 2.9115 5.5757 5.3469 2.7583 6.5423 5.0949 2.0567 3.0442\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09078774764032507 [best r2] 0.9965280319146046\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.07398755397331712 [r2] 0.9976941100288761\n",
      "......Outputs 5.0788 5.8276 2.9794 5.8276 5.4383 2.8640 6.7624 5.1963 2.1274 3.2022\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07398755397331712 [best r2] 0.9976941100288761\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.07012990961858717 [r2] 0.9979282955483104\n",
      "......Outputs 5.0939 5.7948 2.9875 5.7948 5.4094 2.9167 6.7669 5.1654 2.1552 3.1872\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07012990961858717 [best r2] 0.9979282955483104\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.06384254248433995 [r2] 0.9982831134863998\n",
      "......Outputs 4.9978 5.6540 2.9317 5.6540 5.3593 2.8336 6.6295 5.1111 2.0985 3.0816\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06384254248433995 [best r2] 0.9982831134863998\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.057653319103693874 [r2] 0.9985998651347003\n",
      "......Outputs 5.0298 5.7263 2.9493 5.7263 5.3876 2.8478 6.6918 5.1787 2.1059 3.1247\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.057653319103693874 [best r2] 0.9985998651347003\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.059444999653380154 [r2] 0.9985114895134716\n",
      "......Outputs 5.0875 5.7952 2.9817 5.7952 5.4120 2.9001 6.7664 5.1964 2.1406 3.1701\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.057653319103693874 [best r2] 0.9985998651347003\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.05401989396166098 [r2] 0.9987707826102867\n",
      "......Outputs 5.0455 5.7212 2.9633 5.7212 5.3846 2.8668 6.6893 5.1356 2.1228 3.1240\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05401989396166098 [best r2] 0.9987707826102867\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.05313765216604855 [r2] 0.9988106053952864\n",
      "......Outputs 5.0129 5.6953 2.9450 5.6953 5.3691 2.8376 6.6592 5.1291 2.1034 3.1058\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05313765216604855 [best r2] 0.9988106053952864\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.051551202157392846 [r2] 0.9988805651179792\n",
      "......Outputs 5.0546 5.7595 2.9634 5.7595 5.3933 2.8755 6.7316 5.1758 2.1234 3.1386\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.051551202157392846 [best r2] 0.9988805651179792\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.05248676890564109 [r2] 0.9988395647369557\n",
      "......Outputs 5.0746 5.7620 2.9777 5.7620 5.4013 2.8909 6.7346 5.1803 2.1341 3.1391\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.051551202157392846 [best r2] 0.9988805651179792\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.04987873428679123 [r2] 0.9989520221835438\n",
      "......Outputs 5.0340 5.7088 2.9594 5.7088 5.3769 2.8509 6.6701 5.1405 2.1128 3.1094\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04987873428679123 [best r2] 0.9989520221835438\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.04894563903371627 [r2] 0.9989908650516309\n",
      "......Outputs 5.0235 5.7182 2.9506 5.7182 5.3693 2.8458 6.6798 5.1372 2.1070 3.1156\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04894563903371627 [best r2] 0.9989908650516309\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.04985974865660112 [r2] 0.998952819827385\n",
      "......Outputs 5.0645 5.7649 2.9707 5.7649 5.3944 2.8850 6.7380 5.1758 2.1272 3.1373\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04894563903371627 [best r2] 0.9989908650516309\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.05021610333657047 [r2] 0.9989377976459246\n",
      "......Outputs 5.0710 5.7492 2.9776 5.7492 5.4011 2.8835 6.7223 5.1784 2.1298 3.1227\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04894563903371627 [best r2] 0.9989908650516309\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.04858752465125856 [r2] 0.9990055778511056\n",
      "......Outputs 5.0297 5.7048 2.9576 5.7048 5.3704 2.8454 6.6658 5.1381 2.1097 3.1046\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04858752465125856 [best r2] 0.9990055778511056\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.0483221401685037 [r2] 0.9990164112281376\n",
      "......Outputs 5.0237 5.7234 2.9513 5.7234 5.3621 2.8505 6.6844 5.1357 2.1068 3.1178\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.0504927936709029 [r2] 0.9989260599442178\n",
      "......Outputs 5.0700 5.7707 2.9747 5.7707 5.3962 2.8915 6.7453 5.1836 2.1281 3.1317\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.05177860871571561 [r2] 0.9988706670630226\n",
      "......Outputs 5.0785 5.7511 2.9841 5.7511 5.4071 2.8856 6.7259 5.1865 2.1327 3.1165\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.05059400879717551 [r2] 0.9989217501044766\n",
      "......Outputs 5.0234 5.6969 2.9572 5.6969 5.3636 2.8390 6.6548 5.1268 2.1083 3.1018\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.052414574623910824 [r2] 0.9988427548427892\n",
      "......Outputs 5.0082 5.7130 2.9434 5.7130 5.3450 2.8427 6.6710 5.1202 2.0996 3.1142\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.05569649361842336 [r2] 0.9986932968116213\n",
      "......Outputs 5.0778 5.7822 2.9763 5.7822 5.3972 2.9002 6.7619 5.1974 2.1292 3.1300\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.06274529797797092 [r2] 0.9983416216742412\n",
      "......Outputs 5.1083 5.7740 3.0006 5.7740 5.4305 2.9031 6.7542 5.2151 2.1461 3.1188\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.06132392555713422 [r2] 0.9984159053164705\n",
      "......Outputs 5.0180 5.6813 2.9599 5.6813 5.3638 2.8280 6.6332 5.1125 2.1087 3.0973\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.07316489435666697 [r2] 0.9977451028350315\n",
      "......Outputs 4.9586 5.6686 2.9179 5.6686 5.3036 2.8104 6.6184 5.0714 2.0770 3.1019\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.07555405832769577 [r2] 0.9975954332632392\n",
      "......Outputs 5.0720 5.7966 2.9655 5.7966 5.3845 2.9107 6.7832 5.2071 2.1223 3.1303\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.10406223541243269 [r2] 0.9954385014327953\n",
      "......Outputs 5.1875 5.8514 3.0413 5.8514 5.4967 2.9628 6.8478 5.2996 2.1819 3.1357\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.10022635400895143 [r2] 0.9957685900299064\n",
      "......Outputs 5.0429 5.6764 2.9873 5.6764 5.3971 2.8281 6.6201 5.1195 2.1298 3.0947\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.14108500848793556 [r2] 0.9916153878491158\n",
      "......Outputs 4.8402 5.5480 2.8633 5.5480 5.2181 2.7201 6.4729 4.9449 2.0287 3.0720\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.14265473922639918 [r2] 0.9914277732706864\n",
      "......Outputs 4.9887 5.7669 2.9060 5.7669 5.3070 2.8812 6.7556 5.1494 2.0764 3.1231\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.20876109593269743 [r2] 0.9816422108643292\n",
      "......Outputs 5.3425 6.0358 3.1053 6.0358 5.6155 3.0973 7.0815 5.4804 2.2450 3.1782\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.20474771713872256 [r2] 0.9823412736166619\n",
      "......Outputs 5.1766 5.7295 3.0844 5.7295 5.5302 2.8997 6.6873 5.2238 2.2101 3.0999\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.26556626044472575 [r2] 0.9702924362925395\n",
      "......Outputs 4.6495 5.3439 2.7943 5.3439 5.1174 2.5656 6.1925 4.7391 1.9741 3.0123\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.2589461285165229 [r2] 0.9717550971382902\n",
      "......Outputs 4.7833 5.6511 2.8043 5.6511 5.1938 2.7699 6.6082 4.9778 1.9947 3.1400\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.30981456328145995 [r2] 0.9595680304072205\n",
      "......Outputs 5.4415 6.2395 3.1418 6.2395 5.7513 3.2310 7.3763 5.6265 2.2754 3.2712\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.2559035398150205 [r2] 0.9724149466906107\n",
      "......Outputs 5.2741 5.7406 3.1346 5.7406 5.5735 2.9690 6.7354 5.3163 2.2432 3.0428\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.2706212363409755 [r2] 0.9691507231180204\n",
      "......Outputs 4.6508 5.2966 2.8020 5.2966 5.1246 2.5206 6.0899 4.7532 1.9892 2.9215\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.19463479618277646 [r2] 0.9840425971105418\n",
      "......Outputs 4.8801 5.8303 2.8853 5.8303 5.3635 2.7651 6.7061 5.0929 2.0760 3.2233\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.1958230848335099 [r2] 0.983847155342324\n",
      "......Outputs 5.2985 6.0655 3.0809 6.0655 5.6079 3.1002 7.1551 5.4404 2.2441 3.3146\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.12798766064935105 [r2] 0.9930998672971724\n",
      "......Outputs 5.0562 5.5343 2.9598 5.5343 5.3281 2.8524 6.5669 5.1506 2.1192 3.0019\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.12009978751046524 [r2] 0.9939241685312453\n",
      "......Outputs 4.9186 5.5600 2.8914 5.5600 5.2956 2.7154 6.4633 5.0754 2.0513 2.9872\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.09220136340323441 [r2] 0.9964190692117597\n",
      "......Outputs 5.1148 5.9239 3.0024 5.9239 5.4770 2.8860 6.8541 5.2567 2.1529 3.2078\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.07634542050434963 [r2] 0.9975447980301054\n",
      "......Outputs 5.1028 5.7807 3.0006 5.7807 5.3952 2.9228 6.7764 5.1734 2.1588 3.1845\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.06852296614890248 [r2] 0.9980221491020183\n",
      "......Outputs 4.9673 5.6093 2.9192 5.6093 5.3219 2.8215 6.5812 5.0936 2.0814 3.0525\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.05612083287933944 [r2] 0.9986733099980872\n",
      "......Outputs 5.0312 5.7431 2.9508 5.7431 5.3955 2.8454 6.6996 5.2084 2.0964 3.0955\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.05778167165950908 [r2] 0.9985936240046209\n",
      "......Outputs 5.1041 5.8036 2.9951 5.8036 5.4096 2.8980 6.7779 5.2196 2.1356 3.1568\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.04870098157342578 [r2] 0.999000928270575\n",
      "......Outputs 5.0378 5.7079 2.9618 5.7079 5.3607 2.8633 6.6753 5.1162 2.1141 3.1139\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0483221401685037 [best r2] 0.9990164112281376\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. The optimal capacity is 1\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}