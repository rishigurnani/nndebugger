{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim, reshape\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Consider using `trainer` function for all tests in `dl_debug`\n",
    "1. Remove all 'importlib' statements\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Try atom vocabulary instead of sum\n",
    "1. Delete this cell\n",
    "1. Delete the cell below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO for Rishi on presentation:\n",
    "1. Describe GNN?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "data_set = {}\n",
    "data_set['train'] = train_X\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class FFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, complexity):\n",
    "\n",
    "        super(FFNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = complexity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "complexity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : FFNet(N_FEATURES, 1, complexity) for complexity in\n",
    "                          complexity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, complexity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = complexity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here.\n",
    "\n",
    "# a list of models that are buggy\n",
    "complexity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, complexity) for complexity in\n",
    "                          complexity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6a3807f87d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import importlib\n",
    "importlib.reload(dl_debug)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'nndebugger.dl_debug' from '/data/rgur/nndebugger/nndebugger/dl_debug.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 137.30295944213867\n",
      "..last epoch zero_data_loss 135.52406883239746\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-64853c3debe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(model_type, correct_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    118\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    119\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    121\u001b[0m                 )\n\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import importlib\n",
    "importlib.reload(constants)\n",
    "importlib.reload(dl_debug)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'nndebugger.dl_debug' from '/data/rgur/nndebugger/nndebugger/dl_debug.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 5.080452065966185\n",
      "....Outputs -0.0215 -0.0118 -0.0215 -0.0048 -0.0261\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "..Epoch 1\n",
      "....Loss: 5.013669021778016\n",
      "....Outputs 0.0494 0.0489 0.0772 0.0489 0.0267\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 2\n",
      "....Loss: 4.939931536366104\n",
      "....Outputs 0.1281 0.1659 0.0844 0.1281 0.1160\n",
      "....Labels  5.7012 5.3739 5.0497 5.6991 2.9694\n",
      "..Epoch 3\n",
      "....Loss: 4.853417574776702\n",
      "....Outputs 0.2705 0.1530 0.1956 0.2201 0.2201\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "..Epoch 4\n",
      "....Loss: 4.747480226046666\n",
      "....Outputs 0.2944 0.3309 0.4002 0.3309 0.2392\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 5\n",
      "....Loss: 4.614684483414844\n",
      "....Outputs 0.5635 0.4170 0.3469 0.4701 0.4701\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 6\n",
      "....Loss: 4.448245296205006\n",
      "....Outputs 0.7672 0.6455 0.4823 0.6455 0.5710\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 4.2390397056813205\n",
      "....Outputs 1.0250 0.8664 0.8664 0.6520 0.7640\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 8\n",
      "....Loss: 3.9801818324969487\n",
      "....Outputs 0.8617 1.1400 1.3457 1.1400 1.0063\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 9\n",
      "....Loss: 3.6597889784133883\n",
      "....Outputs 1.4796 1.3076 1.4796 1.1209 1.7480\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 10\n",
      "....Loss: 3.2675646994269916\n",
      "....Outputs 1.4409 1.8985 2.2447 1.6792 1.8985\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 11\n",
      "....Loss: 2.7926936941155636\n",
      "....Outputs 1.8339 2.4141 2.4141 2.8514 2.1354\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "..Epoch 12\n",
      "....Loss: 2.2323890877199135\n",
      "....Outputs 3.0429 3.0429 2.3081 3.5855 2.6846\n",
      "....Labels  5.7012 5.6991 5.0497 5.3739 2.9694\n",
      "..Epoch 13\n",
      "....Loss: 1.6060330403968386\n",
      "....Outputs 3.8012 3.3304 3.8012 2.8759 4.4634\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 14\n",
      "....Loss: 1.043656140521598\n",
      "....Outputs 3.5428 4.6960 4.0406 4.6960 5.4793\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 15\n",
      "....Loss: 1.0062983767804468\n",
      "....Outputs 4.7193 5.6982 4.2834 6.5630 5.6982\n",
      "....Labels  2.9694 5.7012 5.0497 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.517124967533742\n",
      "....Outputs 7.5070 5.0037 6.6696 5.2224 6.6696\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "..Epoch 17\n",
      "....Loss: 1.9408354812463864\n",
      "....Outputs 7.3538 7.3538 8.0388 5.5404 5.4236\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 18\n",
      "....Loss: 2.0523108599887667\n",
      "....Outputs 5.8075 5.3273 7.6319 8.1057 7.6319\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "..Epoch 19\n",
      "....Loss: 1.885192316803471\n",
      "....Outputs 5.0242 7.8231 7.5613 5.8381 7.5613\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 20\n",
      "....Loss: 1.534874732531883\n",
      "....Outputs 5.6987 7.3279 7.2601 7.2601 4.6043\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.089509099081137\n",
      "....Outputs 6.8283 6.8283 4.1390 5.4584 6.7357\n",
      "....Labels  5.6991 5.7012 2.9694 5.0497 5.3739\n",
      "..Epoch 22\n",
      "....Loss: 0.6259785382839211\n",
      "....Outputs 3.6880 5.1734 6.1308 6.3539 6.3539\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 23\n",
      "....Loss: 0.21782274928121567\n",
      "....Outputs 5.8956 5.8956 3.2816 5.5692 4.8909\n",
      "....Labels  5.7012 5.6991 2.9694 5.3739 5.0497\n",
      "Verified that a small batch can be overfit since the RMSE was less than 0.22499709319526628\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, complexity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = complexity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() # Spoiler! The \"bug\" is here.\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "complexity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, complexity) for complexity in\n",
    "                          complexity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 4.8753192503638925\n",
      "....Outputs 0.1923 0.1942 0.1926 0.1910 0.1910\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 1\n",
      "....Loss: 4.553087323097695\n",
      "....Outputs 0.5218 0.5235 0.5229 0.5208 0.5218\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "..Epoch 2\n",
      "....Loss: 4.230841369280654\n",
      "....Outputs 0.8527 0.8506 0.8557 0.8539 0.8539\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 3\n",
      "....Loss: 3.9045361230395352\n",
      "....Outputs 1.1918 1.1918 1.1881 1.1859 1.1938\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 4\n",
      "....Loss: 3.5711887878576944\n",
      "....Outputs 1.5302 1.5414 1.5391 1.5391 1.5327\n",
      "....Labels  5.0497 5.3739 5.7012 5.6991 2.9694\n",
      "..Epoch 5\n",
      "....Loss: 3.22896163453589\n",
      "....Outputs 1.8865 1.8989 1.9013 1.8989 1.8894\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 6\n",
      "....Loss: 2.877508313177779\n",
      "....Outputs 2.2564 2.2728 2.2754 2.2728 2.2598\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 2.518444968559342\n",
      "....Outputs 2.6618 2.6618 2.6645 2.6447 2.6408\n",
      "....Labels  5.6991 5.7012 5.3739 2.9694 5.0497\n",
      "..Epoch 8\n",
      "....Loss: 2.156291021426768\n",
      "....Outputs 3.0436 3.0392 3.0683 3.0654 3.0654\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 9\n",
      "....Loss: 1.8003493844140595\n",
      "....Outputs 3.4498 3.4848 3.4817 3.4541 3.4817\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "..Epoch 10\n",
      "....Loss: 1.4683976257976312\n",
      "....Outputs 3.9071 3.8689 3.9102 3.9071 3.8718\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 11\n",
      "....Loss: 1.1931570228048027\n",
      "....Outputs 4.3353 4.3382 4.2905 4.2899 4.3353\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "..Epoch 12\n",
      "....Loss: 1.0262729881025852\n",
      "....Outputs 4.7595 4.6989 4.7051 4.7570 4.7570\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "..Epoch 13\n",
      "....Loss: 1.011257935670853\n",
      "....Outputs 5.0998 5.0859 5.1590 5.1590 5.1607\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "..Epoch 14\n",
      "....Loss: 1.124956342061953\n",
      "....Outputs 5.5241 5.4574 5.4345 5.5241 5.5245\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 15\n",
      "....Loss: 1.2923077971288968\n",
      "....Outputs 5.7593 5.8331 5.8319 5.8331 5.7265\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 16\n",
      "....Loss: 1.4492287853952575\n",
      "....Outputs 6.0699 6.0667 5.9895 5.9468 6.0699\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 17\n",
      "....Loss: 1.5610774621847836\n",
      "....Outputs 6.2207 6.2262 6.2262 6.0880 6.1401\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 18\n",
      "....Loss: 1.6157343135723317\n",
      "....Outputs 6.3030 6.3030 6.1515 6.2952 6.2123\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "..Epoch 19\n",
      "....Loss: 1.6146005034815565\n",
      "....Outputs 6.3081 6.1453 6.2138 6.3081 6.2979\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 20\n",
      "....Loss: 1.5661985492703965\n",
      "....Outputs 6.1559 6.0806 6.2529 6.2529 6.2404\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 21\n",
      "....Loss: 1.482416889247627\n",
      "....Outputs 6.0508 5.9697 6.1351 6.1498 6.1498\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 22\n",
      "....Loss: 1.3765389759768603\n",
      "....Outputs 6.0114 5.9946 6.0114 5.9110 5.8248\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 23\n",
      "....Loss: 1.2622074562529506\n",
      "....Outputs 5.6574 5.8305 5.8493 5.7480 5.8493\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 24\n",
      "....Loss: 1.152689322687098\n",
      "....Outputs 5.4776 5.5720 5.6535 5.6741 5.6741\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 25\n",
      "....Loss: 1.0598960832725186\n",
      "....Outputs 5.4950 5.3923 5.4726 5.2944 5.4950\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 26\n",
      "....Loss: 0.9927712172731635\n",
      "....Outputs 5.3196 5.2163 5.2955 5.3196 5.1152\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 27\n",
      "....Loss: 0.9553191460105913\n",
      "....Outputs 5.0502 5.1542 4.9460 5.1542 5.1286\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 28\n",
      "....Loss: 0.9455680186569337\n",
      "....Outputs 5.0036 4.9765 5.0036 4.8987 4.7913\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 29\n",
      "....Loss: 0.9567437187648509\n",
      "....Outputs 4.8427 4.8712 4.6544 4.8712 4.7651\n",
      "....Labels  5.3739 5.6991 2.9694 5.7012 5.0497\n",
      "..Epoch 30\n",
      "....Loss: 0.9800864154317552\n",
      "....Outputs 4.7592 4.7592 4.7294 4.6517 4.5373\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 31\n",
      "....Loss: 1.0074514289298675\n",
      "....Outputs 4.6690 4.4411 4.6378 4.5596 4.6690\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 32\n",
      "....Loss: 1.0326139504497565\n",
      "....Outputs 4.6008 4.3661 4.5682 4.4891 4.6008\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 33\n",
      "....Loss: 1.0514856976467402\n",
      "....Outputs 4.5204 4.4399 4.5543 4.3118 4.5543\n",
      "....Labels  5.3739 5.0497 5.6991 2.9694 5.7012\n",
      "..Epoch 34\n",
      "....Loss: 1.061823854163299\n",
      "....Outputs 4.4935 4.5288 4.2773 4.5288 4.4112\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 35\n",
      "....Loss: 1.0628271160337697\n",
      "....Outputs 4.2614 4.4863 4.4019 4.5231 4.5231\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 36\n",
      "....Loss: 1.0547837390083121\n",
      "....Outputs 4.4102 4.5354 4.2625 4.5354 4.4972\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "..Epoch 37\n",
      "....Loss: 1.0387969287136536\n",
      "....Outputs 4.2788 4.5642 4.5642 4.4345 4.5245\n",
      "....Labels  2.9694 5.7012 5.6991 5.0497 5.3739\n",
      "..Epoch 38\n",
      "....Loss: 1.0165803963897364\n",
      "....Outputs 4.3083 4.4728 4.5660 4.6072 4.6072\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 39\n",
      "....Loss: 0.9902785251864882\n",
      "....Outputs 4.6623 4.3487 4.6196 4.6623 4.5229\n",
      "....Labels  5.7012 2.9694 5.3739 5.6991 5.0497\n",
      "..Epoch 40\n",
      "....Loss: 0.9622879773240117\n",
      "....Outputs 4.6829 4.3979 4.7272 4.7272 4.5824\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "..Epoch 41\n",
      "....Loss: 0.9350450479905756\n",
      "....Outputs 4.4534 4.7992 4.6489 4.7992 4.7534\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 42\n",
      "....Loss: 0.91076508815769\n",
      "....Outputs 4.7197 4.5126 4.8757 4.8757 4.8284\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 43\n",
      "....Loss: 0.8911575599016283\n",
      "....Outputs 4.9541 4.7924 4.5731 4.9052 4.9541\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "..Epoch 44\n",
      "....Loss: 0.8771709350065561\n",
      "....Outputs 4.6322 5.0317 5.0317 4.8642 4.9813\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "..Epoch 45\n",
      "....Loss: 0.8688488073899836\n",
      "....Outputs 5.1059 4.6876 5.0540 4.9326 5.1059\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 46\n",
      "....Loss: 0.8653660107919493\n",
      "....Outputs 5.1210 5.1743 4.9954 5.1743 4.7370\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 47\n",
      "....Loss: 0.8652473259294571\n",
      "....Outputs 5.2350 5.0504 5.1801 5.2350 4.7785\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 48\n",
      "....Loss: 0.8666997001539156\n",
      "....Outputs 5.2862 5.0960 4.8106 5.2862 5.2298\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 49\n",
      "....Loss: 0.8679634609112766\n",
      "....Outputs 5.2688 5.3267 4.8322 5.1311 5.3267\n",
      "....Labels  5.3739 5.7012 2.9694 5.0497 5.6991\n",
      "..Epoch 50\n",
      "....Loss: 0.867597946771854\n",
      "....Outputs 5.3558 4.8427 5.2964 5.3558 5.1549\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 51\n",
      "....Loss: 0.8646563457009314\n",
      "....Outputs 5.1673 5.3735 5.3735 4.8421 5.3125\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 52\n",
      "....Loss: 0.858750758163806\n",
      "....Outputs 4.8308 5.3800 5.3800 5.1688 5.3176\n",
      "....Labels  2.9694 5.6991 5.7012 5.0497 5.3739\n",
      "..Epoch 53\n",
      "....Loss: 0.8500217533132928\n",
      "....Outputs 5.3764 5.3764 5.3124 5.1602 4.8098\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 54\n",
      "....Loss: 0.8390323711647637\n",
      "....Outputs 5.1428 5.3639 5.2983 4.7803 5.3639\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 55\n",
      "....Loss: 0.8266217244785815\n",
      "....Outputs 5.1181 5.3441 5.3441 5.2769 4.7439\n",
      "....Labels  5.0497 5.7012 5.6991 5.3739 2.9694\n",
      "..Epoch 56\n",
      "....Loss: 0.8137309213678295\n",
      "....Outputs 5.3187 5.0879 5.2499 4.7023 5.3187\n",
      "....Labels  5.6991 5.0497 5.3739 2.9694 5.7012\n",
      "..Epoch 57\n",
      "....Loss: 0.801241300598147\n",
      "....Outputs 5.2897 4.6573 5.2193 5.0542 5.2897\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 58\n",
      "....Loss: 0.7898323156357365\n",
      "....Outputs 4.6107 5.0186 5.1868 5.2589 5.2589\n",
      "....Labels  2.9694 5.0497 5.3739 5.6991 5.7012\n",
      "..Epoch 59\n",
      "....Loss: 0.7798976255749281\n",
      "....Outputs 5.2281 5.1544 4.5641 5.2281 4.9830\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 60\n",
      "....Loss: 0.771516862016247\n",
      "....Outputs 5.1990 4.9490 5.1990 4.5189 5.1235\n",
      "....Labels  5.7012 5.0497 5.6991 2.9694 5.3739\n",
      "..Epoch 61\n",
      "....Loss: 0.7644894801302352\n",
      "....Outputs 5.1728 4.9179 5.1728 5.0957 4.4764\n",
      "....Labels  5.6991 5.0497 5.7012 5.3739 2.9694\n",
      "..Epoch 62\n",
      "....Loss: 0.7584152970366802\n",
      "....Outputs 4.4377 5.0720 5.1507 4.8908 5.1507\n",
      "....Labels  2.9694 5.3739 5.6991 5.0497 5.7012\n",
      "..Epoch 63\n",
      "....Loss: 0.7527953744272907\n",
      "....Outputs 5.1337 4.4033 4.8685 5.1337 5.0533\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 64\n",
      "....Loss: 0.7471327412768906\n",
      "....Outputs 5.1222 5.0401 5.1222 4.3738 4.8517\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 65\n",
      "....Loss: 0.7410143364074153\n",
      "....Outputs 4.3493 5.1165 4.8406 5.1165 5.0328\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 66\n",
      "....Loss: 0.7341626042770705\n",
      "....Outputs 4.8351 4.3299 5.0313 5.1167 5.1167\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 67\n",
      "....Loss: 0.7264582241252631\n",
      "....Outputs 4.8352 4.3151 5.1226 5.1226 5.0355\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "..Epoch 68\n",
      "....Loss: 0.7179348095995961\n",
      "....Outputs 4.3047 5.1336 5.0449 5.1336 4.8402\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "..Epoch 69\n",
      "....Loss: 0.7087502841847976\n",
      "....Outputs 5.0588 4.8497 5.1492 5.1492 4.2980\n",
      "....Labels  5.3739 5.0497 5.6991 5.7012 2.9694\n",
      "..Epoch 70\n",
      "....Loss: 0.6991462669950559\n",
      "....Outputs 5.0765 5.1685 5.1685 4.2943 4.8628\n",
      "....Labels  5.3739 5.6991 5.7012 2.9694 5.0497\n",
      "..Epoch 71\n",
      "....Loss: 0.689396647876871\n",
      "....Outputs 5.0970 4.8788 5.1907 4.2928 5.1907\n",
      "....Labels  5.3739 5.0497 5.7012 2.9694 5.6991\n",
      "..Epoch 72\n",
      "....Loss: 0.6797590546925605\n",
      "....Outputs 4.2927 5.2148 5.2148 5.1195 4.8965\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 73\n",
      "....Loss: 0.6704318938374312\n",
      "....Outputs 5.1429 5.2399 5.2399 4.2931 4.9151\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 74\n",
      "....Loss: 0.6615289039712576\n",
      "....Outputs 4.9338 4.2933 5.1663 5.2649 5.2649\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 75\n",
      "....Loss: 0.6530687563466259\n",
      "....Outputs 4.2924 5.2890 4.9515 5.2890 5.1888\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 76\n",
      "....Loss: 0.6449833064913121\n",
      "....Outputs 5.3114 4.9675 4.2900 5.2095 5.3114\n",
      "....Labels  5.7012 5.0497 2.9694 5.3739 5.6991\n",
      "..Epoch 77\n",
      "....Loss: 0.6371445899273283\n",
      "....Outputs 4.2854 4.9812 5.3314 5.3314 5.2280\n",
      "....Labels  2.9694 5.0497 5.6991 5.7012 5.3739\n",
      "..Epoch 78\n",
      "....Loss: 0.6293975380173888\n",
      "....Outputs 4.2783 5.3486 4.9922 5.3486 5.2435\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 79\n",
      "....Loss: 0.6215905415925532\n",
      "....Outputs 5.3627 4.2686 5.0001 5.3627 5.2560\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 80\n",
      "....Loss: 0.6136106104777013\n",
      "....Outputs 5.0049 5.3735 5.3735 4.2560 5.2652\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 81\n",
      "....Loss: 0.6053948388055784\n",
      "....Outputs 5.2711 5.3811 5.0065 5.3811 4.2408\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 82\n",
      "....Loss: 0.5969383815247237\n",
      "....Outputs 5.2740 5.3857 5.0052 5.3857 4.2232\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 83\n",
      "....Loss: 0.5882889168329647\n",
      "....Outputs 4.2034 5.2743 5.0013 5.3877 5.3877\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 84\n",
      "....Loss: 0.5795271597743873\n",
      "....Outputs 5.3874 4.9953 4.1820 5.3874 5.2724\n",
      "....Labels  5.6991 5.0497 2.9694 5.7012 5.3739\n",
      "..Epoch 85\n",
      "....Loss: 0.570749272354805\n",
      "....Outputs 4.1593 5.2688 5.3855 4.9877 5.3855\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 86\n",
      "....Loss: 0.5620452048950376\n",
      "....Outputs 5.2641 4.9791 4.1358 5.3825 5.3825\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "..Epoch 87\n",
      "....Loss: 0.5534819544940709\n",
      "....Outputs 5.3789 4.9700 4.1121 5.3789 5.2589\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 88\n",
      "....Loss: 0.5450937690474613\n",
      "....Outputs 5.3754 5.3754 4.0885 4.9611 5.2537\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 89\n",
      "....Loss: 0.5368806941715204\n",
      "....Outputs 5.3725 4.0655 5.2491 5.3725 4.9527\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 90\n",
      "....Loss: 0.5288133277575496\n",
      "....Outputs 4.9454 4.0434 5.3705 5.3705 5.2455\n",
      "....Labels  5.0497 2.9694 5.7012 5.6991 5.3739\n",
      "..Epoch 91\n",
      "....Loss: 0.5208443704071443\n",
      "....Outputs 4.0224 5.3698 5.3698 5.2432 4.9394\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 92\n",
      "....Loss: 0.5129197564898385\n",
      "....Outputs 4.0027 5.3706 5.3706 5.2424 4.9350\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 93\n",
      "....Loss: 0.5049928290736132\n",
      "....Outputs 5.3731 5.2434 5.3731 3.9845 4.9323\n",
      "....Labels  5.7012 5.3739 5.6991 2.9694 5.0497\n",
      "..Epoch 94\n",
      "....Loss: 0.4970308899001226\n",
      "....Outputs 5.2460 3.9676 4.9314 5.3773 5.3773\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 95\n",
      "....Loss: 0.489022142981577\n",
      "....Outputs 4.9321 5.2503 3.9520 5.3830 5.3830\n",
      "....Labels  5.0497 5.3739 2.9694 5.6991 5.7012\n",
      "..Epoch 96\n",
      "....Loss: 0.480973792389175\n",
      "....Outputs 5.2561 3.9376 5.3903 4.9343 5.3903\n",
      "....Labels  5.3739 2.9694 5.6991 5.0497 5.7012\n",
      "..Epoch 97\n",
      "....Loss: 0.4729090715910358\n",
      "....Outputs 5.3987 5.3987 5.2631 4.9378 3.9242\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 98\n",
      "....Loss: 0.4648603388286617\n",
      "....Outputs 5.4080 3.9116 5.4080 4.9423 5.2711\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 99\n",
      "....Loss: 0.4568609425721275\n",
      "....Outputs 4.9475 5.4180 5.4180 5.2796 3.8995\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2e3e8ce82ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m ds = dl_debug.DebugSession(model_type, buggy_model_class_ls, complexity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      2\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 150\u001b[0;31m                                of data. The minimum RMSE over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not less than {self.EPSILON}''')\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the RMSE was less than {self.EPSILON}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize predictions of a large batch as a function of epoch\n",
    "There should not be a large jump in predicted value between epochs (except, perhaps, in the first few epochs). However, predictions should not stay constant between epochs either."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not pass information along the batch dimension."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# data to illustrate the point\n",
    "polymer_smiles = data_df.smiles.values[[10, 100, 1000]]\n",
    "polymer_smiles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['[*]CCC(C)C([*])C', '[*]CC([*])N1CCCC1=O',\n",
       "       '[*]CCCOC(=O)Nc1cc(NC(=O)O[*])ccc1C'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "feature_dict = {'C': np.array([1,0,0]),\n",
    "    'O': np.array([0,1,0]),\n",
    "    'N': np.array([0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# feature_array = np.zeros((N_DATA_, MAX_N_ATOMS, N_FEATURES_))\n",
    "labels = data_df.value.values[[10,100,1000]]\n",
    "# for ind, smiles in enumerate(polymer_smiles):\n",
    "#     feature_array[ind, ].append(featurize_smiles_by_atom(smiles))\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "# for smiles,data in zip(polymer_smiles,train_X_):\n",
    "#     data.num_atoms = Chem.MolFromSmiles(smiles).GetNumAtoms()\n",
    "data_set_ = {'train': train_X_}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "[(smile, featurize_smiles_by_atom(smiles)) for smile in polymer_smiles]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('[*]CCC(C)C([*])C',\n",
       "  array([[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]])),\n",
       " ('[*]CC([*])N1CCCC1=O',\n",
       "  array([[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]])),\n",
       " ('[*]CCCOC(=O)Nc1cc(NC(=O)O[*])ccc1C',\n",
       "  array([[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]))]"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, complexity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = complexity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = x.sum(dim=1)\n",
    "        # print(x)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "complexity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(N_FEATURES_, 1, complexity) for complexity in\n",
    "                          complexity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession('gnn', correct_graphnet_class_ls, complexity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 3 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 1.0860 1.0352 1.5367\n",
      "....Labels  6.6781 5.4797 5.0413\n",
      "....Loss: 1.0859745740890503\n",
      "Finished charting dependencies. Data is not getting passed along the batch dimension.\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BuggyAtomNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, complexity):\n",
    "\n",
    "        super(BuggyAtomNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = complexity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = x.sum(dim=1)\n",
    "        # print(x)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "complexity_ls = [1,2,3]\n",
    "correct_atomnet_class_ls = [lambda : AtomNet(N_FEATURES_, 1, complexity) for complexity in\n",
    "                          complexity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data & gradient check\n",
    "The capacity of your architecture should be just large enough to overfit the training data. Also, the gradients should not equal zero before overfitting all training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}