{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim, reshape\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Remove all 'importlib' statements\n",
    "1. Consider using `trainer` function for all tests in `dl_debug`\n",
    "1. Clean up arguments passed into DebugSession\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Try using all the polymers for \"chart dependencies\" instead of a small sample\n",
    "1. Change FFNet to MyNet\n",
    "1. Change overfit small batch to an R2 requirement\n",
    "1. Delete this cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "len(data_df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3380"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class FFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(FFNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : FFNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "data_set = {}\n",
    "data_set['train'] = train_X\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here. The correct line is 'return x.view(data.num_graphs,)'\n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-80c0c8fa0ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 134.40209197998047\n",
      "..last epoch zero_data_loss 134.75465393066406\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-66eab23cba70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    159\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    160\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    162\u001b[0m                 )\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 5.164145787847168\n",
      "....Outputs -0.1179 -0.0832 -0.0910 -0.1023 -0.1179\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 1\n",
      "....Loss: 5.1002019730011945\n",
      "....Outputs -0.0351 -0.0503 -0.0165 -0.0337 -0.0503\n",
      "....Labels  2.9694 5.7012 5.3739 5.0497 5.6991\n",
      "..Epoch 2\n",
      "....Loss: 5.040911384104809\n",
      "....Outputs 0.0199 0.0257 0.0519 0.0095 0.0095\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 3\n",
      "....Loss: 4.973188421236432\n",
      "....Outputs 0.0799 0.1340 0.0778 0.0778 0.0908\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 4.888306674583275\n",
      "....Outputs 0.2393 0.1645 0.1709 0.1514 0.1645\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "..Epoch 5\n",
      "....Loss: 4.7807606964063485\n",
      "....Outputs 0.2755 0.2755 0.2396 0.2719 0.3727\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 6\n",
      "....Loss: 4.644630629981769\n",
      "....Outputs 0.5403 0.4175 0.3498 0.4175 0.4008\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 4.474223379589342\n",
      "....Outputs 0.4866 0.7494 0.5966 0.5966 0.5624\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 8\n",
      "....Loss: 4.262562752986111\n",
      "....Outputs 1.0087 0.7640 0.6550 0.8208 0.8208\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 9\n",
      "....Loss: 4.000683249294005\n",
      "....Outputs 1.1004 0.8625 1.3298 1.0138 1.1004\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 10\n",
      "....Loss: 3.6791838931022047\n",
      "....Outputs 1.4460 1.4460 1.3233 1.7261 1.1165\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "..Epoch 11\n",
      "....Loss: 3.2884296161716975\n",
      "....Outputs 1.7023 1.8692 2.2155 1.8692 1.4256\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 12\n",
      "....Loss: 2.8176726352501023\n",
      "....Outputs 2.3882 2.8105 2.3882 2.1640 1.8034\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 13\n",
      "....Loss: 2.2646333826378098\n",
      "....Outputs 2.2582 2.7181 3.0192 3.5265 3.0192\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "..Epoch 14\n",
      "....Loss: 1.6494165198046378\n",
      "....Outputs 4.3795 2.8022 3.3662 3.7759 3.7759\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "..Epoch 15\n",
      "....Loss: 1.0886559799013982\n",
      "....Outputs 4.6727 4.0771 3.4411 5.3692 4.6727\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.0137416690620507\n",
      "....Outputs 4.7595 5.6851 6.4372 5.6851 4.1538\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 17\n",
      "....Loss: 1.5032213905818341\n",
      "....Outputs 6.6780 4.8544 7.3817 5.2751 6.6780\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 18\n",
      "....Loss: 1.9383025814360197\n",
      "....Outputs 7.3862 5.4967 7.3862 7.9418 5.3938\n",
      "....Labels  5.7012 2.9694 5.6991 5.3739 5.0497\n",
      "..Epoch 19\n",
      "....Loss: 2.0708371055563553\n",
      "....Outputs 7.6840 5.4178 7.6840 8.0510 5.6893\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "..Epoch 20\n",
      "....Loss: 1.922636734027545\n",
      "....Outputs 5.7587 7.8103 7.6210 7.6210 5.1292\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.5886829834496452\n",
      "....Outputs 7.3226 7.3511 7.3226 4.7229 5.6589\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 22\n",
      "....Loss: 1.1536622344057446\n",
      "....Outputs 6.8872 4.2626 5.4514 6.8872 6.7891\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 23\n",
      "....Loss: 0.6942094990871401\n",
      "....Outputs 5.1924 6.4081 6.4081 3.8045 6.2042\n",
      "....Labels  5.0497 5.6991 5.7012 2.9694 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 0.27488628496386613\n",
      "....Outputs 4.9270 3.3841 5.6515 5.9386 5.9386\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 25\n",
      "....Loss: 0.21981452876476146\n",
      "....Outputs 4.6885 5.5193 5.1660 5.5193 3.0193\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "Verified that a small batch can be overfit since the RMSE was less than 0.22499709319526628\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x) # Spoiler! The \"bug\" is here.\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 4.805500439356576\n",
      "....Outputs 0.2629 0.2636 0.2640 0.2640 0.2620\n",
      "....Labels  5.3739 5.0497 5.6991 5.7012 2.9694\n",
      "..Epoch 1\n",
      "....Loss: 4.435932796055207\n",
      "....Outputs 0.6406 0.6434 0.6434 0.6420 0.6415\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 2\n",
      "....Loss: 4.065092697306583\n",
      "....Outputs 1.0243 1.0260 1.0221 1.0260 1.0225\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 3\n",
      "....Loss: 3.689960887050353\n",
      "....Outputs 1.4155 1.4155 1.4135 1.4100 1.4102\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 3.3075887871990797\n",
      "....Outputs 1.8080 1.8161 1.8138 1.8161 1.8091\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 5\n",
      "....Loss: 2.9168664772983215\n",
      "....Outputs 2.2281 2.2307 2.2216 2.2307 2.2195\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 6\n",
      "....Loss: 2.519406243113757\n",
      "....Outputs 2.6488 2.6607 2.6607 2.6577 2.6456\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 7\n",
      "....Loss: 2.1208775868573455\n",
      "....Outputs 3.1025 3.1058 3.1058 3.0903 3.0862\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 8\n",
      "....Loss: 1.7338260778610044\n",
      "....Outputs 3.5432 3.5601 3.5637 3.5637 3.5387\n",
      "....Labels  2.9694 5.3739 5.6991 5.7012 5.0497\n",
      "..Epoch 9\n",
      "....Loss: 1.3835805710305429\n",
      "....Outputs 4.0294 4.0255 4.0015 4.0294 3.9984\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 10\n",
      "....Loss: 1.1180571792359077\n",
      "....Outputs 4.4945 4.4560 4.4945 4.4567 4.4900\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 11\n",
      "....Loss: 1.00505095651603\n",
      "....Outputs 4.9460 4.9408 4.9008 4.8938 4.9460\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 12\n",
      "....Loss: 1.0690141585637287\n",
      "....Outputs 5.3663 5.3600 5.3131 5.2978 5.3663\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 13\n",
      "....Loss: 1.2396269023942597\n",
      "....Outputs 5.7335 5.6471 5.7256 5.6721 5.7335\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 14\n",
      "....Loss: 1.425915270215542\n",
      "....Outputs 5.9219 5.9570 6.0164 6.0264 6.0264\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 15\n",
      "....Loss: 1.5722411349266978\n",
      "....Outputs 6.1546 6.2313 6.1096 6.2187 6.2313\n",
      "....Labels  5.0497 5.7012 2.9694 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.6558170832447898\n",
      "....Outputs 6.2084 6.3300 6.3453 6.2625 6.3453\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 17\n",
      "....Loss: 1.6739146545193468\n",
      "....Outputs 6.3570 6.3751 6.3751 6.2873 6.2253\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 18\n",
      "....Loss: 1.6344405823016586\n",
      "....Outputs 6.3327 6.1722 6.2410 6.3118 6.3327\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 19\n",
      "....Loss: 1.5503934668459405\n",
      "....Outputs 6.2088 6.2322 6.1378 6.2322 6.0632\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 20\n",
      "....Loss: 1.4369855872528479\n",
      "....Outputs 6.0883 5.9919 6.0624 6.0883 5.9124\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.3102363411814306\n",
      "....Outputs 5.9146 5.9146 5.8169 5.7335 5.8865\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 22\n",
      "....Loss: 1.186149180221001\n",
      "....Outputs 5.7238 5.6936 5.6252 5.5385 5.7238\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "..Epoch 23\n",
      "....Loss: 1.0796563150895606\n",
      "....Outputs 5.5267 5.3381 5.4276 5.5267 5.4948\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 1.002628269979226\n",
      "....Outputs 5.1413 5.3329 5.2993 5.3329 5.2333\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 25\n",
      "....Loss: 0.9609766076046345\n",
      "....Outputs 5.0496 5.1147 4.9554 5.1497 5.1497\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 26\n",
      "....Loss: 0.9526929657880722\n",
      "....Outputs 4.9468 4.9833 4.8824 4.9833 4.7859\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 27\n",
      "....Loss: 0.9691682958438876\n",
      "....Outputs 4.7356 4.8376 4.7998 4.6368 4.8376\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 28\n",
      "....Loss: 0.9992042295878756\n",
      "....Outputs 4.7155 4.5105 4.6120 4.6764 4.7155\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 29\n",
      "....Loss: 1.0326671688537492\n",
      "....Outputs 4.6184 4.5780 4.4083 4.6184 4.5130\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 30\n",
      "....Loss: 1.0621343432259394\n",
      "....Outputs 4.4388 4.5468 4.3304 4.5468 4.5050\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "..Epoch 31\n",
      "....Loss: 1.0829944386185344\n",
      "....Outputs 4.4570 4.3891 4.5003 4.5003 4.2762\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 32\n",
      "....Loss: 1.0929372219858227\n",
      "....Outputs 4.2448 4.4777 4.3629 4.4777 4.4329\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 33\n",
      "....Loss: 1.0913923739163534\n",
      "....Outputs 4.2344 4.4777 4.4311 4.4777 4.3585\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 34\n",
      "....Loss: 1.079070630212774\n",
      "....Outputs 4.4982 4.4982 4.3742 4.4499 4.2433\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 35\n",
      "....Loss: 1.0576429989131608\n",
      "....Outputs 4.4077 4.5371 4.5371 4.2690 4.4869\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 36\n",
      "....Loss: 1.0294969860030931\n",
      "....Outputs 4.5919 4.4566 4.5397 4.3093 4.5919\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 37\n",
      "....Loss: 0.9975241171041463\n",
      "....Outputs 4.3614 4.6598 4.6056 4.6598 4.5182\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 38\n",
      "....Loss: 0.9648815255743177\n",
      "....Outputs 4.4226 4.7379 4.6815 4.7379 4.5895\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "..Epoch 39\n",
      "....Loss: 0.9346764323893912\n",
      "....Outputs 4.8229 4.4898 4.7645 4.8229 4.6677\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 40\n",
      "....Loss: 0.9095701368781034\n",
      "....Outputs 4.9117 4.5600 4.8512 4.7494 4.9117\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 41\n",
      "....Loss: 0.8913522060370198\n",
      "....Outputs 4.8315 5.0010 4.9384 5.0010 4.6300\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 42\n",
      "....Loss: 0.8806075182249623\n",
      "....Outputs 4.6968 4.9108 5.0228 5.0875 5.0875\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 43\n",
      "....Loss: 0.8766271244373088\n",
      "....Outputs 5.1014 5.1680 4.9842 5.1680 4.7576\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 44\n",
      "....Loss: 0.8776314539680883\n",
      "....Outputs 5.2400 5.0492 4.8097 5.2400 5.1714\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 45\n",
      "....Loss: 0.8812335587719614\n",
      "....Outputs 5.3011 5.3011 5.2306 5.1035 4.8511\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 46\n",
      "....Loss: 0.8849773700967407\n",
      "....Outputs 5.3495 5.1453 4.8803 5.3495 5.2771\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 47\n",
      "....Loss: 0.8867955447648992\n",
      "....Outputs 5.3843 4.8963 5.1738 5.3843 5.3101\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 48\n",
      "....Loss: 0.8852987472699397\n",
      "....Outputs 5.4051 4.8990 5.3292 5.4051 5.1885\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 49\n",
      "....Loss: 0.879891822082931\n",
      "....Outputs 5.1900 5.3347 4.8889 5.4125 5.4125\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 50\n",
      "....Loss: 0.8707346839028719\n",
      "....Outputs 5.4073 4.8669 5.1792 5.4073 5.3278\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 51\n",
      "....Loss: 0.8585994689028964\n",
      "....Outputs 5.1577 5.3100 4.8347 5.3912 5.3912\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 52\n",
      "....Loss: 0.8446523644820416\n",
      "....Outputs 5.2833 5.1275 5.3662 5.3662 4.7942\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 53\n",
      "....Loss: 0.8302037761057399\n",
      "....Outputs 5.3347 5.2500 4.7476 5.3347 5.0908\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 54\n",
      "....Loss: 0.8164592524941318\n",
      "....Outputs 4.6971 5.2989 5.0500 5.2989 5.2125\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 55\n",
      "....Loss: 0.804320131162967\n",
      "....Outputs 4.6449 5.2613 5.1732 5.2613 5.0073\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 56\n",
      "....Loss: 0.7942532438404672\n",
      "....Outputs 5.2240 4.9650 5.2240 4.5930 5.1343\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 57\n",
      "....Loss: 0.7862669646672308\n",
      "....Outputs 5.1892 5.0978 4.5433 5.1892 4.9251\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 58\n",
      "....Loss: 0.7799800469218372\n",
      "....Outputs 4.4972 5.1585 5.1585 5.0654 4.8891\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 59\n",
      "....Loss: 0.7747558378336324\n",
      "....Outputs 4.8585 5.0384 5.1334 4.4559 5.1334\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "..Epoch 60\n",
      "....Loss: 0.7698637345733651\n",
      "....Outputs 5.1146 4.4201 5.0179 4.8340 5.1146\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 61\n",
      "....Loss: 0.7646262602921209\n",
      "....Outputs 5.0045 4.8164 5.1030 5.1030 4.3904\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 62\n",
      "....Loss: 0.7585283025290552\n",
      "....Outputs 5.0987 5.0987 4.9984 4.8058 4.3668\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 63\n",
      "....Loss: 0.7512755673035985\n",
      "....Outputs 4.8021 4.3492 5.1016 5.1016 4.9994\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 64\n",
      "....Loss: 0.7428097977065513\n",
      "....Outputs 4.3372 4.8049 5.1112 5.1112 5.0072\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "..Epoch 65\n",
      "....Loss: 0.7332799742171616\n",
      "....Outputs 5.0210 5.1270 5.1270 4.3300 4.8136\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 66\n",
      "....Loss: 0.7229888975206399\n",
      "....Outputs 4.3268 5.1479 5.1479 5.0400 4.8271\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 67\n",
      "....Loss: 0.7123198013647741\n",
      "....Outputs 5.1729 4.8446 4.3267 5.0630 5.1729\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "..Epoch 68\n",
      "....Loss: 0.7016596890357047\n",
      "....Outputs 4.3285 5.0889 4.8647 5.2007 5.2007\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 69\n",
      "....Loss: 0.6913264815873218\n",
      "....Outputs 5.1164 4.8863 4.3313 5.2300 5.2300\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "..Epoch 70\n",
      "....Loss: 0.6815181720561068\n",
      "....Outputs 5.2597 5.2597 4.3338 4.9082 5.1441\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 71\n",
      "....Loss: 0.6722888115758315\n",
      "....Outputs 5.2886 4.3352 4.9292 5.2886 5.1710\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 72\n",
      "....Loss: 0.6635549962148646\n",
      "....Outputs 5.3154 5.1960 4.3345 5.3154 4.9482\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 73\n",
      "....Loss: 0.6551304986770649\n",
      "....Outputs 4.3311 5.3395 4.9646 5.3395 5.2181\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 74\n",
      "....Loss: 0.6467787196429693\n",
      "....Outputs 4.9775 5.3600 5.3600 5.2368 4.3244\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "..Epoch 75\n",
      "....Loss: 0.6382689998949966\n",
      "....Outputs 5.3766 4.3142 4.9865 5.3766 5.2514\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 76\n",
      "....Loss: 0.6294232014946849\n",
      "....Outputs 5.2620 4.3003 4.9916 5.3890 5.3890\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "..Epoch 77\n",
      "....Loss: 0.6201467672186717\n",
      "....Outputs 5.3974 5.2685 5.3974 4.9927 4.2829\n",
      "....Labels  5.7012 5.3739 5.6991 5.0497 2.9694\n",
      "..Epoch 78\n",
      "....Loss: 0.6104376160039681\n",
      "....Outputs 5.2712 5.4019 5.4019 4.9901 4.2621\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "..Epoch 79\n",
      "....Loss: 0.6003735422596815\n",
      "....Outputs 5.4032 5.2705 4.2386 4.9844 5.4032\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "..Epoch 80\n",
      "....Loss: 0.5900861987088514\n",
      "....Outputs 4.2128 4.9760 5.2672 5.4018 5.4018\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 81\n",
      "....Loss: 0.579726321213472\n",
      "....Outputs 5.3984 4.9658 5.3984 4.1854 5.2619\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 82\n",
      "....Loss: 0.5694292361727803\n",
      "....Outputs 5.3938 5.3938 4.9545 5.2554 4.1570\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 83\n",
      "....Loss: 0.5592894323530254\n",
      "....Outputs 5.3888 5.2485 5.3888 4.9429 4.1283\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 84\n",
      "....Loss: 0.5493464076008651\n",
      "....Outputs 5.2420 4.0998 5.3841 5.3841 4.9316\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "..Epoch 85\n",
      "....Loss: 0.5395842252696949\n",
      "....Outputs 5.3804 4.0721 4.9213 5.2364 5.3804\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 86\n",
      "....Loss: 0.5299442175533777\n",
      "....Outputs 5.2323 5.3782 5.3782 4.9126 4.0455\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 87\n",
      "....Loss: 0.5203444634319783\n",
      "....Outputs 4.9057 5.3779 4.0204 5.3779 5.2302\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 88\n",
      "....Loss: 0.5107023322052716\n",
      "....Outputs 5.3797 3.9970 5.3797 4.9011 5.2302\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 89\n",
      "....Loss: 0.5009533317191649\n",
      "....Outputs 3.9753 5.3838 4.8988 5.3838 5.2325\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 90\n",
      "....Loss: 0.4910637727982867\n",
      "....Outputs 4.8987 5.3902 3.9553 5.3902 5.2371\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 91\n",
      "....Loss: 0.4810351777854053\n",
      "....Outputs 4.9007 5.2437 5.3985 5.3985 3.9368\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 92\n",
      "....Loss: 0.47089934315351106\n",
      "....Outputs 5.4087 5.4087 4.9045 3.9196 5.2522\n",
      "....Labels  5.7012 5.6991 5.0497 2.9694 5.3739\n",
      "..Epoch 93\n",
      "....Loss: 0.46070993592763615\n",
      "....Outputs 4.9098 3.9034 5.4202 5.2620 5.4202\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "..Epoch 94\n",
      "....Loss: 0.4505271102673805\n",
      "....Outputs 5.4327 4.9162 5.4327 3.8880 5.2728\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 95\n",
      "....Loss: 0.44040510696872376\n",
      "....Outputs 5.2842 5.4457 5.4457 3.8729 4.9232\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 96\n",
      "....Loss: 0.4303809206165061\n",
      "....Outputs 5.4588 3.8578 5.4588 4.9304 5.2956\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 97\n",
      "....Loss: 0.4204706731076811\n",
      "....Outputs 3.8425 5.4714 5.4714 5.3066 4.9373\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 98\n",
      "....Loss: 0.4106677544339954\n",
      "....Outputs 4.9437 3.8266 5.3170 5.4834 5.4834\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 99\n",
      "....Loss: 0.4009496926039271\n",
      "....Outputs 3.8100 5.4943 4.9492 5.4943 5.3262\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3d84f2f421bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 191\u001b[0;31m                                of data. The minimum RMSE over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not less than {self.EPSILON}''')\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the RMSE was less than {self.EPSILON}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not mix information from separate instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graphnet.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph_batch2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# data to illustrate the point\n",
    "\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()\n",
    "polymer_smiles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[*]C(C#N)=C([*])c1ccccc1',\n",
       " '[*]CCCCOC(=O)C(=O)O[*]',\n",
       " '[*]CC(CCl)(CCl)C(=O)O[*]',\n",
       " '[*]c1[nH]c([*])c(C(=O)O)c1C']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# feature_array = np.zeros((N_DATA_, MAX_N_ATOMS, N_FEATURES_))\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "# for ind, smiles in enumerate(polymer_smiles):\n",
    "#     feature_array[ind, ].append(featurize_smiles_by_atom(smiles))\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "# for smiles,data in zip(polymer_smiles,train_X_):\n",
    "#     data.num_atoms = Chem.MolFromSmiles(smiles).GetNumAtoms()\n",
    "data_set_ = {'train': train_X_}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=2)\n",
    "        x = x - x_mean[:, :, None] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 0.5427 0.4155 0.3948 0.4339\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: 0.542738676071167\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0)\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.1925 -0.2155 -0.2169 -0.1876\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.19253747165203094\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Data is getting mixed between instances in the same batch.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-64c781c65389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_chart_dependencies=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest_model_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_chart_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_choose_model_size_by_overfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mchart_dependencies\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# print(data.x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is getting mixed between instances in the same batch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished charting dependencies. Data is not getting mixed between instances in the same batch.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is getting mixed between instances in the same batch."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data\n",
    "The capacity of your architecture should be just large enough to overfit the training data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import importlib\n",
    "importlib.reload(dl_debug)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'nndebugger.dl_debug' from '/data/rgur/nndebugger/nndebugger/dl_debug.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.567168941972658 [r2] -7.786489497257955\n",
      "......Outputs 0.0509 0.0604 0.0698 0.0604 0.0365 0.0808 0.0562 0.0560 0.0603 0.0559\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.567168941972658 [best r2] -7.786489497257955\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.6914496056835606 [r2] -4.740042775357589\n",
      "......Outputs 0.4901 0.7204 0.5924 0.7204 0.6978 0.9185 0.5859 0.6776 0.5902 0.8029\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.6914496056835606 [best r2] -4.740042775357589\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6362727976802036 [r2] -1.9275339331338874\n",
      "......Outputs 2.2987 3.6586 2.9156 3.6586 3.7082 4.9065 2.7765 3.5014 2.7815 4.1027\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6362727976802036 [best r2] -1.9275339331338874\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.9236150263941838 [r2] -0.558682152943532\n",
      "......Outputs 3.7625 6.7959 4.3306 6.7959 7.0080 7.4083 5.3082 5.8823 4.2955 5.7877\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.9236150263941838 [best r2] -0.558682152943532\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.7950152251140765 [r2] -0.3572426985890702\n",
      "......Outputs 2.5121 4.9889 2.3927 4.9889 5.2675 4.2389 4.2223 3.8400 2.6463 3.0045\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.7950152251140765 [best r2] -0.3572426985890702\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4919902496522939 [r2] 0.06232311363706988\n",
      "......Outputs 2.7075 5.8985 2.1561 5.8985 6.3257 3.9480 5.2281 4.0928 2.6299 2.5479\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4919902496522939 [best r2] 0.06232311363706988\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.3917735094040196 [r2] 0.1840597205728095\n",
      "......Outputs 3.4236 7.5073 2.4406 7.5073 8.0536 4.3822 6.9646 5.1626 3.0969 2.7837\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.3917735094040196 [best r2] 0.1840597205728095\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2256870612254656 [r2] 0.36717963921579255\n",
      "......Outputs 3.2638 6.3146 2.2285 6.3146 6.7205 3.6075 6.4349 4.6986 2.6882 2.5156\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2256870612254656 [best r2] 0.36717963921579255\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0657333215255484 [r2] 0.5215700814150481\n",
      "......Outputs 3.6024 5.9163 2.4711 5.9163 6.2405 3.5407 6.5752 4.8912 2.6830 2.7398\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0657333215255484 [best r2] 0.5215700814150481\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9554351185848157 [r2] 0.6154758314476064\n",
      "......Outputs 4.3713 6.1758 3.0675 6.1758 6.4800 3.8999 7.3263 5.5783 2.9350 3.2929\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9554351185848157 [best r2] 0.6154758314476064\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8383488555408521 [r2] 0.7039460959502375\n",
      "......Outputs 4.4724 5.5500 3.0417 5.5500 5.7952 3.4394 6.9911 5.2050 2.6548 3.1863\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8383488555408521 [best r2] 0.7039460959502375\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7378780947578538 [r2] 0.7706543663512719\n",
      "......Outputs 4.9178 5.6890 3.2650 5.6890 5.8854 3.3181 7.2591 5.2617 2.5885 3.2166\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7378780947578538 [best r2] 0.7706543663512719\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6566257309701073 [r2] 0.818382775086618\n",
      "......Outputs 5.1606 5.7398 3.3744 5.7398 5.8341 3.1371 7.2626 5.2183 2.4652 3.1452\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6566257309701073 [best r2] 0.818382775086618\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.5846334186097973 [r2] 0.8560245268907364\n",
      "......Outputs 5.2115 5.6220 3.3940 5.6220 5.6194 2.9578 7.0233 5.1118 2.3371 3.0607\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5846334186097973 [best r2] 0.8560245268907364\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5248905914803061 [r2] 0.883946346434663\n",
      "......Outputs 5.3452 5.7883 3.4782 5.7883 5.7245 2.9334 7.0293 5.2348 2.3029 3.1215\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5248905914803061 [best r2] 0.883946346434663\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.46867271961341694 [r2] 0.9074746810671842\n",
      "......Outputs 5.1842 5.7046 3.3147 5.7046 5.6699 2.8224 6.8219 5.0999 2.2081 3.0800\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.46867271961341694 [best r2] 0.9074746810671842\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.42194671504320497 [r2] 0.9250042830698607\n",
      "......Outputs 5.1415 5.7856 3.2695 5.7856 5.7917 2.8215 6.7887 5.1739 2.1750 3.1574\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.42194671504320497 [best r2] 0.9250042830698607\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.3802179179254348 [r2] 0.9391043319092589\n",
      "......Outputs 4.9956 5.6403 3.1726 5.6403 5.7159 2.7827 6.6235 5.1108 2.1028 3.1906\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3802179179254348 [best r2] 0.9391043319092589\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.3447647716964154 [r2] 0.949931220300668\n",
      "......Outputs 4.9807 5.7051 3.1379 5.7051 5.7369 2.7951 6.6155 5.1599 2.0635 3.2442\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3447647716964154 [best r2] 0.949931220300668\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3129202946190174 [r2] 0.9587533480687177\n",
      "......Outputs 4.9458 5.6941 3.0783 5.6941 5.6490 2.7687 6.5796 5.1205 2.0171 3.2226\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3129202946190174 [best r2] 0.9587533480687177\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.28589731106143584 [r2] 0.9655696543763258\n",
      "......Outputs 4.9653 5.7217 3.0437 5.7217 5.6081 2.7702 6.5912 5.1204 1.9976 3.2058\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.28589731106143584 [best r2] 0.9655696543763258\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2623500739102462 [r2] 0.9710076363982334\n",
      "......Outputs 4.9780 5.7363 3.0061 5.7363 5.5850 2.7681 6.6095 5.1059 1.9848 3.1827\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2623500739102462 [best r2] 0.9710076363982334\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.24136408137454055 [r2] 0.9754604540480722\n",
      "......Outputs 4.9777 5.7035 2.9674 5.7035 5.5352 2.7653 6.6063 5.0982 1.9757 3.1742\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24136408137454055 [best r2] 0.9754604540480722\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.2233014041727192 [r2] 0.9789958965349694\n",
      "......Outputs 4.9996 5.7179 2.9425 5.7179 5.5178 2.7857 6.6384 5.1288 1.9805 3.1885\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2233014041727192 [best r2] 0.9789958965349694\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.2070789501466385 [r2] 0.9819368640786688\n",
      "......Outputs 5.0155 5.7172 2.9158 5.7172 5.4940 2.8030 6.6543 5.1403 1.9900 3.2033\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2070789501466385 [best r2] 0.9819368640786688\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.1922049430665851 [r2] 0.984438539814104\n",
      "......Outputs 5.0164 5.7154 2.8892 5.7154 5.4634 2.8068 6.6567 5.1314 1.9937 3.1953\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1922049430665851 [best r2] 0.984438539814104\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.17949862221850096 [r2] 0.986428011603099\n",
      "......Outputs 5.0226 5.7261 2.8675 5.7261 5.4503 2.8141 6.6731 5.1274 2.0009 3.1793\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17949862221850096 [best r2] 0.986428011603099\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.16821961804174554 [r2] 0.9880800474680693\n",
      "......Outputs 5.0318 5.7437 2.8669 5.7437 5.4375 2.8363 6.6946 5.1400 2.0160 3.1701\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16821961804174554 [best r2] 0.9880800474680693\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1582919198108691 [r2] 0.9894454740107144\n",
      "......Outputs 5.0432 5.7538 2.8778 5.7538 5.4382 2.8544 6.7215 5.1529 2.0336 3.1543\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1582919198108691 [best r2] 0.9894454740107144\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.14884317346546805 [r2] 0.9906679064695023\n",
      "......Outputs 5.0337 5.7255 2.8794 5.7255 5.4203 2.8553 6.7134 5.1326 2.0427 3.1392\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14884317346546805 [best r2] 0.9906679064695023\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.14098179668419497 [r2] 0.9916276510145845\n",
      "......Outputs 5.0019 5.6941 2.8697 5.6941 5.3835 2.8471 6.6852 5.1026 2.0441 3.1338\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14098179668419497 [best r2] 0.9916276510145845\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.1350323565273119 [r2] 0.9923193684033809\n",
      "......Outputs 4.9920 5.7234 2.8698 5.7234 5.3609 2.8490 6.6925 5.1088 2.0481 3.1278\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1350323565273119 [best r2] 0.9923193684033809\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.13064835335691305 [r2] 0.9928099961704172\n",
      "......Outputs 5.0421 5.7971 2.9033 5.7971 5.3977 2.8777 6.7499 5.1675 2.0674 3.1348\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13064835335691305 [best r2] 0.9928099961704172\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.1334301254283252 [r2] 0.9925005566402331\n",
      "......Outputs 5.1039 5.8086 2.9580 5.8086 5.4592 2.9092 6.7873 5.2178 2.0982 3.1498\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13064835335691305 [best r2] 0.9928099961704172\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.1295307520261282 [r2] 0.9929324805106438\n",
      "......Outputs 5.0098 5.6232 2.9441 5.6232 5.3869 2.8655 6.6569 5.1099 2.0957 3.1110\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.13874234554981907 [r2] 0.9918915226945585\n",
      "......Outputs 4.8798 5.6107 2.8645 5.6107 5.2944 2.8070 6.5783 4.9994 2.0618 3.0823\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.13454170258615028 [r2] 0.9923750837235579\n",
      "......Outputs 5.0299 5.8861 2.9164 5.8861 5.3897 2.8871 6.7719 5.1896 2.0882 3.1583\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.14635747944839525 [r2] 0.9909769973712583\n",
      "......Outputs 5.2094 5.8954 3.0478 5.8954 5.5198 2.9656 6.9029 5.3488 2.1430 3.1639\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.13074686687087902 [r2] 0.9927991490442051\n",
      "......Outputs 4.9709 5.5334 2.9594 5.5334 5.3396 2.8257 6.5960 5.0520 2.1091 3.0192\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.1303855739449413 [r2] 0.9928388902945877\n",
      "......Outputs 4.8783 5.6355 2.8616 5.6355 5.3030 2.7879 6.5873 4.9858 2.0836 3.0907\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1295307520261282 [best r2] 0.9929324805106438\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.1252536821958186 [r2] 0.9933915098803044\n",
      "......Outputs 5.1373 5.9436 2.9775 5.9436 5.4887 2.9405 6.8818 5.2851 2.1388 3.2471\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1252536821958186 [best r2] 0.9933915098803044\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.1104031955557323 [r2] 0.9948656611323815\n",
      "......Outputs 5.1235 5.7066 3.0119 5.7066 5.4190 2.9087 6.7376 5.2326 2.1350 3.0950\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1104031955557323 [best r2] 0.9948656611323815\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.11180612489911186 [r2] 0.9947343446287049\n",
      "......Outputs 4.9094 5.5913 2.8991 5.5913 5.2851 2.7839 6.5327 5.0137 2.0909 3.0033\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1104031955557323 [best r2] 0.9948656611323815\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.09688267093121489 [r2] 0.9960462113773186\n",
      "......Outputs 5.0186 5.8451 2.9290 5.8451 5.3972 2.8661 6.7390 5.1451 2.1203 3.1903\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09688267093121489 [best r2] 0.9960462113773186\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.10198091672197669 [r2] 0.9956191431322571\n",
      "......Outputs 5.1567 5.8204 3.0118 5.8204 5.4802 2.9397 6.8474 5.2551 2.1463 3.1894\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09688267093121489 [best r2] 0.9960462113773186\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.09033166204645983 [r2] 0.9965628281763895\n",
      "......Outputs 4.9969 5.5893 2.9553 5.5893 5.3456 2.8392 6.6128 5.1103 2.1097 3.0242\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09033166204645983 [best r2] 0.9965628281763895\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.08952608205444905 [r2] 0.9966238603804549\n",
      "......Outputs 4.9466 5.7137 2.9059 5.7137 5.3160 2.8154 6.6118 5.0885 2.0989 3.0897\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08952608205444905 [best r2] 0.9966238603804549\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.089789258032075 [r2] 0.9966039818200423\n",
      "......Outputs 5.1177 5.8863 2.9761 5.8863 5.4469 2.9143 6.8242 5.2379 2.1376 3.2103\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08952608205444905 [best r2] 0.9966238603804549\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.08322959290648316 [r2] 0.9970820571498199\n",
      "......Outputs 5.1060 5.6997 2.9997 5.6997 5.4277 2.9007 6.7448 5.1967 2.1385 3.1007\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08322959290648316 [best r2] 0.9970820571498199\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.08669522114239504 [r2] 0.9968339952773796\n",
      "......Outputs 4.9415 5.6088 2.9209 5.6088 5.3059 2.8109 6.5755 5.0588 2.0998 3.0323\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08322959290648316 [best r2] 0.9970820571498199\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.07912171183965351 [r2] 0.9973629850579421\n",
      "......Outputs 5.0081 5.8057 2.9250 5.8057 5.3580 2.8554 6.7066 5.1448 2.1109 3.1509\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.09044464191456292 [r2] 0.9965542249013511\n",
      "......Outputs 5.1596 5.8466 3.0093 5.8466 5.4691 2.9339 6.8420 5.2696 2.1480 3.1802\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.08046202032496791 [r2] 0.9972728871668606\n",
      "......Outputs 5.0444 5.6353 2.9843 5.6353 5.3798 2.8650 6.6619 5.1396 2.1274 3.0592\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.09137807441260833 [r2] 0.9964827337486563\n",
      "......Outputs 4.9132 5.6432 2.8990 5.6432 5.2816 2.8006 6.5685 5.0329 2.0901 3.0646\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.08616702975168057 [r2] 0.9968724555822734\n",
      "......Outputs 5.0657 5.8618 2.9471 5.8618 5.3998 2.8909 6.7759 5.2051 2.1181 3.1805\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.1021645706027314 [r2] 0.9956033502595523\n",
      "......Outputs 5.1910 5.8239 3.0376 5.8239 5.4940 2.9466 6.8476 5.2966 2.1555 3.1529\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.09525067890152313 [r2] 0.9961782928884526\n",
      "......Outputs 4.9872 5.5807 2.9647 5.5807 5.3351 2.8308 6.5946 5.0756 2.1172 3.0382\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.11076896694155626 [r2] 0.9948315841262131\n",
      "......Outputs 4.8835 5.6463 2.8694 5.6463 5.2584 2.7873 6.5570 5.0065 2.0779 3.0829\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.11198147105115076 [r2] 0.9947178153656421\n",
      "......Outputs 5.1292 5.9314 2.9694 5.9314 5.4501 2.9258 6.8483 5.2807 2.1255 3.2058\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.12852593090415176 [r2] 0.9930417062786426\n",
      "......Outputs 5.2276 5.8215 3.0731 5.8215 5.5201 2.9605 6.8598 5.3243 2.1667 3.1394\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.12784700483080025 [r2] 0.993115025172621\n",
      "......Outputs 4.9135 5.5012 2.9366 5.5012 5.2833 2.7912 6.5107 4.9891 2.0995 3.0084\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.1406116738846619 [r2] 0.9916715535558429\n",
      "......Outputs 4.8440 5.6524 2.8428 5.6524 5.2362 2.7767 6.5387 4.9852 2.0643 3.0946\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.15081287224904147 [r2] 0.9904192818669865\n",
      "......Outputs 5.2046 6.0296 3.0066 6.0296 5.5222 2.9726 6.9419 5.3597 2.1398 3.2431\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.15208108274109516 [r2] 0.9902574726751856\n",
      "......Outputs 5.2392 5.7704 3.0971 5.7704 5.5271 2.9636 6.8388 5.3177 2.1718 3.1253\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.16049192881860413 [r2] 0.9891500525716359\n",
      "......Outputs 4.8288 5.4218 2.8941 5.4218 5.2154 2.7418 6.4130 4.9233 2.0778 2.9651\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.14862645242364572 [r2] 0.9906950624162222\n",
      "......Outputs 4.8698 5.7397 2.8515 5.7397 5.2756 2.7957 6.5958 5.0442 2.0741 3.1336\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.17011753489935377 [r2] 0.9878095593983696\n",
      "......Outputs 5.2794 6.0730 3.0518 6.0730 5.5962 3.0144 7.0312 5.4092 2.1658 3.2824\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.13846391757908777 [r2] 0.9919240342046024\n",
      "......Outputs 5.1557 5.6196 3.0561 5.6196 5.4393 2.9106 6.7179 5.2217 2.1460 3.0685\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.15699144295313092 [r2] 0.9896181869690529\n",
      "......Outputs 4.7988 5.4591 2.8597 5.4591 5.1902 2.7223 6.3985 4.9370 2.0606 2.9517\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.12341782541231469 [r2] 0.9935838128835492\n",
      "......Outputs 4.9914 5.9157 2.9126 5.9157 5.3965 2.8616 6.7536 5.1763 2.1068 3.1936\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.14221714246365397 [r2] 0.9914802836222142\n",
      "......Outputs 5.2615 5.9418 3.0657 5.9418 5.5679 2.9984 6.9652 5.3465 2.1661 3.2555\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.10955607708572576 [r2] 0.9949441499282952\n",
      "......Outputs 5.0203 5.5070 2.9800 5.5070 5.3316 2.8408 6.5744 5.0958 2.1066 3.0096\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.1162560004636481 [r2] 0.9943068582561672\n",
      "......Outputs 4.8731 5.6222 2.8781 5.6222 5.2506 2.7660 6.5073 5.0310 2.0717 3.0105\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.10266295042118975 [r2] 0.9955603501069038\n",
      "......Outputs 5.1127 5.9744 2.9745 5.9744 5.4625 2.9111 6.8532 5.2557 2.1326 3.2263\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.09816570543372752 [r2] 0.9959407964980629\n",
      "......Outputs 5.1765 5.7619 3.0315 5.7619 5.4736 2.9338 6.8197 5.2367 2.1451 3.1797\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.0918782543177463 [r2] 0.9964441231564731\n",
      "......Outputs 4.9548 5.5252 2.9316 5.5252 5.2991 2.8052 6.5406 5.0563 2.0896 3.0054\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.08153974619843114 [r2] 0.9971993428191473\n",
      "......Outputs 4.9598 5.7584 2.9140 5.7584 5.3248 2.8210 6.6371 5.1221 2.0933 3.0868\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.08922403459239929 [r2] 0.996646603119368\n",
      "......Outputs 5.1490 5.9178 3.0046 5.9178 5.4681 2.9273 6.8586 5.2590 2.1400 3.2159\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07912171183965351 [best r2] 0.9973629850579421\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.07450195532919482 [r2] 0.9976619349872378\n",
      "......Outputs 5.0998 5.6727 3.0030 5.6727 5.4124 2.8897 6.7215 5.1687 2.1274 3.1178\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07450195532919482 [best r2] 0.9976619349872378\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.08117172724019532 [r2] 0.997224566566651\n",
      "......Outputs 4.9397 5.5905 2.9209 5.5905 5.2961 2.8035 6.5610 5.0670 2.0890 3.0283\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07450195532919482 [best r2] 0.9976619349872378\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.069904084083267 [r2] 0.9979416162702905\n",
      "......Outputs 5.0156 5.8158 2.9391 5.8158 5.3654 2.8547 6.7133 5.1673 2.1078 3.1299\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.069904084083267 [best r2] 0.9979416162702905\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.08238720414291843 [r2] 0.9971408247812775\n",
      "......Outputs 5.1545 5.8579 3.0143 5.8579 5.4652 2.9265 6.8385 5.2477 2.1419 3.1899\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.069904084083267 [best r2] 0.9979416162702905\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.06917407842305526 [r2] 0.9979843830365042\n",
      "......Outputs 5.0572 5.6427 2.9860 5.6427 5.3800 2.8658 6.6707 5.1390 2.1208 3.0853\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.07838281587268837 [r2] 0.9974120077977597\n",
      "......Outputs 4.9380 5.6315 2.9149 5.6315 5.2927 2.8056 6.5764 5.0716 2.0899 3.0477\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.0707182195497979 [r2] 0.9978933912828148\n",
      "......Outputs 5.0476 5.8354 2.9512 5.8354 5.3865 2.8735 6.7488 5.1873 2.1135 3.1491\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.08461535970615454 [r2] 0.996984081155665\n",
      "......Outputs 5.1622 5.8342 3.0236 5.8342 5.4714 2.9293 6.8320 5.2518 2.1446 3.1723\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.07343907913406868 [r2] 0.9977281707546478\n",
      "......Outputs 5.0351 5.6278 2.9804 5.6278 5.3634 2.8528 6.6448 5.1236 2.1187 3.0688\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.0860900770956241 [r2] 0.9968780392813394\n",
      "......Outputs 4.9227 5.6362 2.9050 5.6362 5.2791 2.7990 6.5699 5.0577 2.0860 3.0547\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.08004304333110039 [r2] 0.997301214139004\n",
      "......Outputs 5.0630 5.8500 2.9549 5.8500 5.3965 2.8829 6.7686 5.2013 2.1141 3.1599\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.09978338818260055 [r2] 0.9958059101140074\n",
      "......Outputs 5.1909 5.8502 3.0414 5.8502 5.4950 2.9424 6.8561 5.2799 2.1525 3.1710\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.08806276512448002 [r2] 0.9967333253917278\n",
      "......Outputs 5.0271 5.6110 2.9847 5.6110 5.3581 2.8467 6.6293 5.1134 2.1217 3.0583\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.11010418903988883 [r2] 0.9948934342754577\n",
      "......Outputs 4.8804 5.5975 2.8819 5.5975 5.2440 2.7783 6.5271 5.0156 2.0754 3.0462\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.10189615661496743 [r2] 0.9956264222899498\n",
      "......Outputs 5.0586 5.8684 2.9422 5.8684 5.3927 2.8861 6.7780 5.2063 2.1086 3.1691\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.135552008142996 [r2] 0.9922601391551067\n",
      "......Outputs 5.2520 5.9160 3.0715 5.9160 5.5464 2.9777 6.9263 5.3423 2.1696 3.1927\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.1195421254496811 [r2] 0.9939804616250822\n",
      "......Outputs 5.0345 5.5830 3.0031 5.5830 5.3690 2.8515 6.6214 5.1082 2.1342 3.0541\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.15811410547232982 [r2] 0.9894691731604297\n",
      "......Outputs 4.7940 5.5033 2.8442 5.5033 5.1805 2.7331 6.4310 4.9358 2.0564 3.0160\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.1433916483247012 [r2] 0.9913389817228145\n",
      "......Outputs 5.0192 5.8875 2.9113 5.8875 5.3668 2.8716 6.7623 5.1953 2.0910 3.1747\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.1961504619002661 [r2] 0.9837931015395538\n",
      "......Outputs 5.3501 6.0496 3.1152 6.0496 5.6325 3.0347 7.0536 5.4429 2.1885 3.2368\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.1695743818848333 [r2] 0.9878872786567432\n",
      "......Outputs 5.0819 5.5535 3.0478 5.5535 5.4064 2.8720 6.6275 5.1182 2.1515 3.0387\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.22221601450330924 [r2] 0.9791995874284399\n",
      "......Outputs 4.6883 5.3683 2.8004 5.3683 5.1034 2.6759 6.2935 4.8184 2.0317 2.9507\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06917407842305526 [best r2] 0.9979843830365042\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.642011675707765 [r2] -8.076819470438192\n",
      "......Outputs -0.0684 -0.0502 -0.0778 -0.0502 -0.0501 -0.0612 -0.0630 -0.0746 -0.0689 -0.0786\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.642011675707765 [best r2] -8.076819470438192\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.519057995418072 [r2] -4.216438473259185\n",
      "......Outputs 0.5923 0.8245 0.5988 0.8245 0.8869 1.1491 0.6124 0.8064 0.6365 0.8612\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.519057995418072 [best r2] -4.216438473259185\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.6227463912092017 [r2] -1.8975693368869857\n",
      "......Outputs 3.0974 4.6626 3.5035 4.6626 4.9329 6.2121 3.4467 4.7094 3.3816 4.9824\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.6227463912092017 [best r2] -1.8975693368869857\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.85620184720819 [r2] -0.4513482778293403\n",
      "......Outputs 3.7367 6.5296 3.7606 6.5296 6.9827 6.6745 5.1043 5.7848 3.8411 4.8968\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.85620184720819 [best r2] -0.4513482778293403\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8147785749770953 [r2] -0.3872940629675379\n",
      "......Outputs 2.4970 4.8040 2.0436 4.8040 5.2529 3.8096 4.1168 3.8199 2.3751 2.3979\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8147785749770953 [best r2] -0.3872940629675379\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4994158920340177 [r2] 0.052966242429975274\n",
      "......Outputs 3.0094 6.1994 2.1147 6.1994 6.8358 4.0088 5.6672 4.6646 2.6809 2.3419\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4994158920340177 [best r2] 0.052966242429975274\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.369184768594499 [r2] 0.21033050900818784\n",
      "......Outputs 3.6880 7.4364 2.4068 7.4364 8.1293 4.3416 7.2343 5.7188 3.0888 2.6463\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.369184768594499 [best r2] 0.21033050900818784\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2198852927431239 [r2] 0.37315634877424053\n",
      "......Outputs 3.3837 6.0235 2.1221 6.0235 6.5022 3.4143 6.4631 4.9915 2.6133 2.4202\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2198852927431239 [best r2] 0.37315634877424053\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0569506638926756 [r2] 0.5294230260863547\n",
      "......Outputs 3.8875 5.9058 2.5510 5.9058 6.2752 3.4860 6.9142 5.3877 2.7562 2.9157\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0569506638926756 [best r2] 0.5294230260863547\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9365427221776575 [r2] 0.6305323421071445\n",
      "......Outputs 4.6243 6.0516 3.1112 6.0516 6.3859 3.6903 7.5722 5.8962 2.9511 3.4667\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9365427221776575 [best r2] 0.6305323421071445\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8195545274697362 [r2] 0.7170713362160185\n",
      "......Outputs 4.5773 5.4424 2.9840 5.4424 5.7079 3.1600 7.1569 5.3525 2.6139 3.1334\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8195545274697362 [best r2] 0.7170713362160185\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7261465021132911 [r2] 0.777889167331694\n",
      "......Outputs 5.0693 5.6150 3.3021 5.6150 5.9528 3.1466 7.5011 5.4337 2.5963 3.1262\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7261465021132911 [best r2] 0.777889167331694\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6404641271650708 [r2] 0.8272130812269106\n",
      "......Outputs 5.1257 5.5159 3.3673 5.5159 5.7170 2.9357 7.2714 5.1895 2.4412 2.9730\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6404641271650708 [best r2] 0.8272130812269106\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.572344406610701 [r2] 0.862013650057009\n",
      "......Outputs 5.1988 5.5626 3.4748 5.5626 5.6164 2.8382 7.1163 5.1378 2.3651 2.9460\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.572344406610701 [best r2] 0.862013650057009\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5090995268216786 [r2] 0.8908241384876118\n",
      "......Outputs 5.2207 5.6949 3.4290 5.6949 5.6967 2.7845 7.0152 5.1663 2.3035 2.9763\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5090995268216786 [best r2] 0.8908241384876118\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.4556506662911328 [r2] 0.9125448759346795\n",
      "......Outputs 5.1074 5.7025 3.2391 5.7025 5.7293 2.7069 6.8095 5.1373 2.2200 2.9966\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4556506662911328 [best r2] 0.9125448759346795\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4088075954113815 [r2] 0.929602189311816\n",
      "......Outputs 5.0845 5.7341 3.1949 5.7341 5.8066 2.7376 6.6909 5.1962 2.1785 3.0839\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4088075954113815 [best r2] 0.929602189311816\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.36819821452121987 [r2] 0.9428936244282393\n",
      "......Outputs 4.9939 5.6662 3.0980 5.6662 5.7531 2.7145 6.5374 5.1946 2.1155 3.1300\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.36819821452121987 [best r2] 0.9428936244282393\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.33306920938841134 [r2] 0.9532705990943033\n",
      "......Outputs 4.9701 5.6830 3.0336 5.6830 5.7371 2.7146 6.5052 5.2362 2.0758 3.1673\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.33306920938841134 [best r2] 0.9532705990943033\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3021976405758525 [r2] 0.9615316658923431\n",
      "......Outputs 4.9441 5.6964 2.9926 5.6964 5.6336 2.7138 6.4779 5.2376 2.0433 3.1628\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3021976405758525 [best r2] 0.9615316658923431\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.2757752685032223 [r2] 0.9679644729749859\n",
      "......Outputs 4.9493 5.7203 2.9643 5.7203 5.5771 2.7301 6.4940 5.2369 2.0307 3.1388\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2757752685032223 [best r2] 0.9679644729749859\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.25322592099990415 [r2] 0.9729891930098756\n",
      "......Outputs 4.9675 5.7296 2.9348 5.7296 5.5479 2.7505 6.5288 5.2358 2.0252 3.1312\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.25322592099990415 [best r2] 0.9729891930098756\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.23387496795762072 [r2] 0.9769596685241652\n",
      "......Outputs 4.9749 5.7214 2.9142 5.7214 5.4995 2.7701 6.5576 5.2211 2.0253 3.1463\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.23387496795762072 [best r2] 0.9769596685241652\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.2174009727902396 [r2] 0.9800912401928847\n",
      "......Outputs 4.9952 5.7264 2.9082 5.7264 5.4987 2.7967 6.6111 5.2151 2.0347 3.1605\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2174009727902396 [best r2] 0.9800912401928847\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.20311059875059823 [r2] 0.9826225353501077\n",
      "......Outputs 5.0017 5.7205 2.9038 5.7205 5.4743 2.8169 6.6431 5.1941 2.0449 3.1562\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20311059875059823 [best r2] 0.9826225353501077\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.19056314864755092 [r2] 0.984703253102384\n",
      "......Outputs 5.0027 5.7208 2.9061 5.7208 5.4539 2.8253 6.6651 5.1807 2.0516 3.1508\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19056314864755092 [best r2] 0.984703253102384\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.17956712950137288 [r2] 0.9864176498803597\n",
      "......Outputs 5.0103 5.7281 2.9117 5.7281 5.4446 2.8438 6.7000 5.1727 2.0618 3.1499\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17956712950137288 [best r2] 0.9864176498803597\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.16958364562709058 [r2] 0.9878859551998357\n",
      "......Outputs 5.0078 5.7254 2.9062 5.7254 5.4374 2.8536 6.7140 5.1631 2.0722 3.1501\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16958364562709058 [best r2] 0.9878859551998357\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.16080071239493923 [r2] 0.9891082622019997\n",
      "......Outputs 5.0071 5.7340 2.9104 5.7340 5.4325 2.8572 6.7158 5.1653 2.0846 3.1491\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16080071239493923 [best r2] 0.9891082622019997\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.15264530095232576 [r2] 0.9901850493604083\n",
      "......Outputs 5.0071 5.7317 2.9146 5.7317 5.4289 2.8569 6.7128 5.1596 2.0950 3.1412\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15264530095232576 [best r2] 0.9901850493604083\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.14525725799373312 [r2] 0.9911121457433221\n",
      "......Outputs 5.0134 5.7320 2.9176 5.7320 5.4236 2.8569 6.7149 5.1498 2.1009 3.1373\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14525725799373312 [best r2] 0.9911121457433221\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.13842274756432812 [r2] 0.991928836007222\n",
      "......Outputs 5.0167 5.7294 2.9201 5.7294 5.4217 2.8552 6.7113 5.1375 2.1048 3.1374\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13842274756432812 [best r2] 0.991928836007222\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.13196673564616634 [r2] 0.992664154282806\n",
      "......Outputs 5.0193 5.7312 2.9243 5.7312 5.4170 2.8543 6.7052 5.1339 2.1120 3.1396\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13196673564616634 [best r2] 0.992664154282806\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.12602023004742235 [r2] 0.9933103749414356\n",
      "......Outputs 5.0259 5.7391 2.9269 5.7391 5.4151 2.8519 6.7091 5.1381 2.1166 3.1382\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12602023004742235 [best r2] 0.9933103749414356\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.12040214571834298 [r2] 0.9938935375030447\n",
      "......Outputs 5.0356 5.7363 2.9310 5.7363 5.4156 2.8533 6.7111 5.1390 2.1183 3.1349\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12040214571834298 [best r2] 0.9938935375030447\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.11506754485528915 [r2] 0.9944226623853382\n",
      "......Outputs 5.0362 5.7340 2.9310 5.7340 5.4145 2.8523 6.7109 5.1384 2.1194 3.1332\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11506754485528915 [best r2] 0.9944226623853382\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.11000373280284426 [r2] 0.9949027482237021\n",
      "......Outputs 5.0336 5.7344 2.9329 5.7344 5.4086 2.8556 6.7071 5.1360 2.1208 3.1318\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11000373280284426 [best r2] 0.9949027482237021\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.10524570392496643 [r2] 0.995334158353187\n",
      "......Outputs 5.0347 5.7403 2.9382 5.7403 5.4079 2.8612 6.7090 5.1408 2.1229 3.1296\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10524570392496643 [best r2] 0.995334158353187\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.10078768594133634 [r2] 0.995721060077275\n",
      "......Outputs 5.0409 5.7413 2.9456 5.7413 5.4078 2.8627 6.7117 5.1477 2.1260 3.1304\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10078768594133634 [best r2] 0.995721060077275\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.09651719507318489 [r2] 0.9960759853032811\n",
      "......Outputs 5.0361 5.7311 2.9431 5.7311 5.4009 2.8563 6.7037 5.1426 2.1236 3.1276\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09651719507318489 [best r2] 0.9960759853032811\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.09235716284812731 [r2] 0.9964069570612074\n",
      "......Outputs 5.0240 5.7299 2.9354 5.7299 5.3916 2.8559 6.6977 5.1385 2.1204 3.1232\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09235716284812731 [best r2] 0.9964069570612074\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.08854671173495124 [r2] 0.9966973228885487\n",
      "......Outputs 5.0379 5.7470 2.9454 5.7470 5.3979 2.8693 6.7145 5.1530 2.1248 3.1259\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08854671173495124 [best r2] 0.9966973228885487\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.0860183487294447 [r2] 0.996883239412669\n",
      "......Outputs 5.0577 5.7497 2.9661 5.7497 5.4134 2.8731 6.7242 5.1656 2.1305 3.1271\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0860183487294447 [best r2] 0.996883239412669\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.08298840760876762 [r2] 0.9970989440573389\n",
      "......Outputs 5.0363 5.7207 2.9594 5.7207 5.3939 2.8542 6.6957 5.1458 2.1217 3.1196\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08298840760876762 [best r2] 0.9970989440573389\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.0829257070432682 [r2] 0.9971033260942616\n",
      "......Outputs 4.9994 5.7006 2.9224 5.7006 5.3551 2.8388 6.6653 5.1153 2.1073 3.1102\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0829257070432682 [best r2] 0.9971033260942616\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.08257019406507941 [r2] 0.9971281096686514\n",
      "......Outputs 5.0381 5.7587 2.9320 5.7587 5.3715 2.8718 6.7183 5.1483 2.1203 3.1198\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.0963062107188083 [r2] 0.9960931221629955\n",
      "......Outputs 5.1384 5.8192 3.0179 5.8192 5.4679 2.9212 6.8019 5.2243 2.1591 3.1423\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.10816989875771561 [r2] 0.9950712806730743\n",
      "......Outputs 5.0785 5.7125 3.0217 5.7125 5.4490 2.8618 6.6965 5.1809 2.1443 3.1243\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.14237771018092538 [r2] 0.9914610346956788\n",
      "......Outputs 4.8512 5.5427 2.8383 5.5427 5.2722 2.7364 6.5067 5.0200 2.0583 3.0704\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.1722950786867867 [r2] 0.9874954811267816\n",
      "......Outputs 4.9097 5.7204 2.7953 5.7204 5.2587 2.8226 6.6224 5.0741 2.0682 3.0921\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.23085555783796233 [r2] 0.9775507461866524\n",
      "......Outputs 5.3866 6.1372 3.1285 6.1372 5.6472 3.1035 7.0952 5.3949 2.2438 3.2322\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.21787987929106792 [r2] 0.9800034306864082\n",
      "......Outputs 5.1955 5.6600 3.1573 5.6600 5.5526 2.8838 6.7430 5.2330 2.2014 3.1142\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.2545342109477944 [r2] 0.9727093697294632\n",
      "......Outputs 4.5819 5.2509 2.7151 5.2509 5.1441 2.5378 6.2564 4.8550 1.9784 2.9331\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.1912743998133374 [r2] 0.9845888539345922\n",
      "......Outputs 4.9688 5.8405 2.8501 5.8405 5.4495 2.8486 6.8020 5.2092 2.0991 3.2149\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.17743474988055577 [r2] 0.9867383183554773\n",
      "......Outputs 5.3044 6.0101 3.0590 6.0101 5.6004 2.9907 7.0143 5.3757 2.2207 3.2479\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.13564121630931594 [r2] 0.992249948439351\n",
      "......Outputs 4.9899 5.4597 2.9482 5.4597 5.2569 2.7471 6.4653 5.0378 2.0950 2.9850\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.10282986068323091 [r2] 0.9955459023337839\n",
      "......Outputs 4.9985 5.6944 2.9353 5.6944 5.4221 2.8064 6.6421 5.0905 2.0967 3.0608\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.0957144641162013 [r2] 0.9961409857239032\n",
      "......Outputs 5.1065 5.9430 2.9860 5.9430 5.5219 2.9368 6.8594 5.2052 2.1613 3.2046\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08257019406507941 [best r2] 0.9971281096686514\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.06949626454940301 [r2] 0.9979655633811495\n",
      "......Outputs 5.0010 5.6898 2.9424 5.6898 5.3391 2.8520 6.6221 5.1264 2.1196 3.1284\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06949626454940301 [best r2] 0.9979655633811495\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.06512759867444035 [r2] 0.9982133010919245\n",
      "......Outputs 5.0222 5.6753 2.9410 5.6753 5.3910 2.8267 6.6419 5.1721 2.1037 3.0832\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06512759867444035 [best r2] 0.9982133010919245\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.05990851443781492 [r2] 0.9984881860730949\n",
      "......Outputs 5.0951 5.7714 2.9901 5.7714 5.4330 2.8790 6.7504 5.2043 2.1318 3.1238\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05990851443781492 [best r2] 0.9984881860730949\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.053821678993531276 [r2] 0.9987797867847582\n",
      "......Outputs 5.0412 5.7233 2.9638 5.7233 5.3778 2.8650 6.6970 5.1287 2.1205 3.1164\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.053821678993531276 [best r2] 0.9987797867847582\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.052938104319112084 [r2] 0.9988195216902015\n",
      "......Outputs 5.0199 5.7262 2.9439 5.7262 5.4002 2.8549 6.6947 5.1370 2.1112 3.1081\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.052938104319112084 [best r2] 0.9988195216902015\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.051444203437685625 [r2] 0.9988852072519959\n",
      "......Outputs 5.0562 5.7582 2.9722 5.7582 5.4115 2.8745 6.7237 5.1867 2.1251 3.1242\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.051444203437685625 [best r2] 0.9988852072519959\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.049440450097422164 [r2] 0.998970358419525\n",
      "......Outputs 5.0498 5.7334 2.9671 5.7334 5.3860 2.8659 6.7026 5.1670 2.1226 3.1106\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.049440450097422164 [best r2] 0.998970358419525\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.04869850582318469 [r2] 0.9990010298450864\n",
      "......Outputs 5.0390 5.7228 2.9568 5.7228 5.3826 2.8596 6.6932 5.1441 2.1163 3.1043\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04869850582318469 [best r2] 0.9990010298450864\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.04806567084087775 [r2] 0.9990268242982261\n",
      "......Outputs 5.0517 5.7441 2.9685 5.7441 5.3929 2.8706 6.7111 5.1579 2.1217 3.1168\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04806567084087775 [best r2] 0.9990268242982261\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.047505874104712444 [r2] 0.9990493604733853\n",
      "......Outputs 5.0525 5.7445 2.9704 5.7445 5.3948 2.8691 6.7115 5.1652 2.1235 3.1137\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.047505874104712444 [best r2] 0.9990493604733853\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.046690343390480345 [r2] 0.9990817194685112\n",
      "......Outputs 5.0431 5.7266 2.9655 5.7266 5.3827 2.8621 6.6941 5.1562 2.1198 3.1070\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046690343390480345 [best r2] 0.9990817194685112\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.046179090237177554 [r2] 0.9991017194704382\n",
      "......Outputs 5.0430 5.7319 2.9640 5.7319 5.3808 2.8642 6.6979 5.1549 2.1189 3.1091\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046179090237177554 [best r2] 0.9991017194704382\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.04610402099043909 [r2] 0.999104637606428\n",
      "......Outputs 5.0521 5.7470 2.9686 5.7470 5.3891 2.8700 6.7134 5.1624 2.1222 3.1126\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04610402099043909 [best r2] 0.999104637606428\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.04567546601042934 [r2] 0.9991212057333528\n",
      "......Outputs 5.0521 5.7385 2.9716 5.7385 5.3862 2.8679 6.7060 5.1608 2.1224 3.1103\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04567546601042934 [best r2] 0.9991212057333528\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.045121255999286264 [r2] 0.9991424023124997\n",
      "......Outputs 5.0417 5.7262 2.9648 5.7262 5.3774 2.8616 6.6928 5.1527 2.1180 3.1055\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.045121255999286264 [best r2] 0.9991424023124997\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.044840007331159164 [r2] 0.9991530601050101\n",
      "......Outputs 5.0443 5.7361 2.9633 5.7361 5.3792 2.8657 6.7019 5.1561 2.1184 3.1088\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.044840007331159164 [best r2] 0.9991530601050101\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.045056049130072405 [r2] 0.9991448792317156\n",
      "......Outputs 5.0560 5.7467 2.9708 5.7467 5.3873 2.8723 6.7141 5.1646 2.1227 3.1127\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.044840007331159164 [best r2] 0.9991530601050101\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.04469745421144267 [r2] 0.9991584366433194\n",
      "......Outputs 5.0520 5.7356 2.9716 5.7356 5.3831 2.8674 6.7036 5.1603 2.1215 3.1078\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04469745421144267 [best r2] 0.9991584366433194\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.04425254485183048 [r2] 0.9991751067635021\n",
      "......Outputs 5.0386 5.7247 2.9633 5.7247 5.3722 2.8606 6.6902 5.1508 2.1164 3.1038\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04425254485183048 [best r2] 0.9991751067635021\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.04417173271631713 [r2] 0.999178116783248\n",
      "......Outputs 5.0443 5.7374 2.9627 5.7374 5.3764 2.8665 6.7026 5.1563 2.1175 3.1090\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.044871420619755735 [r2] 0.9991518730183775\n",
      "......Outputs 5.0611 5.7505 2.9738 5.7505 5.3888 2.8746 6.7187 5.1686 2.1236 3.1132\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.044531462096533025 [r2] 0.9991646756337237\n",
      "......Outputs 5.0541 5.7352 2.9748 5.7352 5.3829 2.8674 6.7037 5.1615 2.1221 3.1069\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.04430360718352622 [r2] 0.999173202001931\n",
      "......Outputs 5.0319 5.7177 2.9603 5.7177 5.3654 2.8570 6.6827 5.1451 2.1139 3.1013\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.04441237612501712 [r2] 0.9991691373064606\n",
      "......Outputs 5.0405 5.7359 2.9587 5.7359 5.3704 2.8659 6.7007 5.1539 2.1153 3.1086\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.04643005256835537 [r2] 0.9990919294489489\n",
      "......Outputs 5.0719 5.7615 2.9790 5.7615 5.3947 2.8808 6.7310 5.1775 2.1266 3.1162\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.046612846412403416 [r2] 0.9990847652763513\n",
      "......Outputs 5.0631 5.7389 2.9838 5.7389 5.3899 2.8704 6.7090 5.1672 2.1257 3.1077\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.047397074367575676 [r2] 0.9990537098677996\n",
      "......Outputs 5.0165 5.6998 2.9542 5.6998 5.3533 2.8478 6.6628 5.1321 2.1086 3.0960\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.048913609956206465 [r2] 0.9989921853362072\n",
      "......Outputs 5.0226 5.7254 2.9431 5.7254 5.3528 2.8592 6.6868 5.1421 2.1070 3.1053\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.054817715013237796 [r2] 0.9987342058088435\n",
      "......Outputs 5.0943 5.7895 2.9857 5.7895 5.4090 2.8948 6.7615 5.1971 2.1324 3.1240\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.06067120221606351 [r2] 0.9984494476188884\n",
      "......Outputs 5.0991 5.7636 3.0126 5.7636 5.4219 2.8850 6.7401 5.1926 2.1404 3.1155\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.06478520766652592 [r2] 0.9982320378977327\n",
      "......Outputs 4.9862 5.6581 2.9489 5.6581 5.3356 2.8268 6.6190 5.1053 2.1011 3.0847\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.07985422276952288 [r2] 0.9973139319265039\n",
      "......Outputs 4.9535 5.6695 2.8914 5.6695 5.2902 2.8279 6.6208 5.0914 2.0786 3.0890\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.09081332147545251 [r2] 0.9965260756143889\n",
      "......Outputs 5.1255 5.8458 2.9765 5.8458 5.4194 2.9205 6.8188 5.2279 2.1350 3.1375\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.1292103478094393 [r2] 0.9929674013668779\n",
      "......Outputs 5.2314 5.8751 3.0992 5.8751 5.5379 2.9489 6.8725 5.2896 2.1900 3.1478\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.1349314502649547 [r2] 0.9923308431975566\n",
      "......Outputs 4.9629 5.5923 2.9815 5.5923 5.3466 2.7999 6.5519 5.0747 2.1083 3.0689\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.20405070544493753 [r2] 0.9824612982855452\n",
      "......Outputs 4.7268 5.4560 2.7527 5.4560 5.1122 2.7132 6.3826 4.9164 1.9948 3.0290\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.21266729205762883 [r2] 0.9809487865389848\n",
      "......Outputs 5.0693 5.8710 2.8606 5.8710 5.3416 2.9253 6.8405 5.2075 2.0859 3.1432\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.32393102553453423 [r2] 0.9557995864456212\n",
      "......Outputs 5.5774 6.2325 3.2449 6.2325 5.8293 3.1486 7.2919 5.5678 2.2972 3.2447\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.3031712137317654 [r2] 0.9612834040895882\n",
      "......Outputs 5.0615 5.5482 3.1254 5.5482 5.4572 2.8079 6.5258 5.1176 2.1805 3.0375\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.40578413383063383 [r2] 0.9306396357680078\n",
      "......Outputs 4.3534 5.0748 2.6119 5.0748 4.8834 2.4842 5.9311 4.5965 1.8953 2.8856\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.3395203065897768 [r2] 0.9514428990601236\n",
      "......Outputs 4.8998 5.8460 2.7734 5.8460 5.3254 2.9045 6.7520 5.1271 2.0390 3.2248\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.37847258536994993 [r2] 0.939662113373988\n",
      "......Outputs 5.6706 6.4043 3.2157 6.4043 5.9455 3.2984 7.4891 5.7112 2.3320 3.3510\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04417173271631713 [best r2] 0.999178116783248\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. The optimal capacity is 1\n",
      "\n",
      "\n",
      "Debug session complete.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run all tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = dl_debug.DebugSession(model_type, correct_model_class_ls, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_all_tests=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}