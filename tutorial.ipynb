{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook is a tutorial on NetDebugger\n",
    "Author: Rishi Gurnani, Georgia Institute of Technology<br />\n",
    "Creation Date: July 21, 2021 4:54 PM EST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import\n",
    "Some python packages are needed to run this notebook. We import all of those below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch import tensor, cuda, manual_seed, zeros, nn, optim\n",
    "from torch import float as torch_float\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch import device as torch_device\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# nndebugger functions\n",
    "from nndebugger import constants, loss, dl_debug\n",
    "from nndebugger import torch_utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO For Rishi before publishing notebook:\n",
    "\n",
    "1. Remove all 'importlib' statements\n",
    "1. Consider using `trainer` function for all tests in `dl_debug`\n",
    "1. Clean up arguments passed into DebugSession\n",
    "1. Run all cells and verify that the outputs are what you expected\n",
    "1. Try using all the polymers for \"chart dependencies\" instead of a small sample\n",
    "1. Change overfit small batch to an R2 requirement\n",
    "1. Clear outputs\n",
    "1. Delete this cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fix random seeds to ensure reproducible results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "random.seed(constants.RANDOM_SEED)\n",
    "manual_seed(constants.RANDOM_SEED)\n",
    "np.random.seed(constants.RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data set "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_df = pd.read_csv('data/export.csv',index_col=0)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               smiles property   value\n",
       "822           [*]C[*]      Egc  6.8972\n",
       "823       [*]CC([*])C      Egc  6.5196\n",
       "824      [*]CC([*])CC      Egc  6.5170\n",
       "825     [*]CC([*])CCC      Egc  6.7336\n",
       "826  [*]CC([*])CC(C)C      Egc  6.7394"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>[*]C[*]</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>[*]CC([*])C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[*]CC([*])CC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>[*]CC([*])CCC</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>[*]CC([*])CC(C)C</td>\n",
       "      <td>Egc</td>\n",
       "      <td>6.7394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "len(data_df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3380"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Featurize data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "N_FEATURES = 512\n",
    "N_DATA = len(data_df)\n",
    "\n",
    "def featurize_smiles(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=N_FEATURES, useChirality=True)\n",
    "    return np.array(fp)\n",
    "\n",
    "feature_array = np.zeros((N_DATA, N_FEATURES))\n",
    "\n",
    "ind = 0\n",
    "for smiles in data_df.smiles.values:\n",
    "    feature_array[ind,:] = featurize_smiles(smiles)\n",
    "    ind += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write a logical architecture that will pass all test cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(MyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_model_class_ls = [lambda : MyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare inputs for DebugSession"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# bug free processing pipeline!\n",
    "model_type = 'mlp'\n",
    "# data_set\n",
    "n_test = int(np.floor(N_DATA*constants.TRAIN_FRAC))\n",
    "n_train = N_DATA - n_test\n",
    "(X_train, X_test, label_train, \n",
    "label_test) = train_test_split(\n",
    "                                    feature_array,\n",
    "                                    data_df.value.values.tolist(),\n",
    "                                    test_size=n_test,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=constants.RANDOM_SEED\n",
    "                                )\n",
    "\n",
    "train_X = [Data(x=tensor(X_train[ind,:], dtype=torch_float).view(1,N_FEATURES),\n",
    "                y=tensor(label_train[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(n_train)]\n",
    "zero_data_set = [Data(x=zeros((1,N_FEATURES)), y=x.y) for x in train_X]\n",
    "data_set = {}\n",
    "data_set['train'] = train_X\n",
    "loss_fn = loss.st_loss()\n",
    "target_mean = np.mean(label_train)\n",
    "epsilon = constants.DL_DBG_OVERFIT_EPS_RATIO*(target_mean)\n",
    "device = torch_device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test output shape\n",
    "\n",
    "The shape of the model output should match the shape of the labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# this cell should pass since it uses a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Verified that shape of model predictions is equal to shape of labels\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# buggy model. Can you spot the bug?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x # Spoiler! The bug is here. The correct line is 'return x.view(data.num_graphs,)'\n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# this cell should NOT pass since it uses a buggy model \n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_output_shape=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rgur/.conda/envs/mpnn/lib/python3.6/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-80c0c8fa0ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_output_shape=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_target_abs_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_abs_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_output_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;31m# self.grad_check(min_model, file_name='first_grad_check.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# print('\\nSet of gradients plotted to first_grad_check.png\\n', flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_output_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The model output shape {self.output.shape} and label shape {self.data.y.shape} are not the same\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nVerified that shape of model predictions is equal to shape of labels\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output shape torch.Size([6, 1]) and label shape torch.Size([6]) are not the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test input independent baseline\n",
    "The loss of the model should be lower when real features are passed in than when zeroed features are passed in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# trainer without bugs!\n",
    "\n",
    "def trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            loss.backward() # perform backward pass\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# this test should pass since we are using a trainer without bugs\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 0.22421006858348846\n",
      "..last epoch zero_data_loss 14.373095989227295\n",
      "Input-independent baseline is verified\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# trainer with bugs! Can you spot the bug?\n",
    "\n",
    "def buggy_trainer(model, data_set, batch_size, learning_rate, n_epochs, device, loss_obj):\n",
    "    \n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimization\n",
    "    model.train() # set model to train mode\n",
    "    loss_history = []\n",
    "    for epoch in range(n_epochs):\n",
    "        per_epoch_loss = 0\n",
    "        for ind, data in enumerate(data_loader): # loop through training batches\n",
    "            data = data.to(device) # send data to GPU, if available\n",
    "            optimizer.zero_grad() # zero the gradients\n",
    "            output = model(data) # perform forward pass\n",
    "            loss = loss_obj(output, data) # compute loss\n",
    "            per_epoch_loss += loss.detach().cpu().numpy()\n",
    "            optimizer.step() # update weights\n",
    "        loss_history.append(per_epoch_loss)\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Spoiler! The bug is that there is no backward pass being performed!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# this test should NOT pass since we are using a buggy trainer\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking input-independent baseline\n",
      "..last epoch real_data_loss 134.40209197998047\n",
      "..last epoch zero_data_loss 134.75465393066406\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-66eab23cba70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_input_independent_baseline=True, trainer=buggy_trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_input_independent_baseline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_input_independent_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_input_independent_baseline\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    159\u001b[0m             raise ValueError('''The loss of zeroed inputs is nearly the same as the loss of\n\u001b[1;32m    160\u001b[0m                     \u001b[0mreal\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     during training. Check your trainer function and your model architecture.'''\n\u001b[0m\u001b[1;32m    162\u001b[0m                 )\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input-independent baseline is verified\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The loss of zeroed inputs is nearly the same as the loss of\n                    real inputs. This may indicate that your model is not learning anything\n                    during training. Check your trainer function and your model architecture."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit small batch\n",
    "If you hope to learn a good map on your whole data set using model archicture ***A***, then ***A*** should have enough capacity to completely overfit a small batch of the data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# this test should pass since we are using a good model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 5.164145787847168\n",
      "....Outputs -0.1179 -0.0832 -0.0910 -0.1023 -0.1179\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 1\n",
      "....Loss: 5.1002019730011945\n",
      "....Outputs -0.0351 -0.0503 -0.0165 -0.0337 -0.0503\n",
      "....Labels  2.9694 5.7012 5.3739 5.0497 5.6991\n",
      "..Epoch 2\n",
      "....Loss: 5.040911384104809\n",
      "....Outputs 0.0199 0.0257 0.0519 0.0095 0.0095\n",
      "....Labels  5.0497 2.9694 5.3739 5.7012 5.6991\n",
      "..Epoch 3\n",
      "....Loss: 4.973188421236432\n",
      "....Outputs 0.0799 0.1340 0.0778 0.0778 0.0908\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 4.888306674583275\n",
      "....Outputs 0.2393 0.1645 0.1709 0.1514 0.1645\n",
      "....Labels  5.3739 5.6991 2.9694 5.0497 5.7012\n",
      "..Epoch 5\n",
      "....Loss: 4.7807606964063485\n",
      "....Outputs 0.2755 0.2755 0.2396 0.2719 0.3727\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 6\n",
      "....Loss: 4.644630629981769\n",
      "....Outputs 0.5403 0.4175 0.3498 0.4175 0.4008\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 7\n",
      "....Loss: 4.474223379589342\n",
      "....Outputs 0.4866 0.7494 0.5966 0.5966 0.5624\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 8\n",
      "....Loss: 4.262562752986111\n",
      "....Outputs 1.0087 0.7640 0.6550 0.8208 0.8208\n",
      "....Labels  5.3739 2.9694 5.0497 5.7012 5.6991\n",
      "..Epoch 9\n",
      "....Loss: 4.000683249294005\n",
      "....Outputs 1.1004 0.8625 1.3298 1.0138 1.1004\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 10\n",
      "....Loss: 3.6791838931022047\n",
      "....Outputs 1.4460 1.4460 1.3233 1.7261 1.1165\n",
      "....Labels  5.6991 5.7012 2.9694 5.3739 5.0497\n",
      "..Epoch 11\n",
      "....Loss: 3.2884296161716975\n",
      "....Outputs 1.7023 1.8692 2.2155 1.8692 1.4256\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 12\n",
      "....Loss: 2.8176726352501023\n",
      "....Outputs 2.3882 2.8105 2.3882 2.1640 1.8034\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 13\n",
      "....Loss: 2.2646333826378098\n",
      "....Outputs 2.2582 2.7181 3.0192 3.5265 3.0192\n",
      "....Labels  5.0497 2.9694 5.7012 5.3739 5.6991\n",
      "..Epoch 14\n",
      "....Loss: 1.6494165198046378\n",
      "....Outputs 4.3795 2.8022 3.3662 3.7759 3.7759\n",
      "....Labels  5.3739 5.0497 2.9694 5.6991 5.7012\n",
      "..Epoch 15\n",
      "....Loss: 1.0886559799013982\n",
      "....Outputs 4.6727 4.0771 3.4411 5.3692 4.6727\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.0137416690620507\n",
      "....Outputs 4.7595 5.6851 6.4372 5.6851 4.1538\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 17\n",
      "....Loss: 1.5032213905818341\n",
      "....Outputs 6.6780 4.8544 7.3817 5.2751 6.6780\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 18\n",
      "....Loss: 1.9383025814360197\n",
      "....Outputs 7.3862 5.4967 7.3862 7.9418 5.3938\n",
      "....Labels  5.7012 2.9694 5.6991 5.3739 5.0497\n",
      "..Epoch 19\n",
      "....Loss: 2.0708371055563553\n",
      "....Outputs 7.6840 5.4178 7.6840 8.0510 5.6893\n",
      "....Labels  5.6991 2.9694 5.7012 5.3739 5.0497\n",
      "..Epoch 20\n",
      "....Loss: 1.922636734027545\n",
      "....Outputs 5.7587 7.8103 7.6210 7.6210 5.1292\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.5886829834496452\n",
      "....Outputs 7.3226 7.3511 7.3226 4.7229 5.6589\n",
      "....Labels  5.6991 5.3739 5.7012 2.9694 5.0497\n",
      "..Epoch 22\n",
      "....Loss: 1.1536622344057446\n",
      "....Outputs 6.8872 4.2626 5.4514 6.8872 6.7891\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 23\n",
      "....Loss: 0.6942094990871401\n",
      "....Outputs 5.1924 6.4081 6.4081 3.8045 6.2042\n",
      "....Labels  5.0497 5.6991 5.7012 2.9694 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 0.27488628496386613\n",
      "....Outputs 4.9270 3.3841 5.6515 5.9386 5.9386\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 25\n",
      "....Loss: 0.21981452876476146\n",
      "....Outputs 4.6885 5.5193 5.1660 5.5193 3.0193\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "Verified that a small batch can be overfit since the RMSE was less than 0.22499709319526628\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# buggy model. Can you spot the \"bug\"?\n",
    "\n",
    "class BuggyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.sigmoid(x) # Spoiler! The \"bug\" is here.\n",
    "   \n",
    "        return x.view(data.num_graphs,) \n",
    "\n",
    "# a list of models that are buggy\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_model_class_ls = [lambda : BuggyNet(N_FEATURES, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_test_overfit_small_batch=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Checking if a small batch can be overfit\n",
      "epsilon is 0.22499709319526628\n",
      "..Epoch 0\n",
      "....Loss: 4.805500439356576\n",
      "....Outputs 0.2629 0.2636 0.2640 0.2640 0.2620\n",
      "....Labels  5.3739 5.0497 5.6991 5.7012 2.9694\n",
      "..Epoch 1\n",
      "....Loss: 4.435932796055207\n",
      "....Outputs 0.6406 0.6434 0.6434 0.6420 0.6415\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 2\n",
      "....Loss: 4.065092697306583\n",
      "....Outputs 1.0243 1.0260 1.0221 1.0260 1.0225\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 3\n",
      "....Loss: 3.689960887050353\n",
      "....Outputs 1.4155 1.4155 1.4135 1.4100 1.4102\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 4\n",
      "....Loss: 3.3075887871990797\n",
      "....Outputs 1.8080 1.8161 1.8138 1.8161 1.8091\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 5\n",
      "....Loss: 2.9168664772983215\n",
      "....Outputs 2.2281 2.2307 2.2216 2.2307 2.2195\n",
      "....Labels  5.3739 5.7012 2.9694 5.6991 5.0497\n",
      "..Epoch 6\n",
      "....Loss: 2.519406243113757\n",
      "....Outputs 2.6488 2.6607 2.6607 2.6577 2.6456\n",
      "....Labels  2.9694 5.7012 5.6991 5.3739 5.0497\n",
      "..Epoch 7\n",
      "....Loss: 2.1208775868573455\n",
      "....Outputs 3.1025 3.1058 3.1058 3.0903 3.0862\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 8\n",
      "....Loss: 1.7338260778610044\n",
      "....Outputs 3.5432 3.5601 3.5637 3.5637 3.5387\n",
      "....Labels  2.9694 5.3739 5.6991 5.7012 5.0497\n",
      "..Epoch 9\n",
      "....Loss: 1.3835805710305429\n",
      "....Outputs 4.0294 4.0255 4.0015 4.0294 3.9984\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 10\n",
      "....Loss: 1.1180571792359077\n",
      "....Outputs 4.4945 4.4560 4.4945 4.4567 4.4900\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 11\n",
      "....Loss: 1.00505095651603\n",
      "....Outputs 4.9460 4.9408 4.9008 4.8938 4.9460\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 12\n",
      "....Loss: 1.0690141585637287\n",
      "....Outputs 5.3663 5.3600 5.3131 5.2978 5.3663\n",
      "....Labels  5.6991 5.3739 5.0497 2.9694 5.7012\n",
      "..Epoch 13\n",
      "....Loss: 1.2396269023942597\n",
      "....Outputs 5.7335 5.6471 5.7256 5.6721 5.7335\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 14\n",
      "....Loss: 1.425915270215542\n",
      "....Outputs 5.9219 5.9570 6.0164 6.0264 6.0264\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 15\n",
      "....Loss: 1.5722411349266978\n",
      "....Outputs 6.1546 6.2313 6.1096 6.2187 6.2313\n",
      "....Labels  5.0497 5.7012 2.9694 5.3739 5.6991\n",
      "..Epoch 16\n",
      "....Loss: 1.6558170832447898\n",
      "....Outputs 6.2084 6.3300 6.3453 6.2625 6.3453\n",
      "....Labels  2.9694 5.3739 5.7012 5.0497 5.6991\n",
      "..Epoch 17\n",
      "....Loss: 1.6739146545193468\n",
      "....Outputs 6.3570 6.3751 6.3751 6.2873 6.2253\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 18\n",
      "....Loss: 1.6344405823016586\n",
      "....Outputs 6.3327 6.1722 6.2410 6.3118 6.3327\n",
      "....Labels  5.7012 2.9694 5.0497 5.3739 5.6991\n",
      "..Epoch 19\n",
      "....Loss: 1.5503934668459405\n",
      "....Outputs 6.2088 6.2322 6.1378 6.2322 6.0632\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 20\n",
      "....Loss: 1.4369855872528479\n",
      "....Outputs 6.0883 5.9919 6.0624 6.0883 5.9124\n",
      "....Labels  5.7012 5.0497 5.3739 5.6991 2.9694\n",
      "..Epoch 21\n",
      "....Loss: 1.3102363411814306\n",
      "....Outputs 5.9146 5.9146 5.8169 5.7335 5.8865\n",
      "....Labels  5.6991 5.7012 5.0497 2.9694 5.3739\n",
      "..Epoch 22\n",
      "....Loss: 1.186149180221001\n",
      "....Outputs 5.7238 5.6936 5.6252 5.5385 5.7238\n",
      "....Labels  5.7012 5.3739 5.0497 2.9694 5.6991\n",
      "..Epoch 23\n",
      "....Loss: 1.0796563150895606\n",
      "....Outputs 5.5267 5.3381 5.4276 5.5267 5.4948\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 24\n",
      "....Loss: 1.002628269979226\n",
      "....Outputs 5.1413 5.3329 5.2993 5.3329 5.2333\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 25\n",
      "....Loss: 0.9609766076046345\n",
      "....Outputs 5.0496 5.1147 4.9554 5.1497 5.1497\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 26\n",
      "....Loss: 0.9526929657880722\n",
      "....Outputs 4.9468 4.9833 4.8824 4.9833 4.7859\n",
      "....Labels  5.3739 5.7012 5.0497 5.6991 2.9694\n",
      "..Epoch 27\n",
      "....Loss: 0.9691682958438876\n",
      "....Outputs 4.7356 4.8376 4.7998 4.6368 4.8376\n",
      "....Labels  5.0497 5.6991 5.3739 2.9694 5.7012\n",
      "..Epoch 28\n",
      "....Loss: 0.9992042295878756\n",
      "....Outputs 4.7155 4.5105 4.6120 4.6764 4.7155\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 29\n",
      "....Loss: 1.0326671688537492\n",
      "....Outputs 4.6184 4.5780 4.4083 4.6184 4.5130\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 30\n",
      "....Loss: 1.0621343432259394\n",
      "....Outputs 4.4388 4.5468 4.3304 4.5468 4.5050\n",
      "....Labels  5.0497 5.7012 2.9694 5.6991 5.3739\n",
      "..Epoch 31\n",
      "....Loss: 1.0829944386185344\n",
      "....Outputs 4.4570 4.3891 4.5003 4.5003 4.2762\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 32\n",
      "....Loss: 1.0929372219858227\n",
      "....Outputs 4.2448 4.4777 4.3629 4.4777 4.4329\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 33\n",
      "....Loss: 1.0913923739163534\n",
      "....Outputs 4.2344 4.4777 4.4311 4.4777 4.3585\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 34\n",
      "....Loss: 1.079070630212774\n",
      "....Outputs 4.4982 4.4982 4.3742 4.4499 4.2433\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 35\n",
      "....Loss: 1.0576429989131608\n",
      "....Outputs 4.4077 4.5371 4.5371 4.2690 4.4869\n",
      "....Labels  5.0497 5.7012 5.6991 2.9694 5.3739\n",
      "..Epoch 36\n",
      "....Loss: 1.0294969860030931\n",
      "....Outputs 4.5919 4.4566 4.5397 4.3093 4.5919\n",
      "....Labels  5.7012 5.0497 5.3739 2.9694 5.6991\n",
      "..Epoch 37\n",
      "....Loss: 0.9975241171041463\n",
      "....Outputs 4.3614 4.6598 4.6056 4.6598 4.5182\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 38\n",
      "....Loss: 0.9648815255743177\n",
      "....Outputs 4.4226 4.7379 4.6815 4.7379 4.5895\n",
      "....Labels  2.9694 5.6991 5.3739 5.7012 5.0497\n",
      "..Epoch 39\n",
      "....Loss: 0.9346764323893912\n",
      "....Outputs 4.8229 4.4898 4.7645 4.8229 4.6677\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 40\n",
      "....Loss: 0.9095701368781034\n",
      "....Outputs 4.9117 4.5600 4.8512 4.7494 4.9117\n",
      "....Labels  5.6991 2.9694 5.3739 5.0497 5.7012\n",
      "..Epoch 41\n",
      "....Loss: 0.8913522060370198\n",
      "....Outputs 4.8315 5.0010 4.9384 5.0010 4.6300\n",
      "....Labels  5.0497 5.6991 5.3739 5.7012 2.9694\n",
      "..Epoch 42\n",
      "....Loss: 0.8806075182249623\n",
      "....Outputs 4.6968 4.9108 5.0228 5.0875 5.0875\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 43\n",
      "....Loss: 0.8766271244373088\n",
      "....Outputs 5.1014 5.1680 4.9842 5.1680 4.7576\n",
      "....Labels  5.3739 5.6991 5.0497 5.7012 2.9694\n",
      "..Epoch 44\n",
      "....Loss: 0.8776314539680883\n",
      "....Outputs 5.2400 5.0492 4.8097 5.2400 5.1714\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 45\n",
      "....Loss: 0.8812335587719614\n",
      "....Outputs 5.3011 5.3011 5.2306 5.1035 4.8511\n",
      "....Labels  5.6991 5.7012 5.3739 5.0497 2.9694\n",
      "..Epoch 46\n",
      "....Loss: 0.8849773700967407\n",
      "....Outputs 5.3495 5.1453 4.8803 5.3495 5.2771\n",
      "....Labels  5.7012 5.0497 2.9694 5.6991 5.3739\n",
      "..Epoch 47\n",
      "....Loss: 0.8867955447648992\n",
      "....Outputs 5.3843 4.8963 5.1738 5.3843 5.3101\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 48\n",
      "....Loss: 0.8852987472699397\n",
      "....Outputs 5.4051 4.8990 5.3292 5.4051 5.1885\n",
      "....Labels  5.6991 2.9694 5.3739 5.7012 5.0497\n",
      "..Epoch 49\n",
      "....Loss: 0.879891822082931\n",
      "....Outputs 5.1900 5.3347 4.8889 5.4125 5.4125\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 50\n",
      "....Loss: 0.8707346839028719\n",
      "....Outputs 5.4073 4.8669 5.1792 5.4073 5.3278\n",
      "....Labels  5.6991 2.9694 5.0497 5.7012 5.3739\n",
      "..Epoch 51\n",
      "....Loss: 0.8585994689028964\n",
      "....Outputs 5.1577 5.3100 4.8347 5.3912 5.3912\n",
      "....Labels  5.0497 5.3739 2.9694 5.7012 5.6991\n",
      "..Epoch 52\n",
      "....Loss: 0.8446523644820416\n",
      "....Outputs 5.2833 5.1275 5.3662 5.3662 4.7942\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 53\n",
      "....Loss: 0.8302037761057399\n",
      "....Outputs 5.3347 5.2500 4.7476 5.3347 5.0908\n",
      "....Labels  5.7012 5.3739 2.9694 5.6991 5.0497\n",
      "..Epoch 54\n",
      "....Loss: 0.8164592524941318\n",
      "....Outputs 4.6971 5.2989 5.0500 5.2989 5.2125\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 55\n",
      "....Loss: 0.804320131162967\n",
      "....Outputs 4.6449 5.2613 5.1732 5.2613 5.0073\n",
      "....Labels  2.9694 5.7012 5.3739 5.6991 5.0497\n",
      "..Epoch 56\n",
      "....Loss: 0.7942532438404672\n",
      "....Outputs 5.2240 4.9650 5.2240 4.5930 5.1343\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 57\n",
      "....Loss: 0.7862669646672308\n",
      "....Outputs 5.1892 5.0978 4.5433 5.1892 4.9251\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 58\n",
      "....Loss: 0.7799800469218372\n",
      "....Outputs 4.4972 5.1585 5.1585 5.0654 4.8891\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 59\n",
      "....Loss: 0.7747558378336324\n",
      "....Outputs 4.8585 5.0384 5.1334 4.4559 5.1334\n",
      "....Labels  5.0497 5.3739 5.7012 2.9694 5.6991\n",
      "..Epoch 60\n",
      "....Loss: 0.7698637345733651\n",
      "....Outputs 5.1146 4.4201 5.0179 4.8340 5.1146\n",
      "....Labels  5.7012 2.9694 5.3739 5.0497 5.6991\n",
      "..Epoch 61\n",
      "....Loss: 0.7646262602921209\n",
      "....Outputs 5.0045 4.8164 5.1030 5.1030 4.3904\n",
      "....Labels  5.3739 5.0497 5.7012 5.6991 2.9694\n",
      "..Epoch 62\n",
      "....Loss: 0.7585283025290552\n",
      "....Outputs 5.0987 5.0987 4.9984 4.8058 4.3668\n",
      "....Labels  5.7012 5.6991 5.3739 5.0497 2.9694\n",
      "..Epoch 63\n",
      "....Loss: 0.7512755673035985\n",
      "....Outputs 4.8021 4.3492 5.1016 5.1016 4.9994\n",
      "....Labels  5.0497 2.9694 5.6991 5.7012 5.3739\n",
      "..Epoch 64\n",
      "....Loss: 0.7428097977065513\n",
      "....Outputs 4.3372 4.8049 5.1112 5.1112 5.0072\n",
      "....Labels  2.9694 5.0497 5.7012 5.6991 5.3739\n",
      "..Epoch 65\n",
      "....Loss: 0.7332799742171616\n",
      "....Outputs 5.0210 5.1270 5.1270 4.3300 4.8136\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 66\n",
      "....Loss: 0.7229888975206399\n",
      "....Outputs 4.3268 5.1479 5.1479 5.0400 4.8271\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 67\n",
      "....Loss: 0.7123198013647741\n",
      "....Outputs 5.1729 4.8446 4.3267 5.0630 5.1729\n",
      "....Labels  5.6991 5.0497 2.9694 5.3739 5.7012\n",
      "..Epoch 68\n",
      "....Loss: 0.7016596890357047\n",
      "....Outputs 4.3285 5.0889 4.8647 5.2007 5.2007\n",
      "....Labels  2.9694 5.3739 5.0497 5.6991 5.7012\n",
      "..Epoch 69\n",
      "....Loss: 0.6913264815873218\n",
      "....Outputs 5.1164 4.8863 4.3313 5.2300 5.2300\n",
      "....Labels  5.3739 5.0497 2.9694 5.7012 5.6991\n",
      "..Epoch 70\n",
      "....Loss: 0.6815181720561068\n",
      "....Outputs 5.2597 5.2597 4.3338 4.9082 5.1441\n",
      "....Labels  5.7012 5.6991 2.9694 5.0497 5.3739\n",
      "..Epoch 71\n",
      "....Loss: 0.6722888115758315\n",
      "....Outputs 5.2886 4.3352 4.9292 5.2886 5.1710\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 72\n",
      "....Loss: 0.6635549962148646\n",
      "....Outputs 5.3154 5.1960 4.3345 5.3154 4.9482\n",
      "....Labels  5.6991 5.3739 2.9694 5.7012 5.0497\n",
      "..Epoch 73\n",
      "....Loss: 0.6551304986770649\n",
      "....Outputs 4.3311 5.3395 4.9646 5.3395 5.2181\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n",
      "..Epoch 74\n",
      "....Loss: 0.6467787196429693\n",
      "....Outputs 4.9775 5.3600 5.3600 5.2368 4.3244\n",
      "....Labels  5.0497 5.6991 5.7012 5.3739 2.9694\n",
      "..Epoch 75\n",
      "....Loss: 0.6382689998949966\n",
      "....Outputs 5.3766 4.3142 4.9865 5.3766 5.2514\n",
      "....Labels  5.7012 2.9694 5.0497 5.6991 5.3739\n",
      "..Epoch 76\n",
      "....Loss: 0.6294232014946849\n",
      "....Outputs 5.2620 4.3003 4.9916 5.3890 5.3890\n",
      "....Labels  5.3739 2.9694 5.0497 5.6991 5.7012\n",
      "..Epoch 77\n",
      "....Loss: 0.6201467672186717\n",
      "....Outputs 5.3974 5.2685 5.3974 4.9927 4.2829\n",
      "....Labels  5.7012 5.3739 5.6991 5.0497 2.9694\n",
      "..Epoch 78\n",
      "....Loss: 0.6104376160039681\n",
      "....Outputs 5.2712 5.4019 5.4019 4.9901 4.2621\n",
      "....Labels  5.3739 5.6991 5.7012 5.0497 2.9694\n",
      "..Epoch 79\n",
      "....Loss: 0.6003735422596815\n",
      "....Outputs 5.4032 5.2705 4.2386 4.9844 5.4032\n",
      "....Labels  5.7012 5.3739 2.9694 5.0497 5.6991\n",
      "..Epoch 80\n",
      "....Loss: 0.5900861987088514\n",
      "....Outputs 4.2128 4.9760 5.2672 5.4018 5.4018\n",
      "....Labels  2.9694 5.0497 5.3739 5.7012 5.6991\n",
      "..Epoch 81\n",
      "....Loss: 0.579726321213472\n",
      "....Outputs 5.3984 4.9658 5.3984 4.1854 5.2619\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 82\n",
      "....Loss: 0.5694292361727803\n",
      "....Outputs 5.3938 5.3938 4.9545 5.2554 4.1570\n",
      "....Labels  5.6991 5.7012 5.0497 5.3739 2.9694\n",
      "..Epoch 83\n",
      "....Loss: 0.5592894323530254\n",
      "....Outputs 5.3888 5.2485 5.3888 4.9429 4.1283\n",
      "....Labels  5.6991 5.3739 5.7012 5.0497 2.9694\n",
      "..Epoch 84\n",
      "....Loss: 0.5493464076008651\n",
      "....Outputs 5.2420 4.0998 5.3841 5.3841 4.9316\n",
      "....Labels  5.3739 2.9694 5.7012 5.6991 5.0497\n",
      "..Epoch 85\n",
      "....Loss: 0.5395842252696949\n",
      "....Outputs 5.3804 4.0721 4.9213 5.2364 5.3804\n",
      "....Labels  5.6991 2.9694 5.0497 5.3739 5.7012\n",
      "..Epoch 86\n",
      "....Loss: 0.5299442175533777\n",
      "....Outputs 5.2323 5.3782 5.3782 4.9126 4.0455\n",
      "....Labels  5.3739 5.7012 5.6991 5.0497 2.9694\n",
      "..Epoch 87\n",
      "....Loss: 0.5203444634319783\n",
      "....Outputs 4.9057 5.3779 4.0204 5.3779 5.2302\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 88\n",
      "....Loss: 0.5107023322052716\n",
      "....Outputs 5.3797 3.9970 5.3797 4.9011 5.2302\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 89\n",
      "....Loss: 0.5009533317191649\n",
      "....Outputs 3.9753 5.3838 4.8988 5.3838 5.2325\n",
      "....Labels  2.9694 5.7012 5.0497 5.6991 5.3739\n",
      "..Epoch 90\n",
      "....Loss: 0.4910637727982867\n",
      "....Outputs 4.8987 5.3902 3.9553 5.3902 5.2371\n",
      "....Labels  5.0497 5.6991 2.9694 5.7012 5.3739\n",
      "..Epoch 91\n",
      "....Loss: 0.4810351777854053\n",
      "....Outputs 4.9007 5.2437 5.3985 5.3985 3.9368\n",
      "....Labels  5.0497 5.3739 5.6991 5.7012 2.9694\n",
      "..Epoch 92\n",
      "....Loss: 0.47089934315351106\n",
      "....Outputs 5.4087 5.4087 4.9045 3.9196 5.2522\n",
      "....Labels  5.7012 5.6991 5.0497 2.9694 5.3739\n",
      "..Epoch 93\n",
      "....Loss: 0.46070993592763615\n",
      "....Outputs 4.9098 3.9034 5.4202 5.2620 5.4202\n",
      "....Labels  5.0497 2.9694 5.6991 5.3739 5.7012\n",
      "..Epoch 94\n",
      "....Loss: 0.4505271102673805\n",
      "....Outputs 5.4327 4.9162 5.4327 3.8880 5.2728\n",
      "....Labels  5.6991 5.0497 5.7012 2.9694 5.3739\n",
      "..Epoch 95\n",
      "....Loss: 0.44040510696872376\n",
      "....Outputs 5.2842 5.4457 5.4457 3.8729 4.9232\n",
      "....Labels  5.3739 5.7012 5.6991 2.9694 5.0497\n",
      "..Epoch 96\n",
      "....Loss: 0.4303809206165061\n",
      "....Outputs 5.4588 3.8578 5.4588 4.9304 5.2956\n",
      "....Labels  5.7012 2.9694 5.6991 5.0497 5.3739\n",
      "..Epoch 97\n",
      "....Loss: 0.4204706731076811\n",
      "....Outputs 3.8425 5.4714 5.4714 5.3066 4.9373\n",
      "....Labels  2.9694 5.6991 5.7012 5.3739 5.0497\n",
      "..Epoch 98\n",
      "....Loss: 0.4106677544339954\n",
      "....Outputs 4.9437 3.8266 5.3170 5.4834 5.4834\n",
      "....Labels  5.0497 2.9694 5.3739 5.6991 5.7012\n",
      "..Epoch 99\n",
      "....Loss: 0.4009496926039271\n",
      "....Outputs 3.8100 5.4943 4.9492 5.4943 5.3262\n",
      "....Labels  2.9694 5.6991 5.0497 5.7012 5.3739\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3d84f2f421bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_test_overfit_small_batch=True, trainer=trainer)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_test_overfit_small_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_overfit_small_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mmin_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_class_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#re-instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mtest_overfit_small_batch\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise ValueError(f'''Error: Your model was not able to overfit a small batch \n\u001b[0;32m--> 191\u001b[0;31m                                of data. The minimum RMSE over {k.DL_DBG_OVERFIT_EPOCHS} epochs was not less than {self.EPSILON}''')\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Verified that a small batch can be overfit since the RMSE was less than {self.EPSILON}\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Your model was not able to overfit a small batch \n                               of data. The minimum RMSE over 100 epochs was not less than 0.22499709319526628"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chart Dependencies\n",
    "The `forward` method should not mix information from separate instances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graphnet.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image info](./images/graph_batch2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# data to illustrate the point\n",
    "\n",
    "np.random.seed(constants.RANDOM_SEED)\n",
    "polymer_indices = data_df.sample(n=4).index\n",
    "polymer_smiles = data_df.loc[polymer_indices, 'smiles'].values.tolist()\n",
    "polymer_smiles"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[*]C(C#N)=C([*])c1ccccc1',\n",
       " '[*]CCCCOC(=O)C(=O)O[*]',\n",
       " '[*]CC(CCl)(CCl)C(=O)O[*]',\n",
       " '[*]c1[nH]c([*])c(C(=O)O)c1C']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "feature_dict = {'C': np.array([1,0,0,0]),\n",
    "    'O': np.array([0,1,0,0]),\n",
    "    'N': np.array([0,0,1,0]),\n",
    "    'Cl': np.array([0,0,0,1])\n",
    "}\n",
    "N_FEATURES_ = len(feature_dict)\n",
    "N_DATA_ = len(polymer_smiles)\n",
    "MAX_N_ATOMS = max([Chem.MolFromSmiles(smile).GetNumAtoms() for smile in polymer_smiles])\n",
    "PROJECTOR_DIM = 100\n",
    "\n",
    "def featurize_smiles_by_atom(smile):\n",
    "    smile = smile.replace('*', 'H')\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    features = np.zeros((MAX_N_ATOMS, N_FEATURES_))\n",
    "    for ind,atom in enumerate(mol.GetAtoms()):\n",
    "        atom_feature = feature_dict[atom.GetSymbol()]\n",
    "        features[ind, :] = atom_feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# feature_array = np.zeros((N_DATA_, MAX_N_ATOMS, N_FEATURES_))\n",
    "labels = data_df.loc[polymer_indices, 'value'].values\n",
    "# for ind, smiles in enumerate(polymer_smiles):\n",
    "#     feature_array[ind, ].append(featurize_smiles_by_atom(smiles))\n",
    "\n",
    "train_X_ = [Data(x=tensor(featurize_smiles_by_atom(polymer_smiles[ind]), dtype=torch_float),\n",
    "                    y=tensor(labels[ind], dtype=torch_float)\n",
    "            ) \n",
    "            for ind in range(N_DATA_)\n",
    "]\n",
    "# for smiles,data in zip(polymer_smiles,train_X_):\n",
    "#     data.num_atoms = Chem.MolFromSmiles(smiles).GetNumAtoms()\n",
    "data_set_ = {'train': train_X_}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(GraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=2)\n",
    "        x = x - x_mean[:, :, None] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "correct_graphnet_class_ls = [lambda : GraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# this test should pass since we are using a bug-free model\n",
    "\n",
    "ds = dl_debug.DebugSession(correct_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs 0.5427 0.4155 0.3948 0.4339\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: 0.542738676071167\n",
      "Finished charting dependencies. Data is not getting mixed between instances in the same batch.\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# this is a buggy model. Can you spot the bugs?\n",
    "\n",
    "class BuggyGraphNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, capacity):\n",
    "\n",
    "        super(BuggyGraphNet,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_hidden = capacity\n",
    "        unit_sequence = utils.unit_sequence(self.input_dim, \n",
    "                                            self.output_dim, \n",
    "                                            self.n_hidden)\n",
    "        self.node_projector = nn.Linear(N_FEATURES_, PROJECTOR_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        # set up hidden layers\n",
    "        for ind,n_units in enumerate(unit_sequence[:-2]):\n",
    "            size_out_ = unit_sequence[ind+1]\n",
    "            layer = nn.Linear(n_units, size_out_)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # set up output layer\n",
    "        size_in_ = unit_sequence[-2]\n",
    "        size_out_ = unit_sequence[-1]\n",
    "        layer = nn.Linear(size_in_, size_out_)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.view(data.num_graphs, MAX_N_ATOMS, N_FEATURES_)\n",
    "        x = self.node_projector(x)\n",
    "        x_mean = x.mean(dim=0) # Spoiler! this is the bug.\n",
    "        x = x - x_mean[None, :, :] # make use of broadcasting\n",
    "        x = x.sum(dim=1)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i < (self.n_hidden - 1):\n",
    "                x = self.relu(x)\n",
    "   \n",
    "        return x.view(data.num_graphs,)\n",
    "\n",
    "# a list of models that are bug free!\n",
    "capacity_ls = [1,2,3]\n",
    "buggy_graphnet_class_ls = [lambda : BuggyGraphNet(PROJECTOR_DIM, 1, capacity) for capacity in\n",
    "                          capacity_ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# this test should not pass since we are using a buggy model\n",
    "\n",
    "ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_chart_dependencies=True)\n",
    "best_model_capacity = ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 4 points\n",
      "\n",
      "\n",
      "Beginning to chart dependencies\n",
      "..Epoch 0\n",
      "....Outputs -0.1925 -0.2155 -0.2169 -0.1876\n",
      "....Labels  4.3452 5.0922 6.5510 3.2017\n",
      "....Loss: -0.19253747165203094\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Data is getting mixed between instances in the same batch.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-64c781c65389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m ds = dl_debug.DebugSession(buggy_graphnet_class_ls, 'gnn', capacity_ls, data_set_, zero_data_set, loss_fn, epsilon,\n\u001b[1;32m      4\u001b[0m                  device, do_chart_dependencies=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest_model_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mmin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_chart_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_choose_model_size_by_overfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/rgur/nndebugger/nndebugger/dl_debug.py\u001b[0m in \u001b[0;36mchart_dependencies\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# print(data.x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is getting mixed between instances in the same batch.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished charting dependencies. Data is not getting mixed between instances in the same batch.\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data is getting mixed between instances in the same batch."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overfit training data\n",
    "The capacity of your architecture should be just large enough to overfit the training data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_choose_model_size_by_overfit=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data contains 676 points\n",
      "\n",
      "\n",
      "Beginning model size search\n",
      "\n",
      "..Training model 0 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.6845570345933005 [r2] -8.243965215860182\n",
      "......Outputs -0.0757 -0.0711 -0.0922 -0.0711 -0.0817 -0.0890 -0.1034 -0.0772 -0.0964 -0.0608\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.6845570345933005 [best r2] -8.243965215860182\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.703427690945832 [r2] -4.777354009391441\n",
      "......Outputs 0.4675 0.6711 0.5293 0.6711 0.6705 0.7734 0.4549 0.6955 0.4367 0.7988\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.703427690945832 [best r2] -4.777354009391441\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.698217714153011 [r2] -2.0667277229710237\n",
      "......Outputs 2.6638 4.0083 3.1470 4.0083 4.0328 4.9461 2.9680 4.0835 2.7473 4.5397\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.698217714153011 [best r2] -2.0667277229710237\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.9489320321445145 [r2] -0.5999802732468287\n",
      "......Outputs 3.9544 6.7532 4.2642 6.7532 6.9171 6.7893 5.2581 6.1751 3.9181 5.6868\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.9489320321445145 [best r2] -0.5999802732468287\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.8757723667891242 [r2] -0.48211365411107243\n",
      "......Outputs 2.5784 4.8481 2.3340 4.8481 5.1040 3.7966 4.1064 4.0009 2.3280 2.8413\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8757723667891242 [best r2] -0.48211365411107243\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.5445847000854198 [r2] -0.004950563613633996\n",
      "......Outputs 2.8823 5.8912 2.2228 5.8912 6.3155 3.7324 5.3146 4.4792 2.4479 2.5008\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.5445847000854198 [best r2] -0.004950563613633996\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.4389162670569364 [r2] 0.12784779007913383\n",
      "......Outputs 3.6652 7.5354 2.5545 7.5354 8.0742 4.2441 7.1954 5.6954 2.9727 2.7660\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4389162670569364 [best r2] 0.12784779007913383\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.2838965090181953 [r2] 0.3056454641632692\n",
      "......Outputs 3.4205 6.2434 2.2607 6.2434 6.7063 3.4103 6.5730 5.1041 2.5747 2.4487\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.2838965090181953 [best r2] 0.3056454641632692\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.1259953133255416 [r2] 0.4659346442926562\n",
      "......Outputs 3.7293 5.8775 2.4368 5.8775 6.2842 3.2993 6.7373 5.2874 2.6353 2.6539\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.1259953133255416 [best r2] 0.4659346442926562\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 1.0139184975379016 [r2] 0.5669606787852706\n",
      "......Outputs 4.5170 6.2031 2.9972 6.2031 6.6195 3.6742 7.5952 6.0318 2.9867 3.2446\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0139184975379016 [best r2] 0.5669606787852706\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.8956844379692688 [r2] 0.6620664596822177\n",
      "......Outputs 4.5777 5.5826 3.0096 5.5826 5.8905 3.2866 7.2535 5.5697 2.7062 3.1868\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.8956844379692688 [best r2] 0.6620664596822177\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7909391486611757 [r2] 0.7364837600229734\n",
      "......Outputs 4.9312 5.6097 3.2629 5.6097 5.8474 3.1845 7.4783 5.5083 2.6203 3.2379\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7909391486611757 [best r2] 0.7364837600229734\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.7054411578289802 [r2] 0.7903751188224846\n",
      "......Outputs 5.1836 5.7225 3.4431 5.7225 5.8678 3.0747 7.6029 5.4283 2.4978 3.1832\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7054411578289802 [best r2] 0.7903751188224846\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.6327048345733691 [r2] 0.8313743849229869\n",
      "......Outputs 5.1586 5.5782 3.4412 5.5782 5.6028 2.8542 7.2873 5.1834 2.2928 3.0146\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6327048345733691 [best r2] 0.8313743849229869\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5700043355479001 [r2] 0.8631396775513933\n",
      "......Outputs 5.3103 5.7510 3.5616 5.7510 5.7240 2.8337 7.2897 5.2598 2.2548 3.0607\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5700043355479001 [best r2] 0.8631396775513933\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.5119058848957166 [r2] 0.8896171799019221\n",
      "......Outputs 5.1742 5.6609 3.3747 5.6609 5.6643 2.7020 7.0132 5.1056 2.1358 3.0302\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5119058848957166 [best r2] 0.8896171799019221\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.46180721215155335 [r2] 0.9101656017406775\n",
      "......Outputs 5.1687 5.7300 3.3161 5.7300 5.8336 2.6908 6.9537 5.1209 2.0995 3.1568\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.46180721215155335 [best r2] 0.9101656017406775\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.4171494430240576 [r2] 0.9266998983375592\n",
      "......Outputs 5.0511 5.6298 3.1888 5.6298 5.7756 2.6277 6.7423 5.0566 2.0313 3.1956\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4171494430240576 [best r2] 0.9266998983375592\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.3780174399644613 [r2] 0.9398071489719942\n",
      "......Outputs 5.0290 5.6826 3.1376 5.6826 5.7967 2.6362 6.6903 5.1035 2.0077 3.2232\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3780174399644613 [best r2] 0.9398071489719942\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.3433685601699893 [r2] 0.9503359313435378\n",
      "......Outputs 4.9664 5.6606 3.0611 5.6606 5.6847 2.6213 6.6084 5.0929 1.9662 3.1714\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3433685601699893 [best r2] 0.9503359313435378\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.313425326032217 [r2] 0.9586201022227775\n",
      "......Outputs 4.9807 5.7004 3.0349 5.7004 5.6543 2.6522 6.6239 5.1347 1.9571 3.1589\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.313425326032217 [best r2] 0.9586201022227775\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2871344977540906 [r2] 0.9652710231545433\n",
      "......Outputs 4.9758 5.6932 2.9913 5.6932 5.5807 2.6740 6.6102 5.1527 1.9470 3.1487\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2871344977540906 [best r2] 0.9652710231545433\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.26515604675459487 [r2] 0.9703841424999329\n",
      "......Outputs 4.9929 5.7161 2.9562 5.7161 5.5680 2.7092 6.6302 5.1926 1.9509 3.1646\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.26515604675459487 [best r2] 0.9703841424999329\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.24574180244060617 [r2] 0.9745622134033368\n",
      "......Outputs 5.0100 5.7135 2.9229 5.7135 5.5207 2.7597 6.6444 5.2186 1.9656 3.1667\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.24574180244060617 [best r2] 0.9745622134033368\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.22915678683285484 [r2] 0.9778799202185727\n",
      "......Outputs 5.0152 5.7133 2.9022 5.7133 5.4625 2.7981 6.6511 5.2324 1.9789 3.1521\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.22915678683285484 [best r2] 0.9778799202185727\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.21534669097351336 [r2] 0.9804657092997662\n",
      "......Outputs 5.0279 5.7370 2.9034 5.7370 5.4527 2.8411 6.6804 5.2504 1.9983 3.1535\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.21534669097351336 [best r2] 0.9804657092997662\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.20276995166320205 [r2] 0.9826807757249526\n",
      "......Outputs 5.0347 5.7185 2.9061 5.7185 5.4148 2.8714 6.6786 5.2456 2.0159 3.1482\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20276995166320205 [best r2] 0.9826807757249526\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.19214074710713422 [r2] 0.9844489330539686\n",
      "......Outputs 5.0379 5.7254 2.9090 5.7254 5.4048 2.8783 6.6913 5.2438 2.0305 3.1427\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19214074710713422 [best r2] 0.9844489330539686\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1826450778663754 [r2] 0.9859480308865185\n",
      "......Outputs 5.0411 5.7346 2.9164 5.7346 5.3988 2.8766 6.7033 5.2499 2.0442 3.1442\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1826450778663754 [best r2] 0.9859480308865185\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.17382556427388365 [r2] 0.9872723408400608\n",
      "......Outputs 5.0321 5.7219 2.9133 5.7219 5.3832 2.8690 6.7042 5.2434 2.0524 3.1349\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17382556427388365 [best r2] 0.9872723408400608\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.1658705470109092 [r2] 0.9884106308977822\n",
      "......Outputs 5.0281 5.7296 2.9180 5.7296 5.3846 2.8720 6.7089 5.2396 2.0678 3.1266\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1658705470109092 [best r2] 0.9884106308977822\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.1585836276011705 [r2] 0.989406537413434\n",
      "......Outputs 5.0287 5.7374 2.9243 5.7374 5.3790 2.8716 6.7154 5.2364 2.0803 3.1215\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1585836276011705 [best r2] 0.989406537413434\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.15182045752095885 [r2] 0.9902908360966112\n",
      "......Outputs 5.0249 5.7328 2.9255 5.7328 5.3774 2.8685 6.7116 5.2255 2.0870 3.1165\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.15182045752095885 [best r2] 0.9902908360966112\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.14548420434454376 [r2] 0.9910843517201513\n",
      "......Outputs 5.0227 5.7314 2.9266 5.7314 5.3797 2.8705 6.7070 5.2132 2.0914 3.1132\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14548420434454376 [best r2] 0.9910843517201513\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.1396299068013167 [r2] 0.991787447910577\n",
      "......Outputs 5.0306 5.7415 2.9338 5.7415 5.3822 2.8743 6.7128 5.2081 2.0989 3.1151\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1396299068013167 [best r2] 0.991787447910577\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.13408038872515451 [r2] 0.992427282397552\n",
      "......Outputs 5.0358 5.7334 2.9381 5.7334 5.3782 2.8703 6.7094 5.2000 2.1051 3.1110\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.13408038872515451 [best r2] 0.992427282397552\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.1287719400290195 [r2] 0.9930150433009388\n",
      "......Outputs 5.0324 5.7290 2.9368 5.7290 5.3712 2.8603 6.7010 5.1882 2.1058 3.1045\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1287719400290195 [best r2] 0.9930150433009388\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.12379771512578042 [r2] 0.993544253042046\n",
      "......Outputs 5.0366 5.7384 2.9423 5.7384 5.3765 2.8652 6.7071 5.1830 2.1132 3.1080\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.12379771512578042 [best r2] 0.993544253042046\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.11923944475306684 [r2] 0.9940109059795214\n",
      "......Outputs 5.0456 5.7416 2.9531 5.7416 5.3906 2.8701 6.7176 5.1844 2.1262 3.1105\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11923944475306684 [best r2] 0.9940109059795214\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.11469890198295522 [r2] 0.9944583414688745\n",
      "......Outputs 5.0379 5.7234 2.9526 5.7234 5.3858 2.8565 6.7052 5.1750 2.1245 3.1027\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11469890198295522 [best r2] 0.9944583414688745\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.11051552512841076 [r2] 0.9948552079658993\n",
      "......Outputs 5.0269 5.7194 2.9432 5.7194 5.3668 2.8550 6.6941 5.1616 2.1150 3.1011\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11051552512841076 [best r2] 0.9948552079658993\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.10650584134344097 [r2] 0.995221758501841\n",
      "......Outputs 5.0390 5.7475 2.9498 5.7475 5.3773 2.8736 6.7119 5.1661 2.1222 3.1075\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10650584134344097 [best r2] 0.995221758501841\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.10374156788505302 [r2] 0.9954665706121656\n",
      "......Outputs 5.0643 5.7594 2.9747 5.7594 5.4107 2.8848 6.7267 5.1809 2.1395 3.1110\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10374156788505302 [best r2] 0.9954665706121656\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.10050796593552622 [r2] 0.9957447781372765\n",
      "......Outputs 5.0462 5.7200 2.9690 5.7200 5.3959 2.8580 6.6943 5.1620 2.1283 3.1017\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10050796593552622 [best r2] 0.9957447781372765\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.100102344871024 [r2] 0.9957790545206852\n",
      "......Outputs 5.0036 5.6891 2.9319 5.6891 5.3286 2.8331 6.6645 5.1271 2.0946 3.0938\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.100102344871024 [best r2] 0.9957790545206852\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.09928089623061923 [r2] 0.9958480451819272\n",
      "......Outputs 5.0256 5.7487 2.9403 5.7487 5.3434 2.8698 6.7148 5.1495 2.1074 3.1038\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.10944152895437696 [r2] 0.9949547168531037\n",
      "......Outputs 5.1221 5.8347 3.0206 5.8347 5.4777 2.9290 6.7959 5.2248 2.1735 3.1273\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.11579828183436483 [r2] 0.9943515996404425\n",
      "......Outputs 5.0879 5.7219 3.0267 5.7219 5.4562 2.8684 6.7006 5.1918 2.1574 3.1134\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.1395388467150378 [r2] 0.9917981561014976\n",
      "......Outputs 4.9069 5.5651 2.8823 5.5651 5.2285 2.7679 6.5455 5.0404 2.0284 3.0690\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.15438866225625258 [r2] 0.9899595760914046\n",
      "......Outputs 4.9421 5.6954 2.8771 5.6954 5.2332 2.8384 6.6960 5.0907 2.0339 3.0994\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.19097472960326847 [r2] 0.9846371054900637\n",
      "......Outputs 5.2625 6.0159 3.0931 6.0159 5.6207 3.0500 7.0274 5.3753 2.2208 3.1840\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.17993084965724873 [r2] 0.9863625710051388\n",
      "......Outputs 5.1490 5.7224 3.1053 5.7224 5.4897 2.8807 6.6467 5.2408 2.1934 3.0820\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.20486859225382437 [r2] 0.9823204174071063\n",
      "......Outputs 4.7681 5.4623 2.8366 5.4623 5.1452 2.6430 6.3384 4.8935 1.9513 3.0148\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.16631220434876237 [r2] 0.9883488315674361\n",
      "......Outputs 4.9977 5.8620 2.9509 5.8620 5.3512 2.8835 6.8187 5.1579 2.0568 3.2207\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.1630001344905288 [r2] 0.9888082714926821\n",
      "......Outputs 5.2469 5.9085 3.1132 5.9085 5.5528 3.0255 6.9905 5.3778 2.2016 3.1564\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.12824091497752974 [r2] 0.9930725331404895\n",
      "......Outputs 4.9983 5.5027 2.9903 5.5027 5.3094 2.7847 6.5016 5.0863 2.0801 2.9782\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.11107583684972169 [r2] 0.9948029077146298\n",
      "......Outputs 4.9577 5.7172 2.9358 5.7172 5.3604 2.7761 6.5958 5.0519 2.0428 3.1087\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.1020502985137387 [r2] 0.9956131801520428\n",
      "......Outputs 5.1150 5.9304 3.0241 5.9304 5.4850 2.9470 6.8837 5.2268 2.1430 3.2223\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09928089623061923 [best r2] 0.9958480451819272\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.08054360664996986 [r2] 0.9972673539248402\n",
      "......Outputs 5.0505 5.6647 2.9985 5.6647 5.3452 2.8908 6.6918 5.1618 2.1328 3.0657\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08054360664996986 [best r2] 0.9972673539248402\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.07724494931780768 [r2] 0.9974866010691901\n",
      "......Outputs 4.9934 5.6437 2.9417 5.6437 5.3200 2.8056 6.6083 5.0975 2.0825 3.0470\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07724494931780768 [best r2] 0.9974866010691901\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.06872635158164431 [r2] 0.9980103906194694\n",
      "......Outputs 5.0772 5.8087 2.9836 5.8087 5.4044 2.8720 6.7506 5.1745 2.1180 3.1699\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06872635158164431 [best r2] 0.9980103906194694\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.06527228285937499 [r2] 0.998205353793893\n",
      "......Outputs 5.0796 5.7617 2.9932 5.7617 5.4005 2.8944 6.7431 5.1802 2.1377 3.1285\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06527228285937499 [best r2] 0.998205353793893\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.061134613018461244 [r2] 0.9984256707087222\n",
      "......Outputs 5.0174 5.6843 2.9587 5.6843 5.3570 2.8475 6.6604 5.1299 2.1112 3.0625\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.061134613018461244 [best r2] 0.9984256707087222\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.05849968243627847 [r2] 0.9985584548212535\n",
      "......Outputs 5.0319 5.7379 2.9646 5.7379 5.3600 2.8568 6.6944 5.1469 2.1119 3.1188\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05849968243627847 [best r2] 0.9985584548212535\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.05793864703733756 [r2] 0.9985859722259594\n",
      "......Outputs 5.0696 5.7733 2.9828 5.7733 5.3886 2.8859 6.7435 5.1780 2.1298 3.1388\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05793864703733756 [best r2] 0.9985859722259594\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.05546618362891353 [r2] 0.9987040811377832\n",
      "......Outputs 5.0536 5.7233 2.9777 5.7233 5.3810 2.8729 6.7021 5.1595 2.1264 3.0954\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05546618362891353 [best r2] 0.9987040811377832\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.05439006051053465 [r2] 0.9987538786846748\n",
      "......Outputs 5.0268 5.7083 2.9631 5.7083 5.3613 2.8511 6.6747 5.1352 2.1109 3.0943\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05439006051053465 [best r2] 0.9987538786846748\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.05316660333026334 [r2] 0.9988093089985656\n",
      "......Outputs 5.0450 5.7527 2.9695 5.7527 5.3762 2.8658 6.7170 5.1548 2.1164 3.1234\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05316660333026334 [best r2] 0.9988093089985656\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.05355065661815569 [r2] 0.9987920447613703\n",
      "......Outputs 5.0687 5.7575 2.9854 5.7575 5.3938 2.8832 6.7325 5.1761 2.1304 3.1183\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05316660333026334 [best r2] 0.9988093089985656\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.05166956355952907 [r2] 0.9988754187786485\n",
      "......Outputs 5.0466 5.7194 2.9761 5.7194 5.3742 2.8637 6.6892 5.1521 2.1216 3.0973\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05166956355952907 [best r2] 0.9988754187786485\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.05136736272999475 [r2] 0.9988885350317243\n",
      "......Outputs 5.0243 5.7169 2.9606 5.7169 5.3558 2.8503 6.6775 5.1324 2.1069 3.1056\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05136736272999475 [best r2] 0.9988885350317243\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.05088899004419545 [r2] 0.9989091402829346\n",
      "......Outputs 5.0506 5.7552 2.9717 5.7552 5.3799 2.8737 6.7236 5.1611 2.1178 3.1209\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05088899004419545 [best r2] 0.9989091402829346\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.05263305666329259 [r2] 0.9988330871409958\n",
      "......Outputs 5.0766 5.7579 2.9897 5.7579 5.4022 2.8870 6.7351 5.1827 2.1342 3.1140\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05088899004419545 [best r2] 0.9989091402829346\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.050857429437623815 [r2] 0.9989104929338084\n",
      "......Outputs 5.0438 5.7142 2.9758 5.7142 5.3711 2.8585 6.6808 5.1475 2.1191 3.1013\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.052442000619234985 [r2] 0.9988415434658342\n",
      "......Outputs 5.0106 5.7085 2.9529 5.7085 5.3431 2.8428 6.6662 5.1219 2.0990 3.1061\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.052697833518717235 [r2] 0.9988302130742027\n",
      "......Outputs 5.0527 5.7623 2.9712 5.7623 5.3810 2.8782 6.7332 5.1676 2.1174 3.1199\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.0591201408164533 [r2] 0.9985277140741554\n",
      "......Outputs 5.0997 5.7758 3.0034 5.7758 5.4233 2.9001 6.7562 5.2039 2.1457 3.1181\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.057430194381592456 [r2] 0.9986106815174236\n",
      "......Outputs 5.0437 5.7058 2.9806 5.7058 5.3734 2.8541 6.6666 5.1431 2.1218 3.1007\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.06658910179096719 [r2] 0.9981322121133995\n",
      "......Outputs 4.9732 5.6755 2.9325 5.6755 5.3109 2.8201 6.6263 5.0881 2.0809 3.0978\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.06808658237752482 [r2] 0.9980472604998349\n",
      "......Outputs 5.0459 5.7718 2.9591 5.7718 5.3654 2.8798 6.7459 5.1683 2.1092 3.1197\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.09056116261749066 [r2] 0.9965453407317875\n",
      "......Outputs 5.1626 5.8325 3.0349 5.8325 5.4747 2.9391 6.8245 5.2627 2.1751 3.1316\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.09112598931020703 [r2] 0.9965021131769402\n",
      "......Outputs 5.0665 5.6978 3.0054 5.6978 5.4066 2.8590 6.6555 5.1538 2.1431 3.1014\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.12358196805460003 [r2] 0.9935667347953868\n",
      "......Outputs 4.8753 5.5789 2.8856 5.5789 5.2373 2.7579 6.5131 4.9963 2.0355 3.0738\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.13184827832908075 [r2] 0.9926773181242025\n",
      "......Outputs 4.9831 5.7597 2.9067 5.7597 5.2937 2.8562 6.7315 5.1288 2.0651 3.1101\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.18733688179777 [r2] 0.9852168216914649\n",
      "......Outputs 5.3060 5.9905 3.0970 5.9905 5.5868 3.0362 7.0136 5.4072 2.2361 3.1658\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.19615121254804224 [r2] 0.9837929774950275\n",
      "......Outputs 5.1738 5.7256 3.0950 5.7256 5.5184 2.9065 6.6901 5.2337 2.2250 3.1087\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.25476587050505406 [r2] 0.9726596710088948\n",
      "......Outputs 4.6858 5.3675 2.8163 5.3675 5.1039 2.6265 6.2465 4.8001 1.9589 3.0105\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.2602927093650666 [r2] 0.9714605730341679\n",
      "......Outputs 4.8065 5.6905 2.8104 5.6905 5.1551 2.7832 6.6320 5.0013 1.9660 3.1088\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.31507763002028943 [r2] 0.9581826621988703\n",
      "......Outputs 5.4553 6.2209 3.1505 6.2209 5.7094 3.1857 7.3344 5.6050 2.2842 3.2397\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.2712530311228583 [r2] 0.9690065129764305\n",
      "......Outputs 5.2616 5.6545 3.1565 5.6545 5.5348 2.9492 6.6903 5.2982 2.2671 3.0489\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.2957413473618585 [r2] 0.9631578187199512\n",
      "......Outputs 4.6203 5.2838 2.7813 5.2838 5.0730 2.5231 6.0779 4.6891 1.9468 2.9038\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.2222278508168467 [r2] 0.9791973715056823\n",
      "......Outputs 4.8849 5.9097 2.8743 5.9097 5.3456 2.8140 6.7403 5.0665 2.0299 3.2237\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.21853802969643912 [r2] 0.9798824408308834\n",
      "......Outputs 5.3189 6.0663 3.1095 6.0663 5.6263 3.1248 7.1887 5.4791 2.2463 3.2807\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.14550783875180562 [r2] 0.9910814547296612\n",
      "......Outputs 5.0563 5.4398 2.9908 5.4398 5.2892 2.8437 6.5301 5.1180 2.1330 2.9586\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.13169424252868328 [r2] 0.9926944180256035\n",
      "......Outputs 4.9238 5.5774 2.8971 5.5774 5.2612 2.7175 6.4459 5.0035 2.0483 2.9977\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.10167548040953744 [r2] 0.995645345465463\n",
      "......Outputs 5.1084 5.9940 2.9936 5.9940 5.4905 2.9054 6.8692 5.2382 2.1442 3.2443\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.07861225604670456 [r2] 0.9973968346137612\n",
      "......Outputs 5.0876 5.7502 3.0063 5.7502 5.4051 2.9290 6.7768 5.2058 2.1581 3.1607\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.0674992856656597 [r2] 0.9980808028305104\n",
      "......Outputs 4.9821 5.5821 2.9344 5.5821 5.3106 2.8226 6.5881 5.0927 2.0926 3.0125\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.05618195585771226 [r2] 0.9986704185440708\n",
      "......Outputs 5.0502 5.7699 2.9555 5.7699 5.3743 2.8463 6.7064 5.1680 2.1061 3.0923\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.05436393646780968 [r2] 0.9987550754440919\n",
      "......Outputs 5.0978 5.8036 2.9888 5.8036 5.3894 2.8910 6.7644 5.2002 2.1345 3.1636\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.050857429437623815 [best r2] 0.9989104929338084\n",
      "..Plotting of gradients skipped\n",
      "\n",
      "..Training model 1 \n",
      "\n",
      "\n",
      "....Epoch 0\n",
      "......[rmse] 4.550741924263732 [r2] -7.723397349961028\n",
      "......Outputs 0.0562 0.0236 0.0237 0.0236 0.0437 0.0593 0.0226 0.0316 0.0380 0.0544\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 4.550741924263732 [best r2] -7.723397349961028\n",
      "\n",
      "....Epoch 1\n",
      "......[rmse] 3.52388276565166 [r2] -4.230752173320372\n",
      "......Outputs 0.6417 0.8080 0.6882 0.8080 0.8695 1.1474 0.7050 0.8861 0.7025 0.9973\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 3.52388276565166 [best r2] -4.230752173320372\n",
      "\n",
      "....Epoch 2\n",
      "......[rmse] 2.609765616780425 [r2] -1.8689584001118158\n",
      "......Outputs 2.9250 4.2884 3.3625 4.2884 4.4261 5.7652 3.3820 4.3962 3.2311 4.9305\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 2.609765616780425 [best r2] -1.8689584001118158\n",
      "\n",
      "....Epoch 3\n",
      "......[rmse] 1.8266932521802997 [r2] -0.40557003462871255\n",
      "......Outputs 3.8120 6.5665 3.9242 6.5665 6.8799 6.8005 5.3476 5.8449 3.9501 5.3016\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.8266932521802997 [best r2] -0.40557003462871255\n",
      "\n",
      "....Epoch 4\n",
      "......[rmse] 1.751707519353906 [r2] -0.29254130151787905\n",
      "......Outputs 2.5631 4.8529 2.1897 4.8529 5.2004 3.9381 4.3362 3.8429 2.4357 2.7169\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.751707519353906 [best r2] -0.29254130151787905\n",
      "\n",
      "....Epoch 5\n",
      "......[rmse] 1.4575808524706013 [r2] 0.10507518632493362\n",
      "......Outputs 2.9312 6.2100 2.1721 6.2100 6.6949 3.9970 5.8046 4.4319 2.5953 2.4942\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.4575808524706013 [best r2] 0.10507518632493362\n",
      "\n",
      "....Epoch 6\n",
      "......[rmse] 1.3283610052976347 [r2] 0.256718236898274\n",
      "......Outputs 3.4933 7.3739 2.3987 7.3739 7.8962 4.2982 7.3269 5.3103 2.9348 2.7200\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.3283610052976347 [best r2] 0.256718236898274\n",
      "\n",
      "....Epoch 7\n",
      "......[rmse] 1.1756015180452266 [r2] 0.4178411322083523\n",
      "......Outputs 3.2655 5.9087 2.1782 5.9087 6.2961 3.5414 6.5612 4.7523 2.5663 2.5294\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.1756015180452266 [best r2] 0.4178411322083523\n",
      "\n",
      "....Epoch 8\n",
      "......[rmse] 1.0216190749596068 [r2] 0.5603579470031308\n",
      "......Outputs 3.7932 5.8156 2.5538 5.8156 6.1804 3.6887 7.0632 5.2225 2.7716 3.0209\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 1.0216190749596068 [best r2] 0.5603579470031308\n",
      "\n",
      "....Epoch 9\n",
      "......[rmse] 0.9058573413479803 [r2] 0.6543465805860542\n",
      "......Outputs 4.3740 5.8792 2.9503 5.8792 6.1858 3.7903 7.5700 5.6289 2.9226 3.4431\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.9058573413479803 [best r2] 0.6543465805860542\n",
      "\n",
      "....Epoch 10\n",
      "......[rmse] 0.793054415298701 [r2] 0.7350723936259616\n",
      "......Outputs 4.3618 5.3801 2.8691 5.3801 5.5691 3.3410 7.1529 5.2019 2.6237 3.2065\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.793054415298701 [best r2] 0.7350723936259616\n",
      "\n",
      "....Epoch 11\n",
      "......[rmse] 0.7077217294381449 [r2] 0.7890175646223094\n",
      "......Outputs 4.8565 5.7476 3.1551 5.7476 5.8414 3.2987 7.5526 5.3475 2.6193 3.2479\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.7077217294381449 [best r2] 0.7890175646223094\n",
      "\n",
      "....Epoch 12\n",
      "......[rmse] 0.6260605157605597 [r2] 0.8348974160759326\n",
      "......Outputs 4.8662 5.5818 3.1762 5.5818 5.4869 3.0480 7.1577 5.0258 2.4133 3.0650\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.6260605157605597 [best r2] 0.8348974160759326\n",
      "\n",
      "....Epoch 13\n",
      "......[rmse] 0.562457669182885 [r2] 0.8667396570805616\n",
      "......Outputs 5.1237 5.8293 3.4209 5.8293 5.5757 3.0188 7.1811 5.0948 2.3808 3.1031\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.562457669182885 [best r2] 0.8667396570805616\n",
      "\n",
      "....Epoch 14\n",
      "......[rmse] 0.5017057993005302 [r2] 0.8939722651976963\n",
      "......Outputs 5.0741 5.7465 3.3554 5.7465 5.5192 2.8887 6.9525 4.9981 2.2646 3.0137\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.5017057993005302 [best r2] 0.8939722651976963\n",
      "\n",
      "....Epoch 15\n",
      "......[rmse] 0.4508271202653358 [r2] 0.9143866859652577\n",
      "......Outputs 5.0956 5.8071 3.2885 5.8071 5.7212 2.8377 6.9122 5.0512 2.2186 3.0223\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4508271202653358 [best r2] 0.9143866859652577\n",
      "\n",
      "....Epoch 16\n",
      "......[rmse] 0.4052576139248748 [r2] 0.9308195142631137\n",
      "......Outputs 4.9973 5.6557 3.1510 5.6557 5.7266 2.7768 6.7376 5.0204 2.1393 3.0546\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.4052576139248748 [best r2] 0.9308195142631137\n",
      "\n",
      "....Epoch 17\n",
      "......[rmse] 0.36616596770345144 [r2] 0.9435222748040948\n",
      "......Outputs 5.0014 5.6867 3.1026 5.6867 5.7853 2.7624 6.7041 5.1109 2.1046 3.1569\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.36616596770345144 [best r2] 0.9435222748040948\n",
      "\n",
      "....Epoch 18\n",
      "......[rmse] 0.3318003084434598 [r2] 0.9536259728761759\n",
      "......Outputs 4.9540 5.6638 3.0143 5.6638 5.6852 2.7039 6.6093 5.1134 2.0442 3.1415\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.3318003084434598 [best r2] 0.9536259728761759\n",
      "\n",
      "....Epoch 19\n",
      "......[rmse] 0.30236261498720834 [r2] 0.9614896534991769\n",
      "......Outputs 4.9965 5.7317 3.0050 5.7317 5.6397 2.6832 6.6027 5.1913 2.0166 3.1591\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.30236261498720834 [best r2] 0.9614896534991769\n",
      "\n",
      "....Epoch 20\n",
      "......[rmse] 0.27651025615871144 [r2] 0.9677934852451284\n",
      "......Outputs 5.0033 5.7077 2.9737 5.7077 5.5726 2.6583 6.5695 5.1847 1.9925 3.1408\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.27651025615871144 [best r2] 0.9677934852451284\n",
      "\n",
      "....Epoch 21\n",
      "......[rmse] 0.2550032638457556 [r2] 0.9726086952958839\n",
      "......Outputs 5.0001 5.7088 2.9495 5.7088 5.5393 2.6475 6.5728 5.1833 1.9801 3.1553\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.2550032638457556 [best r2] 0.9726086952958839\n",
      "\n",
      "....Epoch 22\n",
      "......[rmse] 0.23691241000298513 [r2] 0.9763573113990355\n",
      "......Outputs 5.0151 5.7417 2.9378 5.7417 5.5287 2.6552 6.6073 5.1847 1.9809 3.1966\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.23691241000298513 [best r2] 0.9763573113990355\n",
      "\n",
      "....Epoch 23\n",
      "......[rmse] 0.22114455826290408 [r2] 0.9793996900364472\n",
      "......Outputs 5.0175 5.7202 2.9232 5.7202 5.5138 2.6744 6.6249 5.1675 1.9825 3.2021\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.22114455826290408 [best r2] 0.9793996900364472\n",
      "\n",
      "....Epoch 24\n",
      "......[rmse] 0.20766408629836414 [r2] 0.9818346390420128\n",
      "......Outputs 5.0176 5.7193 2.9156 5.7193 5.4949 2.7011 6.6444 5.1475 1.9882 3.1892\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.20766408629836414 [best r2] 0.9818346390420128\n",
      "\n",
      "....Epoch 25\n",
      "......[rmse] 0.19625130132277835 [r2] 0.9837764335760466\n",
      "......Outputs 5.0321 5.7497 2.9303 5.7497 5.4745 2.7362 6.6761 5.1487 2.0016 3.1916\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.19625130132277835 [best r2] 0.9837764335760466\n",
      "\n",
      "....Epoch 26\n",
      "......[rmse] 0.18616966396429768 [r2] 0.9854004634202539\n",
      "......Outputs 5.0484 5.7420 2.9458 5.7420 5.4557 2.7704 6.7037 5.1516 2.0248 3.1869\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.18616966396429768 [best r2] 0.9854004634202539\n",
      "\n",
      "....Epoch 27\n",
      "......[rmse] 0.17679804750147304 [r2] 0.9868333233598519\n",
      "......Outputs 5.0432 5.7121 2.9420 5.7121 5.4305 2.7952 6.7084 5.1419 2.0424 3.1775\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.17679804750147304 [best r2] 0.9868333233598519\n",
      "\n",
      "....Epoch 28\n",
      "......[rmse] 0.1684896975154617 [r2] 0.9880417413668862\n",
      "......Outputs 5.0333 5.7262 2.9282 5.7262 5.4101 2.8113 6.7072 5.1263 2.0576 3.1784\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1684896975154617 [best r2] 0.9880417413668862\n",
      "\n",
      "....Epoch 29\n",
      "......[rmse] 0.16103647643485314 [r2] 0.989076300122182\n",
      "......Outputs 5.0448 5.7491 2.9365 5.7491 5.4175 2.8302 6.7210 5.1425 2.0778 3.1688\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.16103647643485314 [best r2] 0.989076300122182\n",
      "\n",
      "....Epoch 30\n",
      "......[rmse] 0.1545721936946071 [r2] 0.9899356905456552\n",
      "......Outputs 5.0684 5.7592 2.9622 5.7592 5.4319 2.8565 6.7405 5.1714 2.1016 3.1711\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1545721936946071 [best r2] 0.9899356905456552\n",
      "\n",
      "....Epoch 31\n",
      "......[rmse] 0.14902826097824187 [r2] 0.9906446829938613\n",
      "......Outputs 5.0677 5.7326 2.9649 5.7326 5.4284 2.8660 6.7366 5.1586 2.1135 3.1726\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14902826097824187 [best r2] 0.9906446829938613\n",
      "\n",
      "....Epoch 32\n",
      "......[rmse] 0.14374983922707874 [r2] 0.991295657404514\n",
      "......Outputs 5.0233 5.6839 2.9308 5.6839 5.3696 2.8477 6.6950 5.1018 2.1009 3.1494\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14374983922707874 [best r2] 0.991295657404514\n",
      "\n",
      "....Epoch 33\n",
      "......[rmse] 0.14360829218859048 [r2] 0.9913127908820344\n",
      "......Outputs 4.9895 5.7056 2.8990 5.7056 5.3226 2.8342 6.6657 5.0825 2.0834 3.1197\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 34\n",
      "......[rmse] 0.14394657071996025 [r2] 0.9912718161243651\n",
      "......Outputs 5.0463 5.8222 2.9544 5.8222 5.3807 2.8654 6.7296 5.1931 2.1141 3.1449\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 35\n",
      "......[rmse] 0.16627481909668163 [r2] 0.9883540691014278\n",
      "......Outputs 5.1642 5.8658 3.0779 5.8658 5.5427 2.9259 6.8503 5.3281 2.1915 3.2039\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 36\n",
      "......[rmse] 0.1750356950027506 [r2] 0.9870945103300116\n",
      "......Outputs 5.0817 5.6123 3.0250 5.6123 5.4538 2.8824 6.7409 5.1355 2.1694 3.1776\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 37\n",
      "......[rmse] 0.21693164714250876 [r2] 0.9801771055492564\n",
      "......Outputs 4.8058 5.4484 2.7674 5.4484 5.1755 2.7516 6.4730 4.7981 2.0139 3.0671\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 38\n",
      "......[rmse] 0.21884772752460674 [r2] 0.979825381846466\n",
      "......Outputs 4.9116 5.8102 2.8164 5.8102 5.2459 2.8028 6.6541 5.0407 2.0243 3.1180\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 39\n",
      "......[rmse] 0.25566688515710917 [r2] 0.9724659433498529\n",
      "......Outputs 5.3549 6.1821 3.1633 6.1821 5.7118 3.0164 7.1308 5.5574 2.2334 3.2582\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 40\n",
      "......[rmse] 0.19902507957869578 [r2] 0.9833145911298699\n",
      "......Outputs 5.1022 5.5057 3.0692 5.5057 5.4382 2.8836 6.6613 5.1666 2.1964 3.0856\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 41\n",
      "......[rmse] 0.19817364930401235 [r2] 0.9834570462880802\n",
      "......Outputs 4.7797 5.4865 2.7892 5.4865 5.2314 2.7299 6.4038 4.8300 2.0393 3.0326\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 42\n",
      "......[rmse] 0.15303088962645206 [r2] 0.9901354007472031\n",
      "......Outputs 5.0746 6.0476 2.9487 6.0476 5.5399 2.8958 6.8663 5.2309 2.1168 3.2727\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.14360829218859048 [best r2] 0.9913127908820344\n",
      "\n",
      "....Epoch 43\n",
      "......[rmse] 0.1325432981315941 [r2] 0.9925999136555158\n",
      "......Outputs 5.1576 5.6998 3.0463 5.6998 5.4713 2.9363 6.8249 5.2650 2.1537 3.1756\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.1325432981315941 [best r2] 0.9925999136555158\n",
      "\n",
      "....Epoch 44\n",
      "......[rmse] 0.11583616769065659 [r2] 0.9943479030486223\n",
      "......Outputs 4.9694 5.4986 2.9154 5.4986 5.3139 2.8249 6.5802 5.0752 2.0909 3.0138\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.11583616769065659 [best r2] 0.9943479030486223\n",
      "\n",
      "....Epoch 45\n",
      "......[rmse] 0.10187942894024832 [r2] 0.9956278581395956\n",
      "......Outputs 5.0387 5.8665 2.9498 5.8665 5.4250 2.8495 6.7238 5.1832 2.1174 3.1497\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.10187942894024832 [best r2] 0.9956278581395956\n",
      "\n",
      "....Epoch 46\n",
      "......[rmse] 0.09582881776171201 [r2] 0.9961317591985529\n",
      "......Outputs 5.0947 5.8278 3.0028 5.8278 5.4323 2.8790 6.7650 5.1899 2.1384 3.2079\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.09582881776171201 [best r2] 0.9961317591985529\n",
      "\n",
      "....Epoch 47\n",
      "......[rmse] 0.08884330655169304 [r2] 0.9966751606320762\n",
      "......Outputs 5.0137 5.6192 2.9467 5.6192 5.3665 2.8447 6.6669 5.1040 2.1025 3.0967\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08884330655169304 [best r2] 0.9966751606320762\n",
      "\n",
      "....Epoch 48\n",
      "......[rmse] 0.08411201174139048 [r2] 0.9970198557813709\n",
      "......Outputs 5.0186 5.7296 2.9478 5.7296 5.3764 2.8488 6.6970 5.1670 2.1064 3.1016\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08411201174139048 [best r2] 0.9970198557813709\n",
      "\n",
      "....Epoch 49\n",
      "......[rmse] 0.08132415345004862 [r2] 0.9972141332293569\n",
      "......Outputs 5.0708 5.8076 2.9840 5.8076 5.3903 2.8677 6.7412 5.2033 2.1316 3.1533\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.08132415345004862 [best r2] 0.9972141332293569\n",
      "\n",
      "....Epoch 50\n",
      "......[rmse] 0.07729149392221898 [r2] 0.9974835712166185\n",
      "......Outputs 5.0538 5.7119 2.9678 5.7119 5.3769 2.8565 6.7026 5.1363 2.1246 3.1311\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07729149392221898 [best r2] 0.9974835712166185\n",
      "\n",
      "....Epoch 51\n",
      "......[rmse] 0.07475373721876223 [r2] 0.9976461051432995\n",
      "......Outputs 5.0269 5.6999 2.9503 5.6999 5.3686 2.8500 6.6845 5.1241 2.1134 3.1129\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07475373721876223 [best r2] 0.9976461051432995\n",
      "\n",
      "....Epoch 52\n",
      "......[rmse] 0.0720462698143321 [r2] 0.9978135263832483\n",
      "......Outputs 5.0449 5.7663 2.9630 5.7663 5.3860 2.8636 6.7206 5.1689 2.1208 3.1246\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0720462698143321 [best r2] 0.9978135263832483\n",
      "\n",
      "....Epoch 53\n",
      "......[rmse] 0.07038741213557659 [r2] 0.9979130538773096\n",
      "......Outputs 5.0631 5.7533 2.9791 5.7533 5.3956 2.8725 6.7278 5.1761 2.1300 3.1287\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.07038741213557659 [best r2] 0.9979130538773096\n",
      "\n",
      "....Epoch 54\n",
      "......[rmse] 0.06772602568384989 [r2] 0.9980678874439844\n",
      "......Outputs 5.0425 5.7081 2.9685 5.7081 5.3727 2.8623 6.6905 5.1434 2.1232 3.1176\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06772602568384989 [best r2] 0.9980678874439844\n",
      "\n",
      "....Epoch 55\n",
      "......[rmse] 0.0657820518419025 [r2] 0.9981772123719594\n",
      "......Outputs 5.0283 5.7292 2.9529 5.7292 5.3643 2.8551 6.6864 5.1363 2.1141 3.1132\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0657820518419025 [best r2] 0.9981772123719594\n",
      "\n",
      "....Epoch 56\n",
      "......[rmse] 0.06425734512889647 [r2] 0.998260730836271\n",
      "......Outputs 5.0548 5.7607 2.9702 5.7607 5.3894 2.8697 6.7223 5.1702 2.1230 3.1256\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06425734512889647 [best r2] 0.998260730836271\n",
      "\n",
      "....Epoch 57\n",
      "......[rmse] 0.0635812332798605 [r2] 0.9982971392468897\n",
      "......Outputs 5.0681 5.7427 2.9861 5.7427 5.3999 2.8771 6.7269 5.1781 2.1322 3.1243\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0635812332798605 [best r2] 0.9982971392468897\n",
      "\n",
      "....Epoch 58\n",
      "......[rmse] 0.0614208154365933 [r2] 0.9984108957222377\n",
      "......Outputs 5.0359 5.7125 2.9671 5.7125 5.3703 2.8607 6.6858 5.1385 2.1211 3.1099\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.0614208154365933 [best r2] 0.9984108957222377\n",
      "\n",
      "....Epoch 59\n",
      "......[rmse] 0.06088589699846337 [r2] 0.998438454445984\n",
      "......Outputs 5.0190 5.7232 2.9495 5.7232 5.3542 2.8527 6.6753 5.1269 2.1089 3.1119\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06088589699846337 [best r2] 0.998438454445984\n",
      "\n",
      "....Epoch 60\n",
      "......[rmse] 0.06032236483881613 [r2] 0.9984672265863025\n",
      "......Outputs 5.0571 5.7640 2.9706 5.7640 5.3895 2.8716 6.7257 5.1780 2.1213 3.1245\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 61\n",
      "......[rmse] 0.06322495039252751 [r2] 0.9983161700289779\n",
      "......Outputs 5.0855 5.7621 2.9987 5.7621 5.4162 2.8845 6.7465 5.2020 2.1401 3.1247\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 62\n",
      "......[rmse] 0.06143273737929422 [r2] 0.9984102787636816\n",
      "......Outputs 5.0378 5.7021 2.9735 5.7021 5.3726 2.8612 6.6795 5.1317 2.1244 3.1089\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 63\n",
      "......[rmse] 0.0674397173395128 [r2] 0.9980841887305668\n",
      "......Outputs 4.9875 5.6872 2.9276 5.6872 5.3228 2.8382 6.6364 5.0870 2.0929 3.1014\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 64\n",
      "......[rmse] 0.06800274427902803 [r2] 0.9980520665325839\n",
      "......Outputs 5.0449 5.7739 2.9533 5.7739 5.3719 2.8663 6.7213 5.1804 2.1084 3.1183\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 65\n",
      "......[rmse] 0.08735796629107166 [r2] 0.9967854049551459\n",
      "......Outputs 5.1413 5.8270 3.0339 5.8270 5.4674 2.9101 6.8149 5.2724 2.1625 3.1388\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 66\n",
      "......[rmse] 0.08951096611630704 [r2] 0.9966250003657671\n",
      "......Outputs 5.0767 5.7013 3.0160 5.7013 5.4164 2.8789 6.7071 5.1454 2.1517 3.1196\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 67\n",
      "......[rmse] 0.1196721217850916 [r2] 0.9939673625873658\n",
      "......Outputs 4.9004 5.5791 2.8782 5.5791 5.2437 2.7993 6.5266 4.9645 2.0584 3.0748\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 68\n",
      "......[rmse] 0.13488000922297055 [r2] 0.9923366896361034\n",
      "......Outputs 4.9452 5.7373 2.8614 5.7373 5.2575 2.8221 6.6336 5.1032 2.0459 3.0890\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 69\n",
      "......[rmse] 0.18662860402344453 [r2] 0.9853283939891097\n",
      "......Outputs 5.2579 6.0044 3.0801 6.0044 5.5650 2.9641 6.9870 5.4573 2.1944 3.1758\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 70\n",
      "......[rmse] 0.21837573995442597 [r2] 0.9799123089615615\n",
      "......Outputs 5.2694 5.7941 3.1760 5.7941 5.6177 2.9681 6.8943 5.3169 2.2609 3.1694\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 71\n",
      "......[rmse] 0.26148813265493426 [r2] 0.9711978300774231\n",
      "......Outputs 4.7581 5.3330 2.8381 5.3330 5.1371 2.7311 6.3193 4.7181 2.0342 3.0238\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 72\n",
      "......[rmse] 0.30328110619519427 [r2] 0.96125533128442\n",
      "......Outputs 4.6786 5.5354 2.6733 5.5354 5.0296 2.7138 6.3512 4.8004 1.9158 3.0484\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 73\n",
      "......[rmse] 0.3257029850282548 [r2] 0.9553146958240407\n",
      "......Outputs 5.3359 6.3214 3.0677 6.3214 5.6990 3.0441 7.1931 5.6569 2.1793 3.2556\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 74\n",
      "......[rmse] 0.30112522763358024 [r2] 0.9618042076638402\n",
      "......Outputs 5.3957 5.7996 3.2405 5.7996 5.6950 3.0537 7.0121 5.5193 2.3173 3.1452\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 75\n",
      "......[rmse] 0.27992043624855717 [r2] 0.9669941855767922\n",
      "......Outputs 4.7040 5.1868 2.8251 5.1868 5.0806 2.6892 6.2017 4.6736 2.0549 2.9193\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 76\n",
      "......[rmse] 0.2054857101003107 [r2] 0.9822137459228538\n",
      "......Outputs 4.8415 5.7999 2.8321 5.7999 5.3235 2.7902 6.5769 4.9534 2.0410 3.1803\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 77\n",
      "......[rmse] 0.1837836363735735 [r2] 0.9857722927195394\n",
      "......Outputs 5.2606 6.0961 3.0900 6.0961 5.6496 3.0183 7.0717 5.4410 2.1852 3.3065\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 78\n",
      "......[rmse] 0.11383361333211732 [r2] 0.9945416386391538\n",
      "......Outputs 5.0433 5.4731 2.9986 5.4731 5.2980 2.8754 6.6184 5.1593 2.1265 3.0291\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 79\n",
      "......[rmse] 0.09815468321282929 [r2] 0.9959417079961395\n",
      "......Outputs 4.9532 5.6100 2.9147 5.6100 5.3028 2.7945 6.5609 5.0943 2.0926 3.0164\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 80\n",
      "......[rmse] 0.08088805781529883 [r2] 0.9972439311888165\n",
      "......Outputs 5.1134 5.9388 3.0037 5.9388 5.4817 2.8711 6.8079 5.2396 2.1506 3.1989\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 81\n",
      "......[rmse] 0.06336260793718632 [r2] 0.998308829754788\n",
      "......Outputs 5.0697 5.6952 3.0002 5.6952 5.3747 2.8660 6.6910 5.1262 2.1287 3.1728\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.06032236483881613 [best r2] 0.9984672265863025\n",
      "\n",
      "....Epoch 82\n",
      "......[rmse] 0.060280926661195784 [r2] 0.9984693317266216\n",
      "......Outputs 4.9965 5.6398 2.9361 5.6398 5.3594 2.8409 6.6421 5.1028 2.0891 3.0772\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.060280926661195784 [best r2] 0.9984693317266216\n",
      "\n",
      "....Epoch 83\n",
      "......[rmse] 0.05607680485789429 [r2] 0.9986753908162181\n",
      "......Outputs 5.0568 5.7907 2.9704 5.7907 5.4060 2.8698 6.7406 5.2220 2.1159 3.1119\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.05607680485789429 [best r2] 0.9986753908162181\n",
      "\n",
      "....Epoch 84\n",
      "......[rmse] 0.051259794870852624 [r2] 0.998893185172137\n",
      "......Outputs 5.0675 5.7515 2.9871 5.7515 5.3569 2.8650 6.7093 5.1840 2.1263 3.1388\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.051259794870852624 [best r2] 0.998893185172137\n",
      "\n",
      "....Epoch 85\n",
      "......[rmse] 0.04980080632901248 [r2] 0.9989552942383073\n",
      "......Outputs 5.0334 5.6990 2.9524 5.6990 5.3555 2.8462 6.6748 5.1118 2.1091 3.1163\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04980080632901248 [best r2] 0.9989552942383073\n",
      "\n",
      "....Epoch 86\n",
      "......[rmse] 0.04867297918276307 [r2] 0.9990020768451188\n",
      "......Outputs 5.0500 5.7426 2.9622 5.7426 5.3912 2.8655 6.7179 5.1561 2.1172 3.1256\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04867297918276307 [best r2] 0.9990020768451188\n",
      "\n",
      "....Epoch 87\n",
      "......[rmse] 0.04778549466958215 [r2] 0.9990381365701249\n",
      "......Outputs 5.0577 5.7485 2.9752 5.7485 5.3767 2.8740 6.7221 5.1805 2.1253 3.1237\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04778549466958215 [best r2] 0.9990381365701249\n",
      "\n",
      "....Epoch 88\n",
      "......[rmse] 0.04648856960227427 [r2] 0.9990896390754661\n",
      "......Outputs 5.0398 5.7245 2.9614 5.7245 5.3596 2.8612 6.6942 5.1507 2.1171 3.1070\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04648856960227427 [best r2] 0.9990896390754661\n",
      "\n",
      "....Epoch 89\n",
      "......[rmse] 0.046351803633537546 [r2] 0.9990949876267534\n",
      "......Outputs 5.0459 5.7322 2.9646 5.7322 5.3680 2.8635 6.6987 5.1525 2.1174 3.1210\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.046351803633537546 [best r2] 0.9990949876267534\n",
      "\n",
      "....Epoch 90\n",
      "......[rmse] 0.04617881066400295 [r2] 0.999101730346979\n",
      "......Outputs 5.0577 5.7442 2.9718 5.7442 5.3787 2.8703 6.7147 5.1673 2.1225 3.1283\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04617881066400295 [best r2] 0.999101730346979\n",
      "\n",
      "....Epoch 91\n",
      "......[rmse] 0.04573046839730571 [r2] 0.9991190879712616\n",
      "......Outputs 5.0511 5.7348 2.9674 5.7348 5.3756 2.8673 6.7079 5.1599 2.1210 3.1151\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04573046839730571 [best r2] 0.9991190879712616\n",
      "\n",
      "....Epoch 92\n",
      "......[rmse] 0.045391065870812576 [r2] 0.9991321153588787\n",
      "......Outputs 5.0441 5.7280 2.9656 5.7280 5.3683 2.8646 6.6966 5.1528 2.1184 3.1133\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.045391065870812576 [best r2] 0.9991321153588787\n",
      "\n",
      "....Epoch 93\n",
      "......[rmse] 0.04526587770329261 [r2] 0.9991368959939845\n",
      "......Outputs 5.0483 5.7384 2.9676 5.7384 5.3720 2.8665 6.7032 5.1589 2.1189 3.1193\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04526587770329261 [best r2] 0.9991368959939845\n",
      "\n",
      "....Epoch 94\n",
      "......[rmse] 0.04534548951380275 [r2] 0.9991338573387704\n",
      "......Outputs 5.0537 5.7421 2.9701 5.7421 5.3781 2.8689 6.7107 5.1643 2.1208 3.1197\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04526587770329261 [best r2] 0.9991368959939845\n",
      "\n",
      "....Epoch 95\n",
      "......[rmse] 0.04504487586702279 [r2] 0.9991453032948449\n",
      "......Outputs 5.0499 5.7321 2.9694 5.7321 5.3747 2.8673 6.7035 5.1590 2.1201 3.1158\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04504487586702279 [best r2] 0.9991453032948449\n",
      "\n",
      "....Epoch 96\n",
      "......[rmse] 0.04480373280221787 [r2] 0.9991544298603124\n",
      "......Outputs 5.0445 5.7301 2.9660 5.7301 5.3708 2.8647 6.6981 5.1538 2.1177 3.1137\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04480373280221787 [best r2] 0.9991544298603124\n",
      "\n",
      "....Epoch 97\n",
      "......[rmse] 0.04478369048999586 [r2] 0.9991551861986416\n",
      "......Outputs 5.0485 5.7386 2.9674 5.7386 5.3743 2.8665 6.7046 5.1590 2.1184 3.1166\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04478369048999586 [best r2] 0.9991551861986416\n",
      "\n",
      "....Epoch 98\n",
      "......[rmse] 0.04489534671297791 [r2] 0.9991509683099891\n",
      "......Outputs 5.0535 5.7400 2.9711 5.7400 5.3783 2.8691 6.7091 5.1639 2.1206 3.1177\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04478369048999586 [best r2] 0.9991551861986416\n",
      "\n",
      "....Epoch 99\n",
      "......[rmse] 0.04467981456775834 [r2] 0.9991591007504271\n",
      "......Outputs 5.0497 5.7333 2.9696 5.7333 5.3752 2.8670 6.7028 5.1588 2.1198 3.1140\n",
      "......Labels  5.0497 5.6991 2.9694 5.7012 5.3739 2.8673 6.7030 5.1590 2.1188 3.1085\n",
      "......[best rmse] 0.04467981456775834 [best r2] 0.9991591007504271\n",
      "..Plotting of gradients skipped\n",
      "Finished model size search. The optimal capacity is 1\n",
      "\n",
      "\n",
      "Debug session complete. No errors detected.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run all tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = dl_debug.DebugSession(correct_model_class_ls, model_type, capacity_ls, data_set, zero_data_set, loss_fn, epsilon,\n",
    "                 device, do_all_tests=True, trainer=trainer)\n",
    "ds.main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('mpnn': conda)"
  },
  "interpreter": {
   "hash": "d41d4a34214ac41d40982b9575bce1bf3fd1035657656ceea5e87f8f277f244b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}